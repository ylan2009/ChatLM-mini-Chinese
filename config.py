from dataclasses import dataclass
from os.path import dirname, abspath

# replace '\' on windows to '/'
PROJECT_ROOT: str = '/'.join(abspath(dirname(__file__)).split('\\')) if '\\' in abspath(dirname(__file__)) else abspath(dirname(__file__))

# ===================================================================================
# ä»¥ä¸‹ä¸ºæ¨æ–­çš„é…ç½®
@dataclass
class InferConfig:
    max_seq_len: int = 320                          # å›ç­”çš„æœ€å¤§é•¿åº¦
    mixed_precision: str = "bf16"                   # æ··åˆç²¾åº¦ ''no','fp16','bf16' or 'fp8'

    # æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
    # 1. ç›®å½•è·¯å¾„ï¼ˆåŒ…å«huggingfaceæ ¼å¼çš„æ¨¡å‹æ–‡ä»¶ï¼Œå¦‚pytorch_model.binæˆ–model.safetensorsï¼‰
    # 2. .safetensorsæ–‡ä»¶è·¯å¾„
    # 3. .binæ–‡ä»¶è·¯å¾„ï¼ˆcheckpointæ ¼å¼ï¼‰
    # æ³¨æ„ï¼šç”Ÿæˆreject responseæ—¶åº”ä½¿ç”¨SFTåçš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯åŸå§‹é¢„è®­ç»ƒæ¨¡å‹
    # å¦‚æœä½¿ç”¨SFTçš„.binæ ¼å¼checkpointï¼Œéœ€è¦æŒ‡å®šå…·ä½“æ–‡ä»¶è·¯å¾„ï¼Œå¦‚ï¼š
    # model_dir = PROJECT_ROOT + '/model_save/sft/chat_small_t5.best.bin'
    model_dir: str = PROJECT_ROOT + '/model_save/ChatLM-mini-Chinese/'
    
    # tokenizerç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨model_dirä½œä¸ºtokenizerè·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
    # æ³¨æ„ï¼šå¦‚æœmodel_diræŒ‡å‘.binæ–‡ä»¶ï¼Œå¿…é¡»å•ç‹¬æŒ‡å®štokenizer_dir
    # ä¾‹å¦‚ï¼štokenizer_dir = PROJECT_ROOT + '/model_save/my_tokenizer_wiki/'
    tokenizer_dir: str = None

    # lora PDO åˆå¹¶åçš„æ¨¡å‹æ–‡ä»¶
    # model_file: str = PROJECT_ROOT + '/model_save/chat_small_t5.best.dpo.lora_merged.bin'
    
    # this confing for api demo:
    api_key: str = ""
    host: str = '127.0.0.1'
    port: int = 8812
    reload: bool = True
    workers: int = 1
    log_level: str = 'info'


#===================================================================================
# ä»¥ä¸‹ä¸ºdpoè®­ç»ƒé…ç½®
@dataclass
class DpoConfig:
    max_seq_len: int = 512 + 8                  # 8 for eos token 
    sft_model_file: str = PROJECT_ROOT + '/model_save/'

    tokenizer_dir: str = PROJECT_ROOT + '/model_save/'   # tokenizerä¸€èˆ¬å’Œmodelæƒé‡æ”¾åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹

    dpo_train_file: str = PROJECT_ROOT + '/data/my_dpo_data.json'
    dpo_eval_file: str = PROJECT_ROOT + '/data/my_dpo_eval.json'

    adapter_file: str = PROJECT_ROOT + '/data/dpo/adapter_model.safetensors'
    log_dir: str = PROJECT_ROOT + '/logs/'

    per_device_train_batch_size: int = 4
    num_train_epochs: int = 4
    gradient_accumulation_steps: int = 8
    learning_rate: float = 1e-5
    logging_first_step: bool = True
    logging_steps: int = 20                      
    save_steps: int = 2000
    output_dir: str = PROJECT_ROOT + '/model_save/dpo'
    warmup_steps: int = 1000
    fp16: bool = True
    seed: int = 23333
    beta: float = 0.1



# ä»¥ä¸‹ä¸ºsfté…ç½®
@dataclass
class SFTconfig:
    max_seq_len: int = 384 + 8                # 8 for eos token 

    finetune_from_ckp_file = PROJECT_ROOT + '/model_save/'

    tokenizer_dir: str = PROJECT_ROOT + '/model_save/'  # tokenizerä¸€èˆ¬å’Œmodelæƒé‡æ”¾åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹
    sft_train_file: str = PROJECT_ROOT + '/data/sft_train.json'

    batch_size: int = 12
    num_train_epochs: int = 4
    save_steps: int = 5000
    gradient_accumulation_steps: int = 4
    learning_rate: float = 1e-5
    logging_first_step: bool = True
    logging_steps: int = 100                      
    output_dir: str = PROJECT_ROOT + '/model_save/sft'
    warmup_steps: int = 100
    fp16: bool = True
    seed: int = 23333


# ===================================================================================
# ä»¥ä¸‹ä¸ºè®­ç»ƒçš„é…ç½®
@dataclass
class TrainConfig:
    epochs: int = 5                                 # å¢åŠ åˆ°5ä¸ªepochï¼Œè®©æ¨¡å‹å……åˆ†å­¦ä¹ 
    batch_size_per_gpu: int = 16
    
    learn_rate: float = 0.0001                      # æœ€å¤§ div_factor * learn_rate
    div_factor: int = 50

    mixed_precision: str = "bf16"                   # æ··åˆç²¾åº¦ ''no','fp16','bf16' or 'fp8'

    # æ³¨æ„ï¼šè®¡ç®—æ¢¯åº¦æ—¶ç›¸å½“äºbatch_size * gradient_accumulation_stepsï¼Œè¯´äººè¯å°±æ˜¯æ¢¯åº¦ç´¯ç§¯æ­¥æ•°>1æ—¶ï¼Œç­‰äºå¢å¤§nå€çš„batch_size
    gradient_accumulation_steps: int = 8           # ç´¯ç§¯æ¢¯åº¦æ›´æ–°æ­¥æ•°

    warmup_steps: int = 1024                        # æ¨¡å‹å‚æ•°é¢„çƒ­æ­¥æ•°ï¼Œé¢„çƒ­æ ·æœ¬æ•°=warmup_steps * batch_size * gradient_accumulation_steps

    tokenizer_dir: str = PROJECT_ROOT + '/model_save/my_tokenizer_sp/'  # tokenizerä¸€èˆ¬å’Œmodelæƒé‡æ”¾åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹
    model_file: str = PROJECT_ROOT + '/model_save/chat_small_t5.{}.bin'
    model_config_file: str = PROJECT_ROOT + '/model_save/model_config.json'
    train_file: str = PROJECT_ROOT + '/data/my_train_dataset.parquet'
    validation_file: str = PROJECT_ROOT + '/data/my_valid_dataset.parquet'
    test_file: str = PROJECT_ROOT + '/data/my_test_dataset.parquet'

    # ä»å“ªä¸ªæ¨¡å‹å¼€å§‹å¾®è°ƒï¼Œä»…å½“traing å‡½æ•° is_finetune = Trueæ—¶ç”Ÿæ•ˆ
    # å¾®è°ƒè®°å¾—å†»ç»“æŸäº›å±‚æˆ–è€…è°ƒä½å­¦ä¹ ç‡
    finetune_from_ckp_file = PROJECT_ROOT + '/model_save/chat_small_t5.best.bin'

    # è®­ç»ƒçŠ¶æ€ä¿å­˜ï¼Œä¸­æ–­åå¯ä»¥ä»æ­¤å¤„ç»§ç»­è®­ç»ƒ
    train_state_dir: str = PROJECT_ROOT + '/model_save/train_latest_state'
    output_dir: str = PROJECT_ROOT + '/model_save/pretrain'

    logging_steps: int = 50
    save_steps: int = 5000
    
    # dataset_cache_dir: str = PROJECT_ROOT + '/data/.cache'
    # trainer_log_file: str = PROJECT_ROOT + '/logs/trainer.log'

    keep_latest_n_ckp: int = 8                  # è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ€å¤šä¿ç•™å¤šå°‘ä¸ªåˆ†æ•°æœ€å¥½çš„æ¨¡å‹æ–‡ä»¶

    seed: int = 23333
    dataloader_buffer_size: int = 50000
    max_seq_len: int = 256                      # æœ€å¤§å¥å­é•¿åº¦ï¼Œé»˜è®¤ï¼š256



# ===================================================================================
# ä»¥ä¸‹ä¸ºè®­ç»ƒçš„é…ç½®
@dataclass
class TrainConfigSFT:
    epochs: int = 20                             # å¢åŠ åˆ°20ä¸ªepochï¼Œå› ä¸ºé¢„è®­ç»ƒä¸å……åˆ†ï¼Œéœ€è¦æ›´å¤šè®­ç»ƒ
    batch_size_per_gpu: int = 20                 # ä¿æŒ20ï¼Œå¹³è¡¡æ˜¾å­˜å’Œè®­ç»ƒç¨³å®šæ€§
    
    learn_rate: float = 5e-5                     # æé«˜åˆ°5e-5ï¼Œå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹è´¨é‡ä¸å¥½ï¼Œéœ€è¦æ›´å¤§çš„å­¦ä¹ ç‡
    div_factor: int = 25                         # é™ä½åˆ°25ï¼Œè®©åˆå§‹å­¦ä¹ ç‡æ›´é«˜

    mixed_precision: str = "bf16"                   # æ··åˆç²¾åº¦ ''no','fp16','bf16' or 'fp8'

    # æ³¨æ„ï¼šè®¡ç®—æ¢¯åº¦æ—¶ç›¸å½“äºbatch_size * gradient_accumulation_stepsï¼Œè¯´äººè¯å°±æ˜¯æ¢¯åº¦ç´¯ç§¯æ­¥æ•°>1æ—¶ï¼Œç­‰äºå¢å¤§nå€çš„batch_size
    gradient_accumulation_steps: int = 6           # å¢åŠ åˆ°6ï¼Œå®é™…batch_size=20*2*6=240ï¼ˆæ›´å¤§çš„æœ‰æ•ˆbatch sizeï¼‰

    warmup_steps: int = 200                        # é™ä½åˆ°200ï¼Œå› ä¸ºæ•°æ®é‡å°ï¼Œå¿«é€Ÿè¿›å…¥æ­£å¸¸å­¦ä¹ ç‡
                                                   # é¢„çƒ­æ ·æœ¬æ•°=warmup_steps * batch_size * gradient_accumulation_steps
    
    max_grad_norm: float = 1.0                     # æ·»åŠ æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´lossæ³¢åŠ¨

    tokenizer_dir: str = PROJECT_ROOT + '/model_save/my_tokenizer_sp/'  # tokenizerä¸€èˆ¬å’Œmodelæƒé‡æ”¾åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹
    model_file: str = PROJECT_ROOT + '/model_save/sft/chat_small_t5.{}.bin'
    model_config_file: str = PROJECT_ROOT + '/model_save/sft/model_config.json'
    train_file: str = PROJECT_ROOT + '/data/sft_train_dataset_10k.parquet'      # 1ä¸‡æ¡è®­ç»ƒæ•°æ®
    validation_file: str = PROJECT_ROOT + '/data/sft_valid_dataset_1k.parquet'  # 1åƒæ¡éªŒè¯æ•°æ®
    test_file: str = PROJECT_ROOT + '/data/sft_test_dataset.parquet'

    # ä»å“ªä¸ªæ¨¡å‹å¼€å§‹å¾®è°ƒï¼Œä»…å½“traing å‡½æ•° is_finetune = Trueæ—¶ç”Ÿæ•ˆ
    # å¾®è°ƒè®°å¾—å†»ç»“æŸäº›å±‚æˆ–è€…è°ƒä½å­¦ä¹ ç‡
    finetune_from_ckp_file = PROJECT_ROOT + '/model_save/chat_small_t5.best.bin'

    # è®­ç»ƒçŠ¶æ€ä¿å­˜ï¼Œä¸­æ–­åå¯ä»¥ä»æ­¤å¤„ç»§ç»­è®­ç»ƒ
    train_state_dir: str = PROJECT_ROOT + '/model_save/sft/train_latest_state_sft'
    output_dir: str = PROJECT_ROOT + '/model_save/sft'

    # æ ¹æ®è®­ç»ƒé›†å¤§å°ï¼ˆ8kæ¡ï¼‰è°ƒæ•´ï¼šæ¯ä¸ªepochçº¦83æ­¥ï¼ˆ2ä¸ªGPUï¼Œbatch_size=20ï¼Œgradient_accumulation=6ï¼‰
    # æ¯20æ­¥è®°å½•ä¸€æ¬¡ï¼Œæ¯83æ­¥ä¿å­˜ä¸€æ¬¡ï¼ˆå³æ¯ä¸ªepochä¿å­˜ä¸€æ¬¡ï¼‰
    logging_steps: int = 20                        # æ¯ä¸ªepochçº¦4æ¬¡æ—¥å¿—è®°å½•
    save_steps: int = 83                           # æ¯ä¸ªepochä¿å­˜1æ¬¡æ£€æŸ¥ç‚¹
    
    # dataset_cache_dir: str = PROJECT_ROOT + '/data/.cache'
    # trainer_log_file: str = PROJECT_ROOT + '/logs/trainer.log'

    keep_latest_n_ckp: int = 5                     # è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ€å¤šä¿ç•™å¤šå°‘ä¸ªåˆ†æ•°æœ€å¥½çš„æ¨¡å‹æ–‡ä»¶ï¼ˆ8ä¸‡æ¡æ•°æ®æ€»æ­¥æ•°è¾ƒå°‘ï¼Œé™ä½ä¿ç•™æ•°é‡ï¼‰

    seed: int = 23333
    dataloader_buffer_size: int = 50000
    max_seq_len: int = 256                      # æœ€å¤§å¥å­é•¿åº¦ï¼Œé»˜è®¤ï¼š256


# ===================================================================================
# ä»¥ä¸‹ä¸ºå°æ•°æ®é›†SFTè®­ç»ƒé…ç½® - é’ˆå¯¹16Gå†…å­˜ä¼˜åŒ–
@dataclass
class TrainConfigSFTSmall:
    """
    å°æ•°æ®é›†SFTè®­ç»ƒé…ç½® - é€‚ç”¨äº16Gå†…å­˜ç¯å¢ƒ
    
    æ¨èæ•°æ®é‡ï¼š
    - è®­ç»ƒé›†ï¼š5,000æ ·æœ¬
    - éªŒè¯é›†ï¼š500æ ·æœ¬
    
    é¢„æœŸå†…å­˜å ç”¨ï¼š8-10GBï¼ˆåŒGPUï¼‰
    é¢„æœŸè®­ç»ƒæ—¶é•¿ï¼š2-4å°æ—¶/epoch
    """
    epochs: int = 3                              # å°æ•°æ®é›†è®­ç»ƒ3-5ä¸ªepochå³å¯ï¼Œé¿å…è¿‡æ‹Ÿåˆ
    batch_size_per_gpu: int = 1                  # æè‡´ä½å†…å­˜ï¼šbatch_size=1
    
    learn_rate: float = 5e-5                     # å­¦ä¹ ç‡ä¿æŒä¸å˜
    div_factor: int = 25                         # ä¿æŒä¸å˜

    mixed_precision: str = "bf16"                # æ··åˆç²¾åº¦è®­ç»ƒ

    # å°batch_sizeé€šè¿‡æ¢¯åº¦ç´¯ç§¯è¡¥å¿
    # å®é™…æœ‰æ•ˆbatch_size = 1 * 2(GPU) * 8 = 16
    gradient_accumulation_steps: int = 8         # æ¢¯åº¦ç´¯ç§¯8æ­¥

    warmup_steps: int = 100                      # å°æ•°æ®é›†å‡å°‘warmupæ­¥æ•°
    
    max_grad_norm: float = 1.0                   # æ¢¯åº¦è£å‰ª

    tokenizer_dir: str = PROJECT_ROOT + '/model_save/my_tokenizer_sp/'
    model_file: str = PROJECT_ROOT + '/model_save/sft_small/chat_small_t5.{}.bin'
    model_config_file: str = PROJECT_ROOT + '/model_save/sft_small/model_config.json'
    
    # ä½¿ç”¨prepare_small_sft_data.pyç”Ÿæˆçš„å°æ•°æ®é›†
    train_file: str = PROJECT_ROOT + '/data/sft_train_small_train.parquet'      # å°æ•°æ®é›†è®­ç»ƒæ•°æ®
    validation_file: str = PROJECT_ROOT + '/data/sft_train_small_valid.parquet'  # å°æ•°æ®é›†éªŒè¯æ•°æ®
    test_file: str = PROJECT_ROOT + '/data/sft_test_dataset.parquet'

    # ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹å¾®è°ƒ
    finetune_from_ckp_file = PROJECT_ROOT + '/model_save/chat_small_t5.best.bin'

    # è®­ç»ƒçŠ¶æ€ä¿å­˜
    train_state_dir: str = PROJECT_ROOT + '/model_save/sft_small/train_latest_state_sft_small'
    output_dir: str = PROJECT_ROOT + '/model_save/sft_small'

    # 5000æ ·æœ¬ï¼Œbatch_size=1*2*8=16ï¼Œæ¯ä¸ªepochçº¦312æ­¥
    logging_steps: int = 50                      # æ¯ä¸ªepochçº¦6æ¬¡æ—¥å¿—
    save_steps: int = 312                        # æ¯ä¸ªepochä¿å­˜1æ¬¡
    
    keep_latest_n_ckp: int = 3                   # å°æ•°æ®é›†åªä¿ç•™3ä¸ªæœ€å¥½çš„æ¨¡å‹

    seed: int = 23333
    dataloader_buffer_size: int = 10000          # å‡å°buffer
    max_seq_len: int = 512                       # åºåˆ—é•¿åº¦512


# ===================================================================================
# ä»¥ä¸‹ä¸ºé«˜æ€§èƒ½SFTè®­ç»ƒé…ç½® - å……åˆ†åˆ©ç”¨GPUæ˜¾å­˜
@dataclass
class TrainConfigSFTFast:
    """
    é«˜æ€§èƒ½SFTè®­ç»ƒé…ç½® - å……åˆ†åˆ©ç”¨GPUæ˜¾å­˜ï¼ˆ20GB Ã— 2ï¼‰
    
    æ¨èæ•°æ®é‡ï¼š
    - è®­ç»ƒé›†ï¼š5,000æ ·æœ¬
    - éªŒè¯é›†ï¼š500æ ·æœ¬
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    - å¢å¤§batch_sizeï¼šä»1æå‡åˆ°16ï¼ˆå……åˆ†åˆ©ç”¨GPUæ˜¾å­˜ï¼‰
    - å‡å°‘æ¢¯åº¦ç´¯ç§¯ï¼šä»8é™åˆ°2ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰
    - å®é™…æœ‰æ•ˆbatch_size = 16 * 2(GPU) * 2 = 64ï¼ˆæ¯”åŸæ¥çš„16å¤§4å€ï¼‰
    - å¢åŠ num_workersï¼šåŠ é€Ÿæ•°æ®åŠ è½½
    
    é¢„æœŸå†…å­˜å ç”¨ï¼š10-14GBï¼ˆåŒGPUï¼‰
    é¢„æœŸGPUæ˜¾å­˜å ç”¨ï¼š12-16GB/GPUï¼ˆæå‡6-8å€ï¼‰
    é¢„æœŸè®­ç»ƒé€Ÿåº¦ï¼šæå‡5-6å€
    """
    epochs: int = 3                              # å°æ•°æ®é›†è®­ç»ƒ3-5ä¸ªepochå³å¯
    batch_size_per_gpu: int = 8                 # ğŸš€ ä»1æå‡åˆ°16ï¼Œå……åˆ†åˆ©ç”¨GPUæ˜¾å­˜
    
    learn_rate: float = 5e-5                     # å­¦ä¹ ç‡ä¿æŒä¸å˜
    div_factor: int = 25                         # ä¿æŒä¸å˜

    mixed_precision: str = "bf16"                # æ··åˆç²¾åº¦è®­ç»ƒ

    # å‡å°‘æ¢¯åº¦ç´¯ç§¯ï¼Œå› ä¸ºbatch_sizeå·²ç»å¢å¤§
    # å®é™…æœ‰æ•ˆbatch_size = 16 * 2(GPU) * 2 = 64
    gradient_accumulation_steps: int = 2         # ğŸš€ ä»8é™åˆ°2ï¼Œå‡å°‘å†…å­˜å ç”¨

    warmup_steps: int = 100                      # å°æ•°æ®é›†å‡å°‘warmupæ­¥æ•°
    
    max_grad_norm: float = 1.0                   # æ¢¯åº¦è£å‰ª

    tokenizer_dir: str = PROJECT_ROOT + '/model_save/my_tokenizer_sp/'
    model_file: str = PROJECT_ROOT + '/model_save/sft_fast/chat_small_t5.{}.bin'
    model_config_file: str = PROJECT_ROOT + '/model_save/sft_fast/model_config.json'
    
    # ä½¿ç”¨prepare_small_sft_data.pyç”Ÿæˆçš„å°æ•°æ®é›†
    train_file: str = PROJECT_ROOT + '/data/sft_train_small_train.parquet'      # å°æ•°æ®é›†è®­ç»ƒæ•°æ®
    validation_file: str = PROJECT_ROOT + '/data/sft_train_small_valid.parquet'  # å°æ•°æ®é›†éªŒè¯æ•°æ®
    test_file: str = PROJECT_ROOT + '/data/sft_test_dataset.parquet'

    # ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹å¾®è°ƒ
    finetune_from_ckp_file = PROJECT_ROOT + '/model_save/chat_small_t5.best.bin'

    # è®­ç»ƒçŠ¶æ€ä¿å­˜
    train_state_dir: str = PROJECT_ROOT + '/model_save/sft_fast/train_latest_state_sft_fast'
    output_dir: str = PROJECT_ROOT + '/model_save/sft_fast'

    # 5000æ ·æœ¬ï¼Œbatch_size=16*2*2=64ï¼Œæ¯ä¸ªepochçº¦78æ­¥ï¼ˆæ¯”åŸæ¥çš„312æ­¥å¿«4å€ï¼‰
    logging_steps: int = 15                      # æ¯ä¸ªepochçº¦5æ¬¡æ—¥å¿—
    save_steps: int = 78                         # æ¯ä¸ªepochä¿å­˜1æ¬¡
    
    keep_latest_n_ckp: int = 3                   # å°æ•°æ®é›†åªä¿ç•™3ä¸ªæœ€å¥½çš„æ¨¡å‹

    seed: int = 23333
    dataloader_buffer_size: int = 10000          # å‡å°buffer
    max_seq_len: int = 512                       # åºåˆ—é•¿åº¦512


#======================================================================================
# ä»¥ä¸‹ä¸ºæ¨¡å‹çš„é…ç½®
@dataclass
class T5ModelConfig:

    d_ff: int = 3072                        # å…¨è¿æ¥å±‚ç»´åº¦

    d_model: int = 768                      # è¯å‘é‡ç»´åº¦
    num_heads: int = 12                     # æ³¨æ„åŠ›å¤´æ•° d_model // num_heads == d_kv
    d_kv: int = 64                          # d_model // num_heads

    num_decoder_layers: int = 10            # Transformer decoder éšè—å±‚å±‚æ•°
    num_layers: int = 10                    # Transformer encoder éšè—å±‚å±‚æ•°