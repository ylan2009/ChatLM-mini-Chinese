#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒä¸­æ–‡ Tokenizer è„šæœ¬

åŠŸèƒ½è¯´æ˜ï¼š
1. åŸºäº T5-base tokenizer è®­ç»ƒæ–°çš„ä¸­æ–‡ tokenizerï¼ˆæ¨èæ–¹å¼ï¼‰
   - ä½¿ç”¨ T5-base çš„ tokenizer ä½œä¸ºåŸºç¡€
   - åœ¨ä¸­æ–‡ç»´åŸºç™¾ç§‘è¯­æ–™ä¸Šè®­ç»ƒæ–°çš„è¯æ±‡è¡¨
   - é€‚åˆä¸­æ–‡ä¸ºä¸»çš„åœºæ™¯

2. ä»é›¶å¼€å§‹åˆ›å»ºè‡ªå®šä¹‰ tokenizer
   - å­—ç¬¦çº§åˆ« BPE tokenizer
   - å­—èŠ‚çº§åˆ« BPE tokenizer
   - å®Œå…¨è‡ªå®šä¹‰çš„ tokenizer

3. ä½¿ç”¨ SentencePiece è®­ç»ƒ tokenizerï¼ˆç¨³å®šå¿«é€Ÿï¼‰
   - æ”¯æŒ unigramã€BPEã€charã€word æ¨¡å‹
   - è®­ç»ƒé€Ÿåº¦å¿«ï¼Œå†…å­˜å ç”¨ä½
   - ç¨³å®šæ€§é«˜ï¼Œä¸æ˜“å‡ºé”™

ä½¿ç”¨æ–¹æ³•ï¼š
    # æ–¹å¼1ï¼šåŸºäº T5-base è®­ç»ƒï¼ˆæ¨èï¼‰
    python train_tokenizer.py --method t5-base --wiki-file ../data/wiki.simple.txt --output-dir ../model_save/my_tokenizer_wiki
    
    # æ–¹å¼2ï¼šä»é›¶åˆ›å»ºå­—ç¬¦çº§åˆ« tokenizer
    python train_tokenizer.py --method char-bpe --wiki-file ../data/wiki.simple.txt --output-dir ../model_save/my_tokenizer_char
    
    # æ–¹å¼3ï¼šä»é›¶åˆ›å»ºå­—èŠ‚çº§åˆ« tokenizer
    python train_tokenizer.py --method byte-bpe --wiki-file ../data/wiki.simple.txt --output-dir ../model_save/my_tokenizer_byte
    
    # æ–¹å¼4ï¼šä½¿ç”¨ SentencePiece è®­ç»ƒï¼ˆç¨³å®šå¿«é€Ÿï¼‰
    python train_tokenizer.py --method sentencepiece --wiki-file ../data/wiki.simple.txt --output-dir ../model_save/my_tokenizer_sp
"""

import argparse
import os
import sys
from pathlib import Path
from typing import Iterator, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å»¶è¿Ÿå¯¼å…¥ï¼Œåªåœ¨éœ€è¦æ—¶æ£€æŸ¥
def check_transformers():
    """æ£€æŸ¥å¹¶å¯¼å…¥ transformers å’Œ rich"""
    try:
        from transformers import AutoTokenizer, PreTrainedTokenizerFast
        from rich import progress
        return AutoTokenizer, PreTrainedTokenizerFast, progress
    except ImportError:
        print("é”™è¯¯: ç¼ºå°‘å¿…è¦çš„åº“ï¼Œè¯·å®‰è£…: pip install transformers rich")
        sys.exit(1)

def check_tokenizers():
    """æ£€æŸ¥å¹¶å¯¼å…¥ tokenizers"""
    try:
        import tokenizers
        from tokenizers import Tokenizer, decoders
        from tokenizers.models import BPE
        from tokenizers.pre_tokenizers import Whitespace, Punctuation, Digits, ByteLevel, Metaspace
        from tokenizers.normalizers import NFKC
        return tokenizers, Tokenizer, decoders, BPE, Whitespace, Punctuation, Digits, ByteLevel, Metaspace, NFKC
    except ImportError:
        return None, None, None, None, None, None, None, None, None, None

def check_sentencepiece():
    """æ£€æŸ¥å¹¶å¯¼å…¥ sentencepiece"""
    try:
        import sentencepiece as spm
        return spm
    except ImportError:
        return None

# å…¨å±€å˜é‡ï¼Œå»¶è¿Ÿåˆå§‹åŒ–
AutoTokenizer = None
PreTrainedTokenizerFast = None
progress = None
tokenizers = None
Tokenizer = None
decoders = None
BPE = None
Whitespace = None
Punctuation = None
Digits = None
ByteLevel = None
Metaspace = None
NFKC = None
spm = None

from config import PROJECT_ROOT


def get_wiki_corpus_iterator(wiki_file: str, min_chunk_size: int = 2048, batch_size: int = 1000) -> Iterator[List[str]]:
    """
    ä»ç»´åŸºç™¾ç§‘æ–‡ä»¶ä¸­ç”Ÿæˆè®­ç»ƒè¯­æ–™è¿­ä»£å™¨
    
    Args:
        wiki_file: ç»´åŸºç™¾ç§‘æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        min_chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å°å­—ç¬¦æ•°ï¼ˆé»˜è®¤2048ï¼‰
        batch_size: æ¯æ¬¡è¿­ä»£è¿”å›çš„æ–‡æœ¬å—æ•°é‡ï¼ˆé»˜è®¤1000ï¼‰
    
    Yields:
        åŒ…å«å¤šä¸ªæ–‡æœ¬å—çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æœ¬å—è‡³å°‘ min_chunk_size ä¸ªå­—ç¬¦
    """
    global progress
    if progress is None:
        _, _, progress = check_transformers()
    if not os.path.exists(wiki_file):
        raise FileNotFoundError(f"ç»´åŸºç™¾ç§‘æ–‡ä»¶ä¸å­˜åœ¨: {wiki_file}")
    
    print(f"åŠ è½½ç»´åŸºç™¾ç§‘è¯­æ–™: {wiki_file}")
    lines = []
    with open(wiki_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    print(f"æ€»è¡Œæ•°: {len(lines)}")
    
    def get_training_corpus():
        buffer = []
        txt = []
        len_cnt = 0
        
        for line in progress.track(lines, description="å¤„ç†è¯­æ–™"):
            # è·³è¿‡ç©ºè¡Œ
            line = line.strip()
            if not line:
                continue
            
            len_cnt += len(line)
            txt.append(line)
            
            # å½“ç´¯ç§¯å­—ç¬¦æ•°è¾¾åˆ°æœ€å°å—å¤§å°æ—¶ï¼Œåˆ›å»ºä¸€ä¸ªæ–‡æœ¬å—
            if len_cnt >= min_chunk_size:
                text = ' '.join(txt)  # ä½¿ç”¨ç©ºæ ¼è¿æ¥ï¼Œè€Œä¸æ˜¯ç›´æ¥æ‹¼æ¥
                # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©ºä¸”é•¿åº¦åˆç†
                if text and len(text) >= 10:
                    buffer.append(text)
                txt = []
                len_cnt = 0
            
            # å½“ç¼“å†²åŒºè¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶ï¼Œè¿”å›ä¸€æ‰¹æ•°æ®
            if len(buffer) >= batch_size:
                yield buffer
                buffer = []
        
        # å¤„ç†å‰©ä½™çš„æ–‡æœ¬
        if txt:
            text = ' '.join(txt)
            if text and len(text) >= 10:
                buffer.append(text)
        
        # è¿”å›æœ€åä¸€æ‰¹æ•°æ®
        if len(buffer) > 0:
            yield buffer
    
    return get_training_corpus()


def get_parquet_corpus_iterator(parquet_file: str, batch_size: int = 1000) -> Iterator[List[str]]:
    """
    ä» Parquet æ–‡ä»¶ä¸­ç”Ÿæˆè®­ç»ƒè¯­æ–™è¿­ä»£å™¨
    
    Args:
        parquet_file: Parquet æ–‡ä»¶è·¯å¾„
        batch_size: æ¯æ¬¡è¿­ä»£è¿”å›çš„æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤1000ï¼‰
    
    Yields:
        åŒ…å«å¤šä¸ªæ–‡æœ¬çš„åˆ—è¡¨
    """
    global progress
    if progress is None:
        _, _, progress = check_transformers()
    
    try:
        import pyarrow.parquet as pq
    except ImportError:
        raise ImportError("éœ€è¦ pyarrow åº“: pip install pyarrow")
    
    if not os.path.exists(parquet_file):
        raise FileNotFoundError(f"Parquet æ–‡ä»¶ä¸å­˜åœ¨: {parquet_file}")
    
    print(f"åŠ è½½ Parquet è¯­æ–™: {parquet_file}")
    pf = pq.read_table(parquet_file)
    
    def get_training_corpus():
        buffer = []
        for prompt, response in progress.track(
            zip(pf['prompt'], pf['response']), 
            total=pf.num_rows,
            description="å¤„ç†è¯­æ–™"
        ):
            # è·å–å®é™…çš„å­—ç¬¦ä¸²å€¼
            prompt_str = prompt.as_py() if prompt.as_py() else ""
            response_str = response.as_py() if response.as_py() else ""
            
            # è·³è¿‡ç©ºå€¼
            if not prompt_str and not response_str:
                continue
            
            # ç»„åˆ prompt å’Œ response
            text = f"{prompt_str} {response_str}".strip()
            
            # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©ºä¸”é•¿åº¦åˆç†
            if text and len(text) >= 10:
                buffer.append(text)
            
            if len(buffer) >= batch_size:
                yield buffer
                buffer = []
        
        if buffer:
            yield buffer
    
    return get_training_corpus()


def train_t5_base_tokenizer(
    corpus_iterator: Iterator[List[str]],
    vocab_size: int = 40960,
    output_dir: str = None
):
    """è®­ç»ƒåŸºäº T5-base çš„ tokenizerï¼ˆéœ€è¦ transformersï¼‰"""
    global AutoTokenizer, PreTrainedTokenizerFast
    if AutoTokenizer is None:
        AutoTokenizer, PreTrainedTokenizerFast, _ = check_transformers()
    """
    åŸºäº T5-base tokenizer è®­ç»ƒæ–°çš„ä¸­æ–‡ tokenizerï¼ˆæ¨èæ–¹å¼ï¼‰
    
    Args:
        corpus_iterator: è¯­æ–™è¿­ä»£å™¨
        vocab_size: è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤40960ï¼‰
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        è®­ç»ƒå¥½çš„ tokenizer
    """
    print("\n" + "="*60)
    print("æ–¹æ³•: åŸºäº T5-base tokenizer è®­ç»ƒ")
    print("="*60)
    
    # Step 1: åŠ è½½ T5-base çš„ tokenizer
    print("\næ­¥éª¤ 1: åŠ è½½ T5-base tokenizer...")
    old_tokenizer = AutoTokenizer.from_pretrained('t5-base')
    print("âœ“ T5-base tokenizer åŠ è½½å®Œæˆ")
    
    # Step 2: è®­ç»ƒæ–°çš„ tokenizer
    print(f"\næ­¥éª¤ 2: å¼€å§‹è®­ç»ƒ tokenizer (è¯æ±‡è¡¨å¤§å°: {vocab_size})...")
    print("æ³¨æ„: è¿™æ˜¯ CPU å¯†é›†å‹ä»»åŠ¡ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆçº¦1å°æ—¶ï¼‰")
    print("      æœ€å¤§å†…å­˜å ç”¨çº¦ 20GBï¼Œè¯·ç¡®ä¿æœ‰è¶³å¤Ÿå†…å­˜")
    
    tokenizer = old_tokenizer.train_new_from_iterator(
        corpus_iterator, 
        vocab_size=vocab_size
    )
    
    print("âœ“ Tokenizer è®­ç»ƒå®Œæˆ")
    
    # Step 3: ä¿å­˜ tokenizer
    if output_dir:
        print(f"\næ­¥éª¤ 3: ä¿å­˜ tokenizer åˆ° {output_dir}...")
        os.makedirs(output_dir, exist_ok=True)
        tokenizer.save_pretrained(output_dir)
        print(f"âœ“ Tokenizer å·²ä¿å­˜åˆ° {output_dir}")
    
    return tokenizer


def train_char_level_tokenizer(
    corpus_iterator: Iterator[List[str]],
    vocab_size: int = 40960,
    output_dir: str = None
):
    """
    è®­ç»ƒå­—ç¬¦çº§åˆ«çš„ BPE tokenizer
    
    Args:
        corpus_iterator: è¯­æ–™è¿­ä»£å™¨
        vocab_size: è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤40960ï¼‰
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        è®­ç»ƒå¥½çš„ tokenizer
    """
    global tokenizers, Tokenizer, decoders, BPE, Punctuation, Digits, Metaspace, NFKC, PreTrainedTokenizerFast
    
    # æ£€æŸ¥å¹¶å¯¼å…¥å¿…è¦çš„åº“
    if tokenizers is None:
        result = check_tokenizers()
        if result[0] is None:
            raise ImportError("éœ€è¦ tokenizers åº“: pip install tokenizers")
        tokenizers, Tokenizer, decoders, BPE, _, Punctuation, Digits, Metaspace, NFKC = result[:9]
    
    if PreTrainedTokenizerFast is None:
        _, PreTrainedTokenizerFast, _ = check_transformers()
    
    print("\n" + "="*60)
    print("æ–¹æ³•: å­—ç¬¦çº§åˆ« BPE tokenizer")
    print("="*60)
    
    # åˆ›å»ºå­—ç¬¦çº§åˆ«çš„ BPE tokenizer
    print("\næ­¥éª¤ 1: åˆ›å»ºå­—ç¬¦çº§åˆ« BPE tokenizer...")
    model = BPE(unk_token="[UNK]")
    tokenizer_obj = Tokenizer(model)
    
    # ä½¿ç”¨ NFKC æ ‡å‡†åŒ–ï¼ˆå…¨è§’è½¬åŠè§’ç­‰ï¼‰
    tokenizer_obj.normalizer = tokenizers.normalizers.Sequence([NFKC()])
    
    # é¢„åˆ†å‰²ï¼šæ ‡ç‚¹ç¬¦å·ã€æ•°å­—ã€Metaspace
    tokenizer_obj.pre_tokenizer = tokenizers.pre_tokenizers.Sequence([
        Punctuation(), 
        Digits(individual_digits=True), 
        Metaspace()
    ])
    
    # æ·»åŠ ç‰¹æ®Šæ ‡è®°
    tokenizer_obj.add_special_tokens([
        "[PAD]", "[EOS]", "[SEP]", "[BOS]", 
        "[CLS]", "[MASK]", "[UNK]"
    ])
    
    # è®¾ç½®è§£ç å™¨
    tokenizer_obj.decoder = decoders.Metaspace()
    
    print("âœ“ Tokenizer å¯¹è±¡åˆ›å»ºå®Œæˆ")
    
    # è½¬æ¢ä¸º PreTrainedTokenizerFast
    print("\næ­¥éª¤ 2: è½¬æ¢ä¸º PreTrainedTokenizerFast...")
    tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer_obj,
        unk_token="[UNK]",
        pad_token="[PAD]",
        cls_token="[CLS]",
        sep_token="[SEP]",
        mask_token="[MASK]",
        bos_token='[BOS]',
        eos_token='[EOS]',
    )
    
    # è®­ç»ƒ tokenizer
    print(f"\næ­¥éª¤ 3: å¼€å§‹è®­ç»ƒ tokenizer (è¯æ±‡è¡¨å¤§å°: {vocab_size})...")
    tokenizer = tokenizer.train_new_from_iterator(
        corpus_iterator, 
        vocab_size=vocab_size
    )
    
    print("âœ“ Tokenizer è®­ç»ƒå®Œæˆ")
    
    # ä¿å­˜ tokenizer
    if output_dir:
        print(f"\næ­¥éª¤ 4: ä¿å­˜ tokenizer åˆ° {output_dir}...")
        os.makedirs(output_dir, exist_ok=True)
        tokenizer.save_pretrained(output_dir)
        print(f"âœ“ Tokenizer å·²ä¿å­˜åˆ° {output_dir}")
    
    return tokenizer


def train_byte_level_tokenizer(
    corpus_iterator: Iterator[List[str]],
    vocab_size: int = 40960,
    output_dir: str = None
):
    """
    è®­ç»ƒå­—èŠ‚çº§åˆ«çš„ BPE tokenizer
    
    Args:
        corpus_iterator: è¯­æ–™è¿­ä»£å™¨
        vocab_size: è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤40960ï¼‰
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        è®­ç»ƒå¥½çš„ tokenizer
    """
    global tokenizers, Tokenizer, decoders, BPE, ByteLevel, PreTrainedTokenizerFast
    
    # æ£€æŸ¥å¹¶å¯¼å…¥å¿…è¦çš„åº“
    if tokenizers is None:
        result = check_tokenizers()
        if result[0] is None:
            raise ImportError("éœ€è¦ tokenizers åº“: pip install tokenizers")
        tokenizers, Tokenizer, decoders, BPE, _, _, _, ByteLevel, _, _ = result
    
    if PreTrainedTokenizerFast is None:
        _, PreTrainedTokenizerFast, _ = check_transformers()
    
    print("\n" + "="*60)
    print("æ–¹æ³•: å­—èŠ‚çº§åˆ« BPE tokenizer")
    print("="*60)
    
    # åˆ›å»ºå­—èŠ‚çº§åˆ«çš„ BPE tokenizer
    print("\næ­¥éª¤ 1: åˆ›å»ºå­—èŠ‚çº§åˆ« BPE tokenizer...")
    model = BPE()  # å­—èŠ‚çº§åˆ«ä¸éœ€è¦ unk_token
    tokenizer_obj = Tokenizer(model)
    
    # å­—èŠ‚çº§åˆ«é¢„åˆ†å‰²
    tokenizer_obj.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(
        add_prefix_space=False
    )
    
    # æ·»åŠ ç‰¹æ®Šæ ‡è®°
    tokenizer_obj.add_special_tokens([
        "[PAD]", "[EOS]", "[SEP]", "[BOS]", 
        "[CLS]", "[MASK]", "[UNK]"
    ])
    
    # è®¾ç½®è§£ç å™¨å’Œåå¤„ç†å™¨
    tokenizer_obj.decoder = decoders.ByteLevel(
        add_prefix_space=True, 
        use_regex=True
    )
    tokenizer_obj.post_processor = tokenizers.processors.ByteLevel(
        trim_offsets=False
    )
    
    print("âœ“ Tokenizer å¯¹è±¡åˆ›å»ºå®Œæˆ")
    
    # è½¬æ¢ä¸º PreTrainedTokenizerFast
    print("\næ­¥éª¤ 2: è½¬æ¢ä¸º PreTrainedTokenizerFast...")
    tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer_obj,
        unk_token="[UNK]",
        pad_token="[PAD]",
        cls_token="[CLS]",
        sep_token="[SEP]",
        mask_token="[MASK]",
        bos_token='[BOS]',
        eos_token='[EOS]',
    )
    
    # è®­ç»ƒ tokenizer
    print(f"\næ­¥éª¤ 3: å¼€å§‹è®­ç»ƒ tokenizer (è¯æ±‡è¡¨å¤§å°: {vocab_size})...")
    tokenizer = tokenizer.train_new_from_iterator(
        corpus_iterator, 
        vocab_size=vocab_size
    )
    
    print("âœ“ Tokenizer è®­ç»ƒå®Œæˆ")
    
    # ä¿å­˜ tokenizer
    if output_dir:
        print(f"\næ­¥éª¤ 4: ä¿å­˜ tokenizer åˆ° {output_dir}...")
        os.makedirs(output_dir, exist_ok=True)
        tokenizer.save_pretrained(output_dir)
        print(f"âœ“ Tokenizer å·²ä¿å­˜åˆ° {output_dir}")
    
    return tokenizer


def train_sentencepiece_tokenizer(
    wiki_file: str = None,
    parquet_file: str = None,
    vocab_size: int = 40960,
    output_dir: str = None,
    model_type: str = 'unigram',
    character_coverage: float = 0.9995
):
    """
    ä½¿ç”¨ SentencePiece è®­ç»ƒ tokenizer
    
    Args:
        wiki_file: ç»´åŸºç™¾ç§‘æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        parquet_file: Parquet æ–‡ä»¶è·¯å¾„
        vocab_size: è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤40960ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        model_type: æ¨¡å‹ç±»å‹ï¼Œå¯é€‰ 'unigram', 'bpe', 'char', 'word'ï¼ˆé»˜è®¤ 'unigram'ï¼‰
        character_coverage: å­—ç¬¦è¦†ç›–ç‡ï¼ˆé»˜è®¤ 0.9995ï¼Œé€‚åˆä¸­æ–‡ï¼‰
    
    Returns:
        è®­ç»ƒå¥½çš„ tokenizer
    """
    global spm, progress
    
    # æ£€æŸ¥å¹¶å¯¼å…¥å¿…è¦çš„åº“
    if spm is None:
        spm = check_sentencepiece()
        if spm is None:
            raise ImportError("éœ€è¦ sentencepiece åº“: pip install sentencepiece")
    
    if progress is None:
        _, _, progress = check_transformers()
    
    print("\n" + "="*60)
    print(f"æ–¹æ³•: SentencePiece tokenizer (æ¨¡å‹ç±»å‹: {model_type})")
    print("="*60)
    
    # å‡†å¤‡è¾“å…¥æ–‡ä»¶
    if wiki_file:
        input_file = wiki_file
        print(f"\nä½¿ç”¨ç»´åŸºç™¾ç§‘æ–‡ä»¶: {input_file}")
    elif parquet_file:
        # éœ€è¦å…ˆå°† Parquet è½¬æ¢ä¸ºæ–‡æœ¬æ–‡ä»¶
        print(f"\nä» Parquet æ–‡ä»¶æå–æ–‡æœ¬: {parquet_file}")
        try:
            import pyarrow.parquet as pq
        except ImportError:
            raise ImportError("éœ€è¦ pyarrow åº“: pip install pyarrow")
        
        # åˆ›å»ºä¸´æ—¶æ–‡æœ¬æ–‡ä»¶
        import tempfile
        temp_file = tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False, suffix='.txt')
        input_file = temp_file.name
        
        print("æ­£åœ¨æå–æ–‡æœ¬...")
        pf = pq.read_table(parquet_file)
        with open(input_file, 'w', encoding='utf-8') as f:
            for prompt, response in progress.track(
                zip(pf['prompt'], pf['response']),
                total=pf.num_rows,
                description="æå–æ–‡æœ¬"
            ):
                prompt_str = prompt.as_py() if prompt.as_py() else ""
                response_str = response.as_py() if response.as_py() else ""
                if prompt_str or response_str:
                    text = f"{prompt_str} {response_str}".strip()
                    if text:
                        f.write(text + '\n')
        
        print(f"âœ“ æ–‡æœ¬å·²æå–åˆ°ä¸´æ—¶æ–‡ä»¶: {input_file}")
    else:
        raise ValueError("å¿…é¡»æä¾› wiki_file æˆ– parquet_file")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        model_prefix = os.path.join(output_dir, 'sentencepiece')
    else:
        model_prefix = 'sentencepiece'
    
    # è®­ç»ƒå‰ï¼šç»Ÿè®¡æ•°æ®ä¿¡æ¯
    print(f"\næ­¥éª¤ 1: åˆ†æè®­ç»ƒæ•°æ®...")
    import time
    file_size = os.path.getsize(input_file) / (1024 * 1024)  # MB
    
    # å¿«é€Ÿç»Ÿè®¡è¡Œæ•°
    print("  - æ­£åœ¨ç»Ÿè®¡æ•°æ®é‡...")
    line_count = 0
    with open(input_file, 'r', encoding='utf-8') as f:
        for _ in f:
            line_count += 1
    
    print(f"  âœ“ æ•°æ®æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    print(f"  âœ“ æ•°æ®è¡Œæ•°: {line_count:,} è¡Œ")
    
    # é¢„ä¼°è®­ç»ƒæ—¶é—´
    estimated_minutes = max(5, int(file_size / 200 * 15))  # ç²—ç•¥ä¼°è®¡ï¼šæ¯200MBçº¦15åˆ†é’Ÿ
    print(f"  âœ“ é¢„ä¼°è®­ç»ƒæ—¶é—´: {estimated_minutes}-{estimated_minutes*2} åˆ†é’Ÿ")
    
    # è®­ç»ƒ SentencePiece æ¨¡å‹
    print(f"\næ­¥éª¤ 2: å¼€å§‹è®­ç»ƒ SentencePiece æ¨¡å‹...")
    print(f"  - è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  - æ¨¡å‹ç±»å‹: {model_type}")
    print(f"  - å­—ç¬¦è¦†ç›–ç‡: {character_coverage}")
    print(f"  - è®­ç»ƒæ¨¡å¼: å¤§è¯­æ–™åº“æ¨¡å¼")
    print(f"\n  ğŸš€ è®­ç»ƒè¿›è¡Œä¸­ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print(f"  ğŸ’¡ æç¤º: SentencePiece ä¼šè¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼Œè¯·å…³æ³¨æ—¥å¿—ä¿¡æ¯")
    print("  " + "="*50)
    
    # SentencePiece è®­ç»ƒå‚æ•°
    train_args = [
        f'--input={input_file}',
        f'--model_prefix={model_prefix}',
        f'--vocab_size={vocab_size}',
        f'--model_type={model_type}',
        f'--character_coverage={character_coverage}',
        '--pad_id=0',
        '--unk_id=1',
        '--bos_id=2',
        '--eos_id=3',
        '--pad_piece=[PAD]',
        '--unk_piece=[UNK]',
        '--bos_piece=[BOS]',
        '--eos_piece=[EOS]',
        '--user_defined_symbols=[CLS],[SEP],[MASK]',
        '--normalization_rule_name=nfkc',
        '--remove_extra_whitespaces=true',
        '--max_sentence_length=16384',
        '--num_threads=16',
        '--train_extremely_large_corpus=true',  # å¯ç”¨å¤§è¯­æ–™åº“è®­ç»ƒæ¨¡å¼
    ]
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # å¼€å§‹è®­ç»ƒ
    spm.SentencePieceTrainer.train(' '.join(train_args))
    
    # è®¡ç®—è®­ç»ƒè€—æ—¶
    elapsed_time = time.time() - start_time
    elapsed_minutes = int(elapsed_time / 60)
    elapsed_seconds = int(elapsed_time % 60)
    
    print("  " + "="*50)
    print(f"  âœ“ SentencePiece æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"  â±  è®­ç»ƒè€—æ—¶: {elapsed_minutes} åˆ† {elapsed_seconds} ç§’")
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    print(f"\næ­¥éª¤ 3: åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
    sp = spm.SentencePieceProcessor()
    sp.load(f'{model_prefix}.model')
    print(f"  âœ“ æ¨¡å‹å·²åŠ è½½ï¼Œè¯æ±‡è¡¨å¤§å°: {sp.get_piece_size()}")
    
    # è½¬æ¢ä¸º Hugging Face tokenizer
    print(f"\næ­¥éª¤ 4: è½¬æ¢ä¸º Hugging Face tokenizer...")
    try:
        from transformers import T5Tokenizer
        
        # åˆ›å»º tokenizer é…ç½®
        tokenizer = T5Tokenizer(
            vocab_file=f'{model_prefix}.model',
            eos_token='[EOS]',
            unk_token='[UNK]',
            pad_token='[PAD]',
            bos_token='[BOS]',
            extra_ids=0,
        )
        
        # æ·»åŠ ç‰¹æ®Š token
        special_tokens = {
            'additional_special_tokens': ['[CLS]', '[SEP]', '[MASK]']
        }
        tokenizer.add_special_tokens(special_tokens)
        
        print("  âœ“ å·²è½¬æ¢ä¸º Hugging Face T5Tokenizer")
        
        # ä¿å­˜ tokenizer
        if output_dir:
            print(f"\næ­¥éª¤ 5: ä¿å­˜ tokenizer åˆ° {output_dir}...")
            tokenizer.save_pretrained(output_dir)
            print(f"  âœ“ Tokenizer å·²ä¿å­˜åˆ° {output_dir}")
            print(f"    - tokenizer_config.json")
            print(f"    - sentencepiece.model")
            print(f"    - special_tokens_map.json")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if parquet_file and os.path.exists(input_file):
            os.unlink(input_file)
            print(f"\n  âœ“ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
        
        # æ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
        print("\n" + "="*60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"  - æ•°æ®é‡: {line_count:,} è¡Œ ({file_size:.2f} MB)")
        print(f"  - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        print(f"  - è®­ç»ƒè€—æ—¶: {elapsed_minutes} åˆ† {elapsed_seconds} ç§’")
        print(f"  - è¾“å‡ºç›®å½•: {output_dir}")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"  python quick_test_tokenizer.py {output_dir}")
        print("="*60 + "\n")
        
        return tokenizer
        
    except Exception as e:
        print(f"\nâš  è½¬æ¢ä¸º Hugging Face tokenizer å¤±è´¥: {e}")
        print(f"ä½† SentencePiece æ¨¡å‹å·²ä¿å­˜åˆ°: {model_prefix}.model")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if parquet_file and os.path.exists(input_file):
            os.unlink(input_file)
        
        return sp


def test_tokenizer(tokenizer, test_text: str = None):
    """æµ‹è¯• tokenizer çš„åŠŸèƒ½"""
    if test_text is None:
        test_text = 'è¿™æ˜¯ä¸€æ®µä¸­è‹±æ··è¾“çš„å¥å­, ï¼ˆchinese and English, here are words.ï¼‰'
    
    print("\n" + "="*60)
    print("æµ‹è¯• Tokenizer")
    print("="*60)
    print(f"æµ‹è¯•æ–‡æœ¬: {test_text}")
    
    # Tokenize
    tokens = tokenizer.tokenize(test_text)
    print(f"\nTokenize ç»“æœ ({len(tokens)} ä¸ª tokens):")
    print(tokens[:20])  # åªæ˜¾ç¤ºå‰20ä¸ª
    if len(tokens) > 20:
        print(f"... (å…± {len(tokens)} ä¸ª tokens)")
    
    # Encode
    ids = tokenizer.encode(test_text)
    print(f"\nEncode ç»“æœ ({len(ids)} ä¸ª IDs):")
    print(ids[:20])  # åªæ˜¾ç¤ºå‰20ä¸ª
    if len(ids) > 20:
        print(f"... (å…± {len(ids)} ä¸ª IDs)")
    
    # Decode
    decoded = tokenizer.decode(
        ids, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=True
    )
    print(f"\nDecode ç»“æœ:")
    print(decoded)
    
    # éªŒè¯
    if decoded.strip() == test_text.strip():
        print("\nâœ“ ç¼–ç è§£ç ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
    else:
        print("\nâš  ç¼–ç è§£ç ç»“æœç•¥æœ‰å·®å¼‚ï¼ˆå¯èƒ½æ˜¯ç©ºæ ¼å¤„ç†ï¼‰")


def main():
    parser = argparse.ArgumentParser(
        description='è®­ç»ƒä¸­æ–‡ Tokenizer',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æ–¹å¼1ï¼šåŸºäº T5-base è®­ç»ƒï¼ˆæ¨èï¼‰
  python train_tokenizer.py --method t5-base --wiki-file ../data/my_corpus_processed.txt --output-dir ../model_save/my_tokenizer_wiki
  
  # æ–¹å¼2ï¼šä»é›¶åˆ›å»ºå­—ç¬¦çº§åˆ« tokenizer
  python train_tokenizer.py --method char-bpe --wiki-file ../data/wiki.simple.txt --output-dir ../model_save/my_tokenizer_char
  
  # æ–¹å¼3ï¼šä»é›¶åˆ›å»ºå­—èŠ‚çº§åˆ« tokenizer
  python train_tokenizer.py --method byte-bpe --wiki-file ../data/wiki.simple.txt --output-dir ../model_save/my_tokenizer_byte
  
  # æ–¹å¼4ï¼šä½¿ç”¨ SentencePiece è®­ç»ƒï¼ˆç¨³å®šå¿«é€Ÿï¼‰
  python train_tokenizer.py --method sentencepiece --wiki-file ../data/wiki.simple.txt --output-dir ../model_save/my_tokenizer_sp
  
  # ä½¿ç”¨ Parquet æ–‡ä»¶ä½œä¸ºè¯­æ–™
  python train_tokenizer.py --method t5-base --parquet-file ../data/my_dataset.shuffle.parquet --output-dir ../model_save/my_tokenizer_wiki
  
  # SentencePiece ä½¿ç”¨ BPE æ¨¡å‹
  python train_tokenizer.py --method sentencepiece --wiki-file ../data/wiki.simple.txt --output-dir ../model_save/my_tokenizer_sp_bpe --sp-model-type bpe
        """
    )
    
    parser.add_argument(
        '--method',
        type=str,
        choices=['t5-base', 'char-bpe', 'byte-bpe', 'sentencepiece'],
        default='t5-base',
        help='è®­ç»ƒæ–¹æ³•: t5-base (æ¨è), char-bpe, byte-bpe, sentencepiece'
    )
    
    parser.add_argument(
        '--wiki-file',
        type=str,
        help='ç»´åŸºç™¾ç§‘æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆ.txt æ ¼å¼ï¼‰'
    )
    
    parser.add_argument(
        '--parquet-file',
        type=str,
        help='Parquet æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å« prompt å’Œ response åˆ—ï¼‰'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default=None,
        help='è¾“å‡ºç›®å½•ï¼ˆä¿å­˜è®­ç»ƒå¥½çš„ tokenizerï¼‰'
    )
    
    parser.add_argument(
        '--vocab-size',
        type=int,
        default=40960,
        help='è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤: 40960ï¼‰'
    )
    
    parser.add_argument(
        '--min-chunk-size',
        type=int,
        default=2048,
        help='æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å°å­—ç¬¦æ•°ï¼ˆä»…ç”¨äº wiki-fileï¼Œé»˜è®¤: 2048ï¼‰'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=1000,
        help='æ¯æ¬¡è¿­ä»£çš„æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤: 1000ï¼‰'
    )
    
    parser.add_argument(
        '--test',
        action='store_true',
        help='è®­ç»ƒå®Œæˆåæµ‹è¯• tokenizer'
    )
    
    parser.add_argument(
        '--sp-model-type',
        type=str,
        choices=['unigram', 'bpe', 'char', 'word'],
        default='unigram',
        help='SentencePiece æ¨¡å‹ç±»å‹ï¼ˆä»…ç”¨äº sentencepiece æ–¹æ³•ï¼Œé»˜è®¤: unigramï¼‰'
    )
    
    parser.add_argument(
        '--sp-character-coverage',
        type=float,
        default=0.9995,
        help='SentencePiece å­—ç¬¦è¦†ç›–ç‡ï¼ˆä»…ç”¨äº sentencepiece æ–¹æ³•ï¼Œé»˜è®¤: 0.9995ï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not args.wiki_file and not args.parquet_file:
        parser.error("å¿…é¡»æä¾› --wiki-file æˆ– --parquet-file ä¹‹ä¸€")
    
    if args.wiki_file and args.parquet_file:
        parser.error("ä¸èƒ½åŒæ—¶æä¾› --wiki-file å’Œ --parquet-file")
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    if args.output_dir is None:
        args.output_dir = os.path.join(PROJECT_ROOT, 'model_save', f'my_tokenizer_{args.method}')
    
    # è·å–è¯­æ–™è¿­ä»£å™¨
    try:
        if args.wiki_file:
            corpus_iterator = get_wiki_corpus_iterator(
                args.wiki_file,
                min_chunk_size=args.min_chunk_size,
                batch_size=args.batch_size
            )
        else:
            corpus_iterator = get_parquet_corpus_iterator(
                args.parquet_file,
                batch_size=args.batch_size
            )
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½è¯­æ–™: {e}", file=sys.stderr)
        sys.exit(1)
    
    # è®­ç»ƒ tokenizer
    try:
        if args.method == 't5-base':
            tokenizer = train_t5_base_tokenizer(
                corpus_iterator,
                vocab_size=args.vocab_size,
                output_dir=args.output_dir
            )
        elif args.method == 'char-bpe':
            tokenizer = train_char_level_tokenizer(
                corpus_iterator,
                vocab_size=args.vocab_size,
                output_dir=args.output_dir
            )
        elif args.method == 'byte-bpe':
            tokenizer = train_byte_level_tokenizer(
                corpus_iterator,
                vocab_size=args.vocab_size,
                output_dir=args.output_dir
            )
        elif args.method == 'sentencepiece':
            tokenizer = train_sentencepiece_tokenizer(
                wiki_file=args.wiki_file,
                parquet_file=args.parquet_file,
                vocab_size=args.vocab_size,
                output_dir=args.output_dir,
                model_type=args.sp_model_type,
                character_coverage=args.sp_character_coverage
            )
        
        # æµ‹è¯• tokenizer
        if args.test:
            test_tokenizer(tokenizer)
        
        print("\n" + "="*60)
        print("è®­ç»ƒå®Œæˆï¼")
        print(f"Tokenizer å·²ä¿å­˜åˆ°: {args.output_dir}")
        print("="*60)
        
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
        sys.exit(1)
    except Exception as e:
        print(f"\né”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
