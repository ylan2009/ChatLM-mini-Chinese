# SentencePiece è®­ç»ƒé”™è¯¯è§£å†³æ–¹æ¡ˆ

## âŒ é”™è¯¯ä¿¡æ¯

```
unigram_model_trainer.cc(208) [array.size() <= (static_cast<size_t>(std::numeric_limits<node_int_type>::max()))] 
Input corpus too large, try with train_extremely_large_corpus=true
Program terminated with an unrecoverable error.
```

---

## ğŸ” é—®é¢˜åŸå› 

**é”™è¯¯åŸå› **ï¼šè®­ç»ƒæ•°æ®å¤ªå¤§ï¼Œè¶…è¿‡äº† SentencePiece é»˜è®¤æ¨¡å¼çš„å¤„ç†èƒ½åŠ›

**è§¦å‘æ¡ä»¶**ï¼š
- è®­ç»ƒæ•°æ® > 5GB
- æˆ–è€…å¥å­æ•°é‡ > 100 ä¸‡è¡Œ
- æˆ–è€…æ€»å­—ç¬¦æ•° > 10 äº¿

**ä½ çš„æ•°æ®æƒ…å†µ**ï¼š
- æ–‡ä»¶å¤§å°ï¼š7.3GB
- è¡Œæ•°ï¼š1,276,893 è¡Œ
- æ€»å­—ç¬¦æ•°ï¼šçº¦ 28 äº¿ï¼ˆä»æ—¥å¿—çœ‹åˆ° `all chars count=2841922986`ï¼‰

---

## âœ… è§£å†³æ–¹æ¡ˆ

### **æ–¹æ¡ˆ 1ï¼šå¯ç”¨å¤§è¯­æ–™åº“è®­ç»ƒæ¨¡å¼ï¼ˆå·²ä¿®å¤ï¼‰**

**ä¿®æ”¹å†…å®¹**ï¼š
åœ¨ `train_tokenizer.py` çš„ç¬¬ 148 è¡Œï¼Œå°†ï¼š
```python
'--train_extremely_large_corpus=false',
```

æ”¹ä¸ºï¼š
```python
'--train_extremely_large_corpus=true',  # å¯ç”¨å¤§è¯­æ–™åº“è®­ç»ƒæ¨¡å¼
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¯ä»¥å¤„ç†ä»»æ„å¤§å°çš„è¯­æ–™åº“
- âœ… ä¸éœ€è¦ä¿®æ”¹æ•°æ®
- âœ… è®­ç»ƒè´¨é‡ä¸å—å½±å“

**ç¼ºç‚¹**ï¼š
- âš ï¸ è®­ç»ƒæ—¶é—´ä¼šæ›´é•¿ï¼ˆå¯èƒ½éœ€è¦ 30-60 åˆ†é’Ÿï¼‰
- âš ï¸ å†…å­˜å ç”¨ä¼šæ›´é«˜ï¼ˆå»ºè®® 16GB+ å†…å­˜ï¼‰

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

# ç°åœ¨å¯ä»¥ç›´æ¥è®­ç»ƒå¤§è¯­æ–™åº“äº†
python train_tokenizer.py \
  --method sentencepiece \
  --wiki-file ../data/my_corpus_clean.txt \
  --output-dir ../model_save/my_tokenizer_sp \
  --vocab-size 40960
```

---

### **æ–¹æ¡ˆ 2ï¼šå‡å°‘è®­ç»ƒæ•°æ®ï¼ˆå¿«é€Ÿæ–¹æ¡ˆï¼‰**

å¦‚æœä½ æƒ³å¿«é€Ÿè®­ç»ƒï¼Œå¯ä»¥ä½¿ç”¨éƒ¨åˆ†æ•°æ®ï¼š

```bash
# ä½¿ç”¨å‰ 50 ä¸‡è¡Œï¼ˆçº¦ 3GBï¼‰
head -n 500000 /Users/twrong/git/code/ChatLM-mini-Chinese/data/my_corpus_clean.txt > \
  /Users/twrong/git/code/ChatLM-mini-Chinese/data/my_corpus_500k.txt

# è®­ç»ƒï¼ˆ10-20 åˆ†é’Ÿï¼‰
python train_tokenizer.py \
  --method sentencepiece \
  --wiki-file ../data/my_corpus_500k.txt \
  --output-dir ../model_save/my_tokenizer_sp_500k \
  --vocab-size 40960
```

**ä¼˜ç‚¹**ï¼š
- âœ… è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆ10-20 åˆ†é’Ÿï¼‰
- âœ… å†…å­˜å ç”¨ä½ï¼ˆ< 8GBï¼‰
- âœ… å¯¹äº tokenizer è®­ç»ƒï¼Œ50 ä¸‡è¡Œå·²ç»è¶³å¤Ÿ

**ç¼ºç‚¹**ï¼š
- âš ï¸ è¯æ±‡è¦†ç›–å¯èƒ½ç•¥ä½ï¼ˆä½†å½±å“ä¸å¤§ï¼‰

---

### **æ–¹æ¡ˆ 3ï¼šä½¿ç”¨é‡‡æ ·æ•°æ®**

éšæœºé‡‡æ ·ï¼Œä¿è¯æ•°æ®å¤šæ ·æ€§ï¼š

```bash
# éšæœºé‡‡æ · 50 ä¸‡è¡Œ
shuf /Users/twrong/git/code/ChatLM-mini-Chinese/data/my_corpus_clean.txt | \
  head -n 500000 > \
  /Users/twrong/git/code/ChatLM-mini-Chinese/data/my_corpus_sampled.txt

# è®­ç»ƒ
python train_tokenizer.py \
  --method sentencepiece \
  --wiki-file ../data/my_corpus_sampled.txt \
  --output-dir ../model_save/my_tokenizer_sp_sampled \
  --vocab-size 40960
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ•°æ®å¤šæ ·æ€§æ›´å¥½
- âœ… è®­ç»ƒé€Ÿåº¦å¿«
- âœ… è¯æ±‡è¦†ç›–æ›´å…¨é¢

---

### **æ–¹æ¡ˆ 4ï¼šè°ƒæ•´ SentencePiece å‚æ•°**

å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥è°ƒæ•´å‚æ•°ï¼š

ä¿®æ”¹ `train_tokenizer.py`ï¼Œæ·»åŠ ä»¥ä¸‹å‚æ•°ï¼š

```python
train_args = [
    # ... å…¶ä»–å‚æ•° ...
    '--train_extremely_large_corpus=true',
    '--input_sentence_size=2000000',  # é™åˆ¶è¾“å…¥å¥å­æ•°é‡
    '--shuffle_input_sentence=true',  # éšæœºæ‰“ä¹±è¾“å…¥
    '--num_threads=8',  # å‡å°‘çº¿ç¨‹æ•°ï¼ˆå¦‚æœå†…å­˜ä¸è¶³ï¼‰
]
```

---

## ğŸ“Š ä¸åŒæ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | è®­ç»ƒæ—¶é—´ | å†…å­˜éœ€æ±‚ | è¯æ±‡è¦†ç›– | æ¨èåº¦ |
|------|---------|---------|---------|--------|
| æ–¹æ¡ˆ 1ï¼šå¤§è¯­æ–™åº“æ¨¡å¼ | 30-60 åˆ†é’Ÿ | 16GB+ | æœ€å¥½ | â­â­â­â­â­ |
| æ–¹æ¡ˆ 2ï¼šå‰ 50 ä¸‡è¡Œ | 10-20 åˆ†é’Ÿ | 8GB | å¾ˆå¥½ | â­â­â­â­ |
| æ–¹æ¡ˆ 3ï¼šéšæœºé‡‡æ · | 10-20 åˆ†é’Ÿ | 8GB | å¾ˆå¥½ | â­â­â­â­â­ |
| æ–¹æ¡ˆ 4ï¼šè°ƒæ•´å‚æ•° | 20-40 åˆ†é’Ÿ | 12GB | å¥½ | â­â­â­ |

---

## ğŸš€ æ¨èæ‰§è¡Œæµç¨‹

### **å¦‚æœä½ çš„å†…å­˜ >= 16GBï¼ˆæ¨èï¼‰**

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

# ç›´æ¥ä½¿ç”¨æ–¹æ¡ˆ 1ï¼šå¤§è¯­æ–™åº“æ¨¡å¼
python train_tokenizer.py \
  --method sentencepiece \
  --wiki-file ../data/my_corpus_clean.txt \
  --output-dir ../model_save/my_tokenizer_sp \
  --vocab-size 40960

# é¢„è®¡æ—¶é—´ï¼š30-60 åˆ†é’Ÿ
# å†…å­˜å ç”¨ï¼š16-24GB
```

---

### **å¦‚æœä½ çš„å†…å­˜ < 16GB æˆ–æƒ³å¿«é€Ÿè®­ç»ƒ**

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

# æ–¹æ¡ˆ 3ï¼šéšæœºé‡‡æ ·ï¼ˆæ¨èï¼‰
shuf ../data/my_corpus_clean.txt | head -n 500000 > ../data/my_corpus_sampled.txt

python train_tokenizer.py \
  --method sentencepiece \
  --wiki-file ../data/my_corpus_sampled.txt \
  --output-dir ../model_save/my_tokenizer_sp_sampled \
  --vocab-size 40960

# é¢„è®¡æ—¶é—´ï¼š10-20 åˆ†é’Ÿ
# å†…å­˜å ç”¨ï¼š6-8GB
```

---

## ğŸ’¡ è®­ç»ƒæ•°æ®é‡å»ºè®®

å¯¹äº tokenizer è®­ç»ƒï¼Œ**ä¸æ˜¯æ•°æ®è¶Šå¤šè¶Šå¥½**ï¼

### **æ¨èæ•°æ®é‡**

| æ•°æ®é‡ | è¡Œæ•° | æ–‡ä»¶å¤§å° | è®­ç»ƒæ—¶é—´ | æ•ˆæœ |
|--------|------|---------|---------|------|
| å°è§„æ¨¡ | 10 ä¸‡ | 500MB | 5 åˆ†é’Ÿ | ä¸€èˆ¬ |
| ä¸­è§„æ¨¡ | 50 ä¸‡ | 3GB | 15 åˆ†é’Ÿ | å¾ˆå¥½ âœ… |
| å¤§è§„æ¨¡ | 100 ä¸‡ | 6GB | 30 åˆ†é’Ÿ | å¾ˆå¥½ âœ… |
| è¶…å¤§è§„æ¨¡ | 200 ä¸‡+ | 12GB+ | 60 åˆ†é’Ÿ+ | ç•¥å¥½ |

**ç»“è®º**ï¼š
- âœ… **50 ä¸‡è¡Œï¼ˆ3GBï¼‰æ˜¯æ€§ä»·æ¯”æœ€é«˜çš„é€‰æ‹©**
- âœ… 100 ä¸‡è¡Œå·²ç»è¶³å¤Ÿè¦†ç›–ç»å¤§éƒ¨åˆ†è¯æ±‡
- âš ï¸ è¶…è¿‡ 100 ä¸‡è¡Œï¼Œæ”¶ç›Šé€’å‡

---

## ğŸ”§ å…¶ä»–ä¼˜åŒ–å»ºè®®

### 1. **ä½¿ç”¨ BPE æ¨¡å‹ï¼ˆæ›´å¿«ï¼‰**

```bash
python train_tokenizer.py \
  --method sentencepiece \
  --sp-model-type bpe \  # ä½¿ç”¨ BPE è€Œä¸æ˜¯ unigram
  --wiki-file ../data/my_corpus_sampled.txt \
  --output-dir ../model_save/my_tokenizer_sp_bpe \
  --vocab-size 40960
```

**ä¼˜ç‚¹**ï¼š
- âœ… è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼ˆå¿« 30-50%ï¼‰
- âœ… å†…å­˜å ç”¨æ›´ä½

**ç¼ºç‚¹**ï¼š
- âš ï¸ æ•ˆæœå¯èƒ½ç•¥å·®äº unigram

---

### 2. **å‡å°‘è¯æ±‡è¡¨å¤§å°**

```bash
python train_tokenizer.py \
  --method sentencepiece \
  --wiki-file ../data/my_corpus_clean.txt \
  --output-dir ../model_save/my_tokenizer_sp_32k \
  --vocab-size 32000  # å‡å°‘åˆ° 32000
```

**ä¼˜ç‚¹**ï¼š
- âœ… è®­ç»ƒé€Ÿåº¦æ›´å¿«
- âœ… æ¨¡å‹å‚æ•°æ›´å°‘

**ç¼ºç‚¹**ï¼š
- âš ï¸ å‹ç¼©ç‡å¯èƒ½ç•¥ä½

---

### 3. **è°ƒæ•´å­—ç¬¦è¦†ç›–ç‡**

```bash
python train_tokenizer.py \
  --method sentencepiece \
  --wiki-file ../data/my_corpus_clean.txt \
  --output-dir ../model_save/my_tokenizer_sp \
  --vocab-size 40960 \
  --sp-character-coverage 0.995  # é™ä½åˆ° 0.995ï¼ˆé»˜è®¤ 0.9995ï¼‰
```

**ä¼˜ç‚¹**ï¼š
- âœ… è®­ç»ƒé€Ÿåº¦æ›´å¿«
- âœ… è¿‡æ»¤æ‰æä½é¢‘å­—ç¬¦

**ç¼ºç‚¹**ï¼š
- âš ï¸ å¯èƒ½å¢åŠ  UNK token

---

## ğŸ“‹ å®Œæ•´è®­ç»ƒå‘½ä»¤

### **æ¨èå‘½ä»¤ï¼ˆæ–¹æ¡ˆ 1 + ä¼˜åŒ–ï¼‰**

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

# å¦‚æœå†…å­˜å……è¶³ï¼ˆ>= 16GBï¼‰ï¼Œä½¿ç”¨å®Œæ•´æ•°æ®
python train_tokenizer.py \
  --method sentencepiece \
  --sp-model-type unigram \
  --wiki-file ../data/my_corpus_clean.txt \
  --output-dir ../model_save/my_tokenizer_sp_full \
  --vocab-size 40960 \
  --sp-character-coverage 0.9995

# é¢„è®¡æ—¶é—´ï¼š30-60 åˆ†é’Ÿ
```

### **æ¨èå‘½ä»¤ï¼ˆæ–¹æ¡ˆ 3 + ä¼˜åŒ–ï¼‰**

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

# å¦‚æœå†…å­˜æœ‰é™æˆ–æƒ³å¿«é€Ÿè®­ç»ƒï¼Œä½¿ç”¨é‡‡æ ·æ•°æ®
shuf ../data/my_corpus_clean.txt | head -n 500000 > ../data/my_corpus_sampled.txt

python train_tokenizer.py \
  --method sentencepiece \
  --sp-model-type unigram \
  --wiki-file ../data/my_corpus_sampled.txt \
  --output-dir ../model_save/my_tokenizer_sp_sampled \
  --vocab-size 40960 \
  --sp-character-coverage 0.9995

# é¢„è®¡æ—¶é—´ï¼š10-20 åˆ†é’Ÿ
```

---

## âœ… éªŒè¯è®­ç»ƒç»“æœ

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨è¯„ä¼°å·¥å…·éªŒè¯ï¼š

```bash
# å¿«é€Ÿæµ‹è¯•
python quick_test_tokenizer.py ../model_save/my_tokenizer_sp_sampled

# å®Œæ•´è¯„ä¼°
python evaluate_tokenizer.py \
  --tokenizer-dir ../model_save/my_tokenizer_sp_sampled \
  --verbose
```

---

## ğŸ¯ æ€»ç»“

1. **å·²ä¿®å¤**ï¼š`train_tokenizer.py` å·²å¯ç”¨ `train_extremely_large_corpus=true`
2. **æ¨èæ–¹æ¡ˆ**ï¼š
   - å†…å­˜å……è¶³ï¼ˆ>= 16GBï¼‰â†’ ä½¿ç”¨å®Œæ•´æ•°æ®ï¼ˆæ–¹æ¡ˆ 1ï¼‰
   - å†…å­˜æœ‰é™æˆ–å¿«é€Ÿè®­ç»ƒ â†’ ä½¿ç”¨é‡‡æ ·æ•°æ®ï¼ˆæ–¹æ¡ˆ 3ï¼‰
3. **æœ€ä½³å®è·µ**ï¼š50 ä¸‡è¡Œé‡‡æ ·æ•°æ®æ˜¯æ€§ä»·æ¯”æœ€é«˜çš„é€‰æ‹©
4. **è®­ç»ƒå**ï¼šä½¿ç”¨è¯„ä¼°å·¥å…·éªŒè¯è´¨é‡

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [TOKENIZER_EVALUATION_GUIDE.md](./TOKENIZER_EVALUATION_GUIDE.md) - è¯„ä¼°æŒ‡å—
- [train_tokenizer.py](./train_tokenizer.py) - è®­ç»ƒè„šæœ¬
- [evaluate_tokenizer.py](./evaluate_tokenizer.py) - è¯„ä¼°å·¥å…·

---

**ç°åœ¨å¯ä»¥é‡æ–°è¿è¡Œè®­ç»ƒå‘½ä»¤äº†ï¼** ğŸš€
