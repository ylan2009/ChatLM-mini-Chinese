#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæµ‹è¯• Tokenizer

å¿«é€Ÿæ£€æŸ¥ tokenizer çš„åŸºæœ¬åŠŸèƒ½å’Œè´¨é‡

ä½¿ç”¨æ–¹æ³•ï¼š
    python quick_test_tokenizer.py ../model_save/my_tokenizer_wiki
"""

import sys
import os


def quick_test(tokenizer_dir: str):
    """å¿«é€Ÿæµ‹è¯• tokenizer"""
    
    # å¯¼å…¥
    try:
        from transformers import AutoTokenizer
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… transformers: pip install transformers")
        return False
    
    # åŠ è½½
    print(f"\n{'='*60}")
    print(f"ğŸ” å¿«é€Ÿæµ‹è¯•: {os.path.basename(tokenizer_dir)}")
    print(f"{'='*60}\n")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return False
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ“š åŸºæœ¬ä¿¡æ¯:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {len(tokenizer):,}")
    print(f"  PAD token: {tokenizer.pad_token}")
    print(f"  UNK token: {tokenizer.unk_token}")
    print(f"  EOS token: {getattr(tokenizer, 'eos_token', 'N/A')}")
    
    # æµ‹è¯•æ ·æœ¬
    test_cases = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        "Machine learning is a subset of AI",
        "ä½¿ç”¨ Python è¿›è¡Œ AI å¼€å‘",
        "2024å¹´ï¼ŒAIå¸‚åœºè§„æ¨¡è¾¾åˆ°5000äº¿ç¾å…ƒ",
    ]
    
    print(f"\nğŸ§ª æµ‹è¯•æ ·æœ¬:")
    
    total_chars = 0
    total_tokens = 0
    total_unk = 0
    
    for i, text in enumerate(test_cases, 1):
        tokens = tokenizer.tokenize(text)
        token_ids = tokenizer.encode(text, add_special_tokens=False)
        decoded = tokenizer.decode(token_ids)
        
        num_chars = len(text)
        num_tokens = len(tokens)
        num_unk = sum(1 for t in tokens if '[UNK]' in str(t) or t == tokenizer.unk_token)
        compression = num_chars / num_tokens if num_tokens > 0 else 0
        
        total_chars += num_chars
        total_tokens += num_tokens
        total_unk += num_unk
        
        print(f"\n  [{i}] {text}")
        print(f"      Tokens ({num_tokens}): {' | '.join(tokens[:8])}{'...' if len(tokens) > 8 else ''}")
        print(f"      å‹ç¼©ç‡: {compression:.2f}, UNK: {num_unk}")
        
        # æ£€æŸ¥å¯é€†æ€§
        if decoded.replace(' ', '') != text.replace(' ', ''):
            print(f"      âš ï¸  è§£ç ä¸ä¸€è‡´: {decoded}")
    
    # æ€»ä½“è¯„ä¼°
    avg_compression = total_chars / total_tokens if total_tokens > 0 else 0
    unk_ratio = total_unk / total_tokens if total_tokens > 0 else 0
    
    print(f"\nğŸ“Š æ€»ä½“è¯„ä¼°:")
    print(f"  å¹³å‡å‹ç¼©ç‡: {avg_compression:.2f} å­—ç¬¦/token")
    print(f"  æœªçŸ¥è¯æ¯”ä¾‹: {unk_ratio*100:.2f}%")
    
    # è¯„åˆ†
    if 2.0 <= avg_compression <= 3.0:
        comp_status = "âœ… ä¼˜ç§€"
    elif 1.5 <= avg_compression < 2.0 or 3.0 < avg_compression <= 3.5:
        comp_status = "âœ… è‰¯å¥½"
    else:
        comp_status = "âš ï¸  éœ€è¦æ”¹è¿›"
    
    if unk_ratio < 0.01:
        unk_status = "âœ… ä¼˜ç§€"
    elif unk_ratio < 0.05:
        unk_status = "âœ… è‰¯å¥½"
    else:
        unk_status = "âš ï¸  éœ€è¦æ”¹è¿›"
    
    print(f"  å‹ç¼©ç‡è¯„ä»·: {comp_status}")
    print(f"  æœªçŸ¥è¯è¯„ä»·: {unk_status}")
    
    # ç»¼åˆè¯„åˆ†
    if "ä¼˜ç§€" in comp_status and "ä¼˜ç§€" in unk_status:
        print(f"\nğŸ‰ ç»¼åˆè¯„ä»·: ä¼˜ç§€ï¼å¯ä»¥ä½¿ç”¨")
    elif "è‰¯å¥½" in comp_status or "è‰¯å¥½" in unk_status:
        print(f"\nâœ… ç»¼åˆè¯„ä»·: è‰¯å¥½ï¼Œå¯ä»¥ä½¿ç”¨")
    else:
        print(f"\nâš ï¸  ç»¼åˆè¯„ä»·: å»ºè®®ä¼˜åŒ–æˆ–é‡æ–°è®­ç»ƒ")
    
    print(f"\nğŸ’¡ æç¤º: è¿è¡Œå®Œæ•´è¯„ä¼°è·å–è¯¦ç»†æŠ¥å‘Š:")
    print(f"   python evaluate_tokenizer.py --tokenizer-dir {tokenizer_dir} --verbose")
    print()
    
    return True


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python quick_test_tokenizer.py <tokenizer_dir>")
        print("ç¤ºä¾‹: python quick_test_tokenizer.py ../model_save/my_tokenizer_wiki")
        sys.exit(1)
    
    tokenizer_dir = sys.argv[1]
    
    if not os.path.exists(tokenizer_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {tokenizer_dir}")
        sys.exit(1)
    
    success = quick_test(tokenizer_dir)
    sys.exit(0 if success else 1)
