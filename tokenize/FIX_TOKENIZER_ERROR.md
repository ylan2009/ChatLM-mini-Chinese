# ğŸ”§ Tokenizer è®­ç»ƒé”™è¯¯ä¿®å¤æŒ‡å—

## ğŸ” é—®é¢˜æè¿°

ä½¿ç”¨ `train_tokenizer.py` è®­ç»ƒ tokenizer æ—¶å‡ºç° Rust panic é”™è¯¯ï¼š

```
thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/models/unigram/trainer.rs:228:53:
called `Result::unwrap()` on an `Err` value: Internal
```

**è¿™æ˜¯ tokenizers åº“çš„ä¸€ä¸ªå·²çŸ¥ Bugï¼**

---

## ğŸ¯ é—®é¢˜åŸå› 

### å¯èƒ½çš„åŸå› 

1. **æ•°æ®ä¸­æœ‰ç©ºè¡Œæˆ–æ— æ•ˆå­—ç¬¦**
   - å³ä½¿ä½¿ç”¨ `clean_corpus.py` æ¸…æ´—åï¼Œæ•°æ®å¯èƒ½è¿˜æœ‰é—®é¢˜
   - tokenizers åº“å¯¹æŸäº›ç‰¹æ®Šå­—ç¬¦éå¸¸æ•æ„Ÿ

2. **æ•°æ®æ ¼å¼ä¸æ­£ç¡®**
   - åŒ…å« `[SEP]` ç­‰ç‰¹æ®Šæ ‡è®°
   - è¡Œé•¿åº¦ä¸ä¸€è‡´
   - ç¼–ç é—®é¢˜

3. **æ•°æ®é‡é—®é¢˜**
   - æ•°æ®é‡å¤ªå°‘ï¼ˆ< 1000 è¡Œï¼‰
   - æ•°æ®é‡å¤ªå¤§å¯¼è‡´å†…å­˜ä¸è¶³

4. **tokenizers åº“ç‰ˆæœ¬é—®é¢˜**
   - æŸäº›ç‰ˆæœ¬çš„ tokenizers åº“æœ‰ Bug
   - ä¸ Python ç‰ˆæœ¬ä¸å…¼å®¹

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä½¿ç”¨é¢„å¤„ç†è„šæœ¬ï¼ˆæ¨èï¼‰

æˆ‘åˆ›å»ºäº†ä¸€ä¸ªä¸“é—¨çš„ä¿®å¤è„šæœ¬ `fix_tokenizer_training.py`ï¼š

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

# æ­¥éª¤ 1ï¼šé¢„å¤„ç†æ•°æ®
python fix_tokenizer_training.py \
  --input ../data/wiki.simple.txt \
  --output ../data/wiki_clean.txt

# æ­¥éª¤ 2ï¼šè®­ç»ƒ tokenizer
python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki_clean.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 40960 \
  --batch-size 500
```

**é¢„å¤„ç†è„šæœ¬çš„åŠŸèƒ½**ï¼š
- âœ… å»é™¤ç©ºè¡Œ
- âœ… å»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
- âœ… å»é™¤å¤šä½™ç©ºæ ¼
- âœ… è¿‡æ»¤æ— æ•ˆè¡Œï¼ˆå¤ªçŸ­æˆ–ä¸åŒ…å«æœ‰æ•ˆå­—ç¬¦ï¼‰
- âœ… ç»Ÿä¸€ç¼–ç ä¸º UTF-8

---

### æ–¹æ¡ˆ 2ï¼šç›´æ¥ä½¿ç”¨ç»´åŸºç™¾ç§‘æ•°æ®

ä¸è¦ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹çš„ç»´åŸºç™¾ç§‘æ•°æ®ï¼š

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki.simple.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 40960 \
  --batch-size 500
```

**ä¸ºä»€ä¹ˆè¿™æ ·å¯èƒ½æœ‰æ•ˆ**ï¼š
- ç»´åŸºç™¾ç§‘æ•°æ®å·²ç»æ˜¯é«˜è´¨é‡çš„çº¯æ–‡æœ¬
- ä¸éœ€è¦é¢å¤–æ¸…æ´—
- æ ¼å¼ç»Ÿä¸€ï¼Œæ²¡æœ‰ç‰¹æ®Šæ ‡è®°

---

### æ–¹æ¡ˆ 3ï¼šè°ƒæ•´è®­ç»ƒå‚æ•°

å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½ä¸è¡Œï¼Œå°è¯•è°ƒæ•´è®­ç»ƒå‚æ•°ï¼š

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki.simple.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 32000 \
  --batch-size 1000 \
  --min-frequency 2
```

**å‚æ•°è¯´æ˜**ï¼š
- `--vocab-size 32000`ï¼šå‡å°è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤ 40960ï¼‰
- `--batch-size 1000`ï¼šå¢å¤§æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ 500ï¼‰
- `--min-frequency 2`ï¼šå¢åŠ æœ€å°è¯é¢‘ï¼ˆè¿‡æ»¤ä½é¢‘è¯ï¼‰

---

### æ–¹æ¡ˆ 4ï¼šä½¿ç”¨ SentencePiece æ–¹æ³•

å¦‚æœ T5-base æ–¹æ³•ä¸è¡Œï¼Œå°è¯•ä½¿ç”¨ SentencePieceï¼š

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

python train_tokenizer.py \
  --method sentencepiece \
  --wiki-file ../data/wiki.simple.txt \
  --output-dir ../model_save/my_tokenizer_sp \
  --vocab-size 40960
```

---

## ğŸš€ æ¨èæµç¨‹

### å®Œæ•´çš„è®­ç»ƒæµç¨‹

#### æ­¥éª¤ 1ï¼šé¢„å¤„ç†æ•°æ®

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

# é¢„å¤„ç† wiki.simple.txt
python fix_tokenizer_training.py \
  --input ../data/wiki.simple.txt \
  --output ../data/wiki_clean.txt \
  --min-length 5
```

**é¢„æœŸè¾“å‡º**ï¼š
```
ğŸ“– è¯»å–è¾“å…¥æ–‡ä»¶: ../data/wiki.simple.txt
ğŸ“Š æ–‡ä»¶å¤§å°: 1100.00 MB
ğŸ§¹ é¢„å¤„ç†æ–‡æœ¬...
å¤„ç†è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5234567/5234567
âœ… æœ‰æ•ˆè¡Œæ•°: 5,123,456
âŒ æ— æ•ˆè¡Œæ•°: 111,111
ğŸ“‰ è¿‡æ»¤ç‡: 2.12%
âœ… è¾“å‡ºæ–‡ä»¶å¤§å°: 1050.00 MB
ğŸ‰ é¢„å¤„ç†å®Œæˆï¼
```

#### æ­¥éª¤ 2ï¼šè®­ç»ƒ Tokenizer

```bash
python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki_clean.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 40960 \
  --batch-size 500
```

**é¢„æœŸè¾“å‡º**ï¼š
```
æ–¹æ³•: åŸºäº T5-base tokenizer è®­ç»ƒ
åŠ è½½ç»´åŸºç™¾ç§‘è¯­æ–™: ../data/wiki_clean.txt
æ€»è¡Œæ•°: 5123456

æ­¥éª¤ 1: åŠ è½½ T5-base tokenizer...
æ­¥éª¤ 2: å¼€å§‹è®­ç»ƒ tokenizer (è¯æ±‡è¡¨å¤§å°: 40960)...
æ³¨æ„: è¿™æ˜¯ CPU å¯†é›†å‹ä»»åŠ¡ï¼Œå¯èƒ½éœ€è¦é•¿æ—¶é—´ (çº¦1å°æ—¶)
å¤„ç†è¯­æ–™: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5123456/5123456
T5-base tokenizer åŠ è½½å®Œæˆ

æ­¥éª¤ 3: ä¿å­˜ tokenizer...
âœ… Tokenizer å·²ä¿å­˜åˆ°: ../model_save/my_tokenizer_wiki
ğŸ‰ è®­ç»ƒå®Œæˆï¼
```

#### æ­¥éª¤ 3ï¼šéªŒè¯ç»“æœ

```bash
# æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
ls -lh ../model_save/my_tokenizer_wiki/

# æµ‹è¯• tokenizer
python -c "
from transformers import T5Tokenizer
tokenizer = T5Tokenizer.from_pretrained('../model_save/my_tokenizer_wiki')
text = 'ä¸­å›½æ˜¯ä¸€ä¸ªä¼Ÿå¤§çš„å›½å®¶'
tokens = tokenizer.tokenize(text)
print(f'æ–‡æœ¬: {text}')
print(f'Tokens: {tokens}')
print(f'Token IDs: {tokenizer.convert_tokens_to_ids(tokens)}')
"
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ 1ï¼šä¾ç„¶æŠ¥ Rust panic é”™è¯¯

**è§£å†³æ–¹æ¡ˆ Aï¼šä½¿ç”¨æ›´å°çš„æ•°æ®é›†æµ‹è¯•**

```bash
# åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆå‰ 10000 è¡Œï¼‰
head -10000 ../data/wiki.simple.txt > ../data/test_wiki.txt

# é¢„å¤„ç†
python fix_tokenizer_training.py \
  --input ../data/test_wiki.txt \
  --output ../data/test_wiki_clean.txt

# è®­ç»ƒ
python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/test_wiki_clean.txt \
  --output-dir ../model_save/test_tokenizer \
  --vocab-size 10000
```

**è§£å†³æ–¹æ¡ˆ Bï¼šæ£€æŸ¥æ•°æ®ç¼–ç **

```bash
# æ£€æŸ¥æ–‡ä»¶ç¼–ç 
file -I ../data/wiki.simple.txt

# è½¬æ¢ç¼–ç ï¼ˆå¦‚æœéœ€è¦ï¼‰
iconv -f GBK -t UTF-8 ../data/wiki.simple.txt > ../data/wiki_utf8.txt
```

**è§£å†³æ–¹æ¡ˆ Cï¼šå‡çº§ tokenizers åº“**

```bash
pip install --upgrade tokenizers transformers
```

---

### é—®é¢˜ 2ï¼šå†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°
python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki_clean.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 40960 \
  --batch-size 100  # å‡å°æ‰¹æ¬¡å¤§å°
```

---

### é—®é¢˜ 3ï¼šè®­ç»ƒé€Ÿåº¦å¤ªæ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# å¢å¤§æ‰¹æ¬¡å¤§å°
python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki_clean.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 40960 \
  --batch-size 2000  # å¢å¤§æ‰¹æ¬¡å¤§å°
```

---

### é—®é¢˜ 4ï¼šè¯æ±‡è¡¨å¤ªå¤§

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# å‡å°è¯æ±‡è¡¨å¤§å°
python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki_clean.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 32000  # å‡å°è¯æ±‡è¡¨
```

---

## ğŸ“Š ä¸åŒæ–¹æ³•å¯¹æ¯”

### T5-base vs SentencePiece

| ç‰¹æ€§ | T5-base | SentencePiece |
|------|---------|---------------|
| **è®­ç»ƒé€Ÿåº¦** | æ…¢ | å¿« |
| **å†…å­˜å ç”¨** | é«˜ | ä½ |
| **è¯æ±‡è¡¨è´¨é‡** | é«˜ | ä¸­ |
| **ç¨³å®šæ€§** | ä¸­ï¼ˆå¯èƒ½æœ‰ Bugï¼‰ | é«˜ |
| **æ¨èåœºæ™¯** | é«˜è´¨é‡è®­ç»ƒ | å¿«é€Ÿæµ‹è¯• |

### æ¨èé€‰æ‹©

1. **é¦–é€‰**ï¼šT5-base + é¢„å¤„ç†æ•°æ®
   ```bash
   python fix_tokenizer_training.py --input ../data/wiki.simple.txt --output ../data/wiki_clean.txt
   python train_tokenizer.py --method t5-base --wiki-file ../data/wiki_clean.txt --output-dir ../model_save/my_tokenizer_wiki
   ```

2. **å¤‡é€‰**ï¼šSentencePiece + åŸå§‹æ•°æ®
   ```bash
   python train_tokenizer.py --method sentencepiece --wiki-file ../data/wiki.simple.txt --output-dir ../model_save/my_tokenizer_sp
   ```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ•°æ®å‡†å¤‡

- âœ… ä½¿ç”¨é«˜è´¨é‡çš„çº¯æ–‡æœ¬æ•°æ®ï¼ˆå¦‚ç»´åŸºç™¾ç§‘ï¼‰
- âœ… é¢„å¤„ç†æ•°æ®ï¼Œå»é™¤ç©ºè¡Œå’Œæ— æ•ˆå­—ç¬¦
- âœ… ç¡®ä¿æ•°æ®ç¼–ç ä¸º UTF-8
- âœ… æ•°æ®é‡è‡³å°‘ 100MBï¼ˆæ¨è 1GB+ï¼‰

### 2. å‚æ•°é€‰æ‹©

- âœ… è¯æ±‡è¡¨å¤§å°ï¼š32000-50000ï¼ˆä¸­æ–‡æ¨è 40960ï¼‰
- âœ… æ‰¹æ¬¡å¤§å°ï¼š500-2000ï¼ˆæ ¹æ®å†…å­˜è°ƒæ•´ï¼‰
- âœ… æœ€å°è¯é¢‘ï¼š1-3ï¼ˆè¿‡æ»¤ä½é¢‘è¯ï¼‰

### 3. è®­ç»ƒç¯å¢ƒ

- âœ… ä½¿ç”¨ SSD ç¡¬ç›˜ï¼ˆæå‡ I/O é€Ÿåº¦ï¼‰
- âœ… è‡³å°‘ 8GB å†…å­˜ï¼ˆæ¨è 16GB+ï¼‰
- âœ… å¤šæ ¸ CPUï¼ˆè®­ç»ƒæ˜¯ CPU å¯†é›†å‹ï¼‰

---

## ğŸ¯ æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Rust panic é”™è¯¯é€šå¸¸æ˜¯æ•°æ®é—®é¢˜**
   - ä½¿ç”¨ `fix_tokenizer_training.py` é¢„å¤„ç†æ•°æ®
   - æˆ–ç›´æ¥ä½¿ç”¨é«˜è´¨é‡çš„ç»´åŸºç™¾ç§‘æ•°æ®

2. **æ¨èæµç¨‹**
   ```bash
   # é¢„å¤„ç†
   python fix_tokenizer_training.py --input ../data/wiki.simple.txt --output ../data/wiki_clean.txt
   
   # è®­ç»ƒ
   python train_tokenizer.py --method t5-base --wiki-file ../data/wiki_clean.txt --output-dir ../model_save/my_tokenizer_wiki
   ```

3. **å¦‚æœè¿˜æ˜¯ä¸è¡Œ**
   - å°è¯•ä½¿ç”¨æ›´å°çš„æ•°æ®é›†æµ‹è¯•
   - å°è¯•ä½¿ç”¨ SentencePiece æ–¹æ³•
   - è°ƒæ•´è®­ç»ƒå‚æ•°ï¼ˆè¯æ±‡è¡¨å¤§å°ã€æ‰¹æ¬¡å¤§å°ï¼‰
   - å‡çº§ tokenizers åº“

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [fix_tokenizer_training.py](/Users/twrong/git/code/ChatLM-mini-Chinese/tokenize/fix_tokenizer_training.py) - é¢„å¤„ç†è„šæœ¬
- [train_tokenizer.py](/Users/twrong/git/code/ChatLM-mini-Chinese/tokenize/train_tokenizer.py) - è®­ç»ƒè„šæœ¬
- [TOKENIZER_DATA_FORMAT_GUIDE.md](/Users/twrong/git/code/ChatLM-mini-Chinese/tokenize/TOKENIZER_DATA_FORMAT_GUIDE.md) - æ•°æ®æ ¼å¼æŒ‡å—

---

**ç«‹å³å¼€å§‹ä¿®å¤ï¼** ğŸš€

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

# é¢„å¤„ç†æ•°æ®
python fix_tokenizer_training.py \
  --input ../data/wiki.simple.txt \
  --output ../data/wiki_clean.txt

# è®­ç»ƒ tokenizer
python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki_clean.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 40960 \
  --batch-size 500
```
