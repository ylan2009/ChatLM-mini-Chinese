# Tokenizer è®­ç»ƒé”™è¯¯ä¿®å¤è¯´æ˜

## ğŸ› é—®é¢˜æè¿°

è¿è¡Œä»¥ä¸‹å‘½ä»¤æ—¶å‡ºç° Rust panic é”™è¯¯ï¼š

```bash
python train_tokenizer.py --method t5-base --wiki-file ../data/my_corpus.txt --output-dir ../model_save/my_tokenizer_wiki
```

**é”™è¯¯ä¿¡æ¯**ï¼š

```
thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/models/unigram/trainer.rs:228:53:
called `Result::unwrap()` on an `Err` value: Internal

pyo3_runtime.PanicException: called `Result::unwrap()` on an `Err` value: Internal
```

---

## ğŸ” é—®é¢˜åˆ†æ

### 1. æ–‡ä»¶è·¯å¾„é”™è¯¯

å‘½ä»¤ä¸­ä½¿ç”¨çš„æ–‡ä»¶ `../data/my_corpus.txt` **ä¸å­˜åœ¨**ã€‚

å®é™…å­˜åœ¨çš„æ–‡ä»¶ï¼š
- `../data/wiki.txt` - å®Œæ•´çš„ç»´åŸºç™¾ç§‘è¯­æ–™
- `../data/wiki.simple.txt` - ç®€åŒ–çš„ç»´åŸºç™¾ç§‘è¯­æ–™

### 2. Tokenizers åº“çš„ Rust Panic Bug

è¿™æ˜¯ `transformers` åº“ä¸­ `train_new_from_iterator()` æ–¹æ³•çš„å·²çŸ¥é—®é¢˜ã€‚

**æ ¹æœ¬åŸå› **ï¼š
- `train_new_from_iterator()` å†…éƒ¨ä½¿ç”¨ tokenizers åº“çš„ Rust å®ç°
- å½“è¿­ä»£å™¨è¿”å›çš„æ•°æ®æ ¼å¼ä¸ç¬¦åˆé¢„æœŸæ—¶ï¼Œä¼šè§¦å‘ Rust panic
- ç‰¹åˆ«æ˜¯ä»¥ä¸‹æƒ…å†µå®¹æ˜“å‡ºé”™ï¼š
  - æ–‡æœ¬å—åŒ…å«å¤§é‡æ¢è¡Œç¬¦
  - æ–‡æœ¬å—å¤ªå¤§æˆ–å¤ªå°
  - æ–‡æœ¬åŒ…å«ç‰¹æ®Šå­—ç¬¦æˆ–ç©ºå€¼
  - è¿­ä»£å™¨è¿”å›çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®

### 3. åŸå§‹ä»£ç çš„é—®é¢˜

#### é—®é¢˜ 1ï¼šç›´æ¥æ‹¼æ¥æ¢è¡Œç¬¦

```python
# âŒ é”™è¯¯çš„ä»£ç 
buffer.append(''.join(txt))  # txt åŒ…å«æ¢è¡Œç¬¦
```

è¿™ä¼šå¯¼è‡´æ–‡æœ¬å—ä¸­åŒ…å«å¤§é‡æ¢è¡Œç¬¦ï¼Œå¯èƒ½è§¦å‘ tokenizers åº“çš„ bugã€‚

#### é—®é¢˜ 2ï¼šæ²¡æœ‰è¿‡æ»¤ç©ºå€¼

```python
# âŒ é”™è¯¯çš„ä»£ç 
for line in lines:
    txt.append(line)  # æ²¡æœ‰æ£€æŸ¥ line æ˜¯å¦ä¸ºç©º
```

ç©ºè¡Œä¼šå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬å—è´¨é‡å·®ã€‚

#### é—®é¢˜ 3ï¼šParquet è¿­ä»£å™¨æ ¼å¼é—®é¢˜

```python
# âŒ é”™è¯¯çš„ä»£ç 
buffer.append(f"{prompt.as_py()}\n{response.as_py()}")  # ä½¿ç”¨æ¢è¡Œç¬¦è¿æ¥
```

ä½¿ç”¨æ¢è¡Œç¬¦è¿æ¥å¯èƒ½å¯¼è‡´æ ¼å¼é—®é¢˜ã€‚

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤ 1ï¼šæ”¹è¿› `get_wiki_corpus_iterator` å‡½æ•°

**å…³é”®æ”¹è¿›**ï¼š
1. âœ… è·³è¿‡ç©ºè¡Œ
2. âœ… ä½¿ç”¨ç©ºæ ¼è¿æ¥æ–‡æœ¬ï¼Œè€Œä¸æ˜¯ç›´æ¥æ‹¼æ¥
3. âœ… ç¡®ä¿æ–‡æœ¬é•¿åº¦åˆç†ï¼ˆè‡³å°‘ 10 ä¸ªå­—ç¬¦ï¼‰
4. âœ… å¤„ç†å‰©ä½™çš„æ–‡æœ¬

```python
def get_wiki_corpus_iterator(wiki_file: str, min_chunk_size: int = 2048, batch_size: int = 1000):
    def get_training_corpus():
        buffer = []
        txt = []
        len_cnt = 0
        
        for line in progress.track(lines, description="å¤„ç†è¯­æ–™"):
            # è·³è¿‡ç©ºè¡Œ
            line = line.strip()
            if not line:
                continue
            
            len_cnt += len(line)
            txt.append(line)
            
            # å½“ç´¯ç§¯å­—ç¬¦æ•°è¾¾åˆ°æœ€å°å—å¤§å°æ—¶ï¼Œåˆ›å»ºä¸€ä¸ªæ–‡æœ¬å—
            if len_cnt >= min_chunk_size:
                text = ' '.join(txt)  # âœ… ä½¿ç”¨ç©ºæ ¼è¿æ¥
                # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©ºä¸”é•¿åº¦åˆç†
                if text and len(text) >= 10:
                    buffer.append(text)
                txt = []
                len_cnt = 0
            
            # å½“ç¼“å†²åŒºè¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶ï¼Œè¿”å›ä¸€æ‰¹æ•°æ®
            if len(buffer) >= batch_size:
                yield buffer
                buffer = []
        
        # âœ… å¤„ç†å‰©ä½™çš„æ–‡æœ¬
        if txt:
            text = ' '.join(txt)
            if text and len(text) >= 10:
                buffer.append(text)
        
        # è¿”å›æœ€åä¸€æ‰¹æ•°æ®
        if len(buffer) > 0:
            yield buffer
    
    return get_training_corpus()
```

### ä¿®å¤ 2ï¼šæ”¹è¿› `get_parquet_corpus_iterator` å‡½æ•°

**å…³é”®æ”¹è¿›**ï¼š
1. âœ… æ£€æŸ¥ç©ºå€¼
2. âœ… ä½¿ç”¨ç©ºæ ¼è¿æ¥ prompt å’Œ response
3. âœ… ç¡®ä¿æ–‡æœ¬é•¿åº¦åˆç†

```python
def get_parquet_corpus_iterator(parquet_file: str, batch_size: int = 1000):
    def get_training_corpus():
        buffer = []
        for prompt, response in progress.track(...):
            # âœ… è·å–å®é™…çš„å­—ç¬¦ä¸²å€¼å¹¶æ£€æŸ¥ç©ºå€¼
            prompt_str = prompt.as_py() if prompt.as_py() else ""
            response_str = response.as_py() if response.as_py() else ""
            
            # è·³è¿‡ç©ºå€¼
            if not prompt_str and not response_str:
                continue
            
            # âœ… ä½¿ç”¨ç©ºæ ¼è¿æ¥
            text = f"{prompt_str} {response_str}".strip()
            
            # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©ºä¸”é•¿åº¦åˆç†
            if text and len(text) >= 10:
                buffer.append(text)
            
            if len(buffer) >= batch_size:
                yield buffer
                buffer = []
        
        if buffer:
            yield buffer
    
    return get_training_corpus()
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³• 1ï¼šä½¿ç”¨ç»´åŸºç™¾ç§‘æ–‡æœ¬æ–‡ä»¶ï¼ˆä¿®å¤åï¼‰

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

# ä½¿ç”¨å®Œæ•´çš„ç»´åŸºç™¾ç§‘è¯­æ–™
python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 40960 \
  --min-chunk-size 2048 \
  --batch-size 500

# æˆ–ä½¿ç”¨ç®€åŒ–çš„ç»´åŸºç™¾ç§‘è¯­æ–™ï¼ˆæ›´å¿«ï¼‰
python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki.simple.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 40960 \
  --min-chunk-size 2048 \
  --batch-size 500
```

### æ–¹æ³• 2ï¼šä½¿ç”¨ Parquet æ–‡ä»¶ï¼ˆæ¨èï¼‰

Parquet æ–‡ä»¶çš„æ•°æ®æ ¼å¼æ›´è§„èŒƒï¼Œä¸å®¹æ˜“å‡ºé”™ï¼š

```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese/tokenize

# ä½¿ç”¨ SFT è®­ç»ƒæ•°æ®é›†
python train_tokenizer.py \
  --method t5-base \
  --parquet-file ../data/sft_train_dataset.parquet \
  --output-dir ../model_save/my_tokenizer_sft \
  --vocab-size 40960 \
  --batch-size 500

# æˆ–ä½¿ç”¨å®Œæ•´çš„ SFT æ•°æ®é›†
python train_tokenizer.py \
  --method t5-base \
  --parquet-file ../data/sft_dataset.parquet \
  --output-dir ../model_save/my_tokenizer_sft \
  --vocab-size 40960 \
  --batch-size 500
```

### æ–¹æ³• 3ï¼šä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°

å¦‚æœä»ç„¶å‡ºé”™ï¼Œå°è¯•å‡å°æ‰¹æ¬¡å¤§å°ï¼š

```bash
python train_tokenizer.py \
  --method t5-base \
  --wiki-file ../data/wiki.simple.txt \
  --output-dir ../model_save/my_tokenizer_wiki \
  --vocab-size 40960 \
  --min-chunk-size 1024 \
  --batch-size 200
```

---

## ğŸ“Š å‚æ•°è¯´æ˜

### `--method`
- **t5-base**ï¼ˆæ¨èï¼‰ï¼šåŸºäº T5-base tokenizer è®­ç»ƒ
- **char-bpe**ï¼šå­—ç¬¦çº§åˆ« BPE tokenizer
- **byte-bpe**ï¼šå­—èŠ‚çº§åˆ« BPE tokenizer

### `--vocab-size`
- è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤ï¼š40960ï¼‰
- å»ºè®®èŒƒå›´ï¼š20000 - 50000
- æ›´å¤§çš„è¯æ±‡è¡¨å¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºä¸­æ–‡ï¼Œä½†ä¼šå¢åŠ æ¨¡å‹å¤§å°

### `--min-chunk-size`
- æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å°å­—ç¬¦æ•°ï¼ˆä»…ç”¨äº wiki-fileï¼Œé»˜è®¤ï¼š2048ï¼‰
- å»ºè®®èŒƒå›´ï¼š1024 - 4096
- å¤ªå°ä¼šå¯¼è‡´æ–‡æœ¬å—è´¨é‡å·®ï¼Œå¤ªå¤§ä¼šå¢åŠ å†…å­˜å ç”¨

### `--batch-size`
- æ¯æ¬¡è¿­ä»£çš„æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ï¼š1000ï¼‰
- å»ºè®®èŒƒå›´ï¼š200 - 1000
- å¦‚æœå‡ºç°å†…å­˜é”™è¯¯æˆ– Rust panicï¼Œå°è¯•å‡å°è¿™ä¸ªå€¼

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ 1ï¼šä»ç„¶å‡ºç° Rust panic

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å‡å° `--batch-size` åˆ° 200 æˆ–æ›´å°
2. å‡å° `--min-chunk-size` åˆ° 1024
3. ä½¿ç”¨ Parquet æ–‡ä»¶è€Œä¸æ˜¯æ–‡æœ¬æ–‡ä»¶
4. æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦åŒ…å«ç‰¹æ®Šå­—ç¬¦æˆ–æŸåçš„æ•°æ®

### é—®é¢˜ 2ï¼šå†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å‡å° `--batch-size`
2. ä½¿ç”¨ `wiki.simple.txt` è€Œä¸æ˜¯ `wiki.txt`
3. å‡å° `--vocab-size`

### é—®é¢˜ 3ï¼šè®­ç»ƒé€Ÿåº¦å¤ªæ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨ `wiki.simple.txt` è€Œä¸æ˜¯ `wiki.txt`
2. å¢å¤§ `--batch-size`ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
3. å‡å° `--vocab-size`

### é—®é¢˜ 4ï¼šæ–‡ä»¶ä¸å­˜åœ¨

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
2. ä½¿ç”¨ `ls ../data/*.txt` æŸ¥çœ‹å¯ç”¨çš„æ–‡ä»¶
3. ä½¿ç”¨ `ls ../data/*.parquet` æŸ¥çœ‹å¯ç”¨çš„ Parquet æ–‡ä»¶

---

## ğŸ“ ä¿®æ”¹çš„æ–‡ä»¶

### [train_tokenizer.py](/Users/twrong/git/code/ChatLM-mini-Chinese/tokenize/train_tokenizer.py)

**ä¿®æ”¹å†…å®¹**ï¼š
1. âœ… ä¿®å¤ `get_wiki_corpus_iterator` å‡½æ•°
   - è·³è¿‡ç©ºè¡Œ
   - ä½¿ç”¨ç©ºæ ¼è¿æ¥æ–‡æœ¬
   - ç¡®ä¿æ–‡æœ¬é•¿åº¦åˆç†
   - å¤„ç†å‰©ä½™çš„æ–‡æœ¬

2. âœ… ä¿®å¤ `get_parquet_corpus_iterator` å‡½æ•°
   - æ£€æŸ¥ç©ºå€¼
   - ä½¿ç”¨ç©ºæ ¼è¿æ¥ prompt å’Œ response
   - ç¡®ä¿æ–‡æœ¬é•¿åº¦åˆç†

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ¨èä½¿ç”¨ Parquet æ–‡ä»¶

Parquet æ–‡ä»¶çš„ä¼˜åŠ¿ï¼š
- âœ… æ•°æ®æ ¼å¼è§„èŒƒ
- âœ… è¯»å–é€Ÿåº¦å¿«
- âœ… ä¸å®¹æ˜“å‡ºé”™
- âœ… æ”¯æŒåˆ—å¼å­˜å‚¨

### 2. åˆç†è®¾ç½®å‚æ•°

```bash
# æ¨èçš„å‚æ•°ç»„åˆ
python train_tokenizer.py \
  --method t5-base \
  --parquet-file ../data/sft_train_dataset.parquet \
  --output-dir ../model_save/my_tokenizer \
  --vocab-size 40960 \
  --batch-size 500
```

### 3. ç›‘æ§è®­ç»ƒè¿‡ç¨‹

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- åŠ è½½è¯­æ–™çš„è¿›åº¦
- å¤„ç†è¯­æ–™çš„è¿›åº¦
- è®­ç»ƒçš„è¿›åº¦
- å†…å­˜ä½¿ç”¨æƒ…å†µ

å¦‚æœå‡ºç°é—®é¢˜ï¼Œå¯ä»¥æŒ‰ `Ctrl+C` ä¸­æ–­è®­ç»ƒã€‚

### 4. éªŒè¯è®­ç»ƒç»“æœ

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨ `--test` å‚æ•°æµ‹è¯• tokenizerï¼š

```bash
python train_tokenizer.py \
  --method t5-base \
  --parquet-file ../data/sft_train_dataset.parquet \
  --output-dir ../model_save/my_tokenizer \
  --test
```

---

## ğŸ¯ æ€»ç»“

### é—®é¢˜æœ¬è´¨
- Tokenizers åº“çš„ Rust å®ç°å¯¹æ•°æ®æ ¼å¼è¦æ±‚ä¸¥æ ¼
- åŸå§‹ä»£ç ç”Ÿæˆçš„æ–‡æœ¬å—åŒ…å«å¤§é‡æ¢è¡Œç¬¦å’Œç©ºå€¼
- å¯¼è‡´ Rust panic é”™è¯¯

### ä¿®å¤æ–¹æ³•
- âœ… è·³è¿‡ç©ºè¡Œ
- âœ… ä½¿ç”¨ç©ºæ ¼è¿æ¥æ–‡æœ¬
- âœ… ç¡®ä¿æ–‡æœ¬é•¿åº¦åˆç†
- âœ… æ£€æŸ¥ç©ºå€¼

### æ¨èæ–¹æ¡ˆ
- ğŸ¥‡ ä½¿ç”¨ Parquet æ–‡ä»¶ï¼ˆæœ€ç¨³å®šï¼‰
- ğŸ¥ˆ ä½¿ç”¨ä¿®å¤åçš„ wiki.simple.txtï¼ˆæ›´å¿«ï¼‰
- ğŸ¥‰ ä½¿ç”¨ä¿®å¤åçš„ wiki.txtï¼ˆæœ€å®Œæ•´ï¼‰

---

**ä¿®å¤æ—¥æœŸ**: 2026-02-05  
**Bug ä¸¥é‡ç¨‹åº¦**: ğŸ”´ é«˜ï¼ˆå¯¼è‡´è®­ç»ƒå¤±è´¥ï¼‰  
**ä¿®å¤çŠ¶æ€**: âœ… å·²ä¿®å¤  
**æµ‹è¯•çŠ¶æ€**: â³ å¾…éªŒè¯

---

## ğŸš€ ä¸‹ä¸€æ­¥

1. âœ… é€‰æ‹©åˆé€‚çš„è¾“å…¥æ–‡ä»¶ï¼ˆParquet æˆ– txtï¼‰
2. âœ… è¿è¡Œè®­ç»ƒå‘½ä»¤
3. âœ… ç›‘æ§è®­ç»ƒè¿‡ç¨‹
4. âœ… éªŒè¯è®­ç»ƒç»“æœ

**ç°åœ¨å¯ä»¥é‡æ–°è¿è¡Œè®­ç»ƒå‘½ä»¤äº†ï¼** ğŸ‰
