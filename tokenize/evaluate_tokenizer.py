#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tokenizer è¯„ä¼°å·¥å…·

åŠŸèƒ½ï¼š
1. è¯æ±‡è¡¨è¦†ç›–ç‡åˆ†æ
2. åˆ†è¯æ•ˆæœæµ‹è¯•
3. å‹ç¼©ç‡è¯„ä¼°
4. ç‰¹æ®Šå­—ç¬¦å¤„ç†
5. ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬å¤„ç†
6. æ•°å­—å’Œæ ‡ç‚¹ç¬¦å·å¤„ç†
7. æœªçŸ¥è¯ï¼ˆUNKï¼‰æ¯”ä¾‹
8. å¹³å‡ token é•¿åº¦

ä½¿ç”¨æ–¹æ³•ï¼š
    # è¯„ä¼°å•ä¸ª tokenizer
    python evaluate_tokenizer.py --tokenizer-dir ../model_save/my_tokenizer_wiki
    
    # å¯¹æ¯”å¤šä¸ª tokenizer
    python evaluate_tokenizer.py \
        --tokenizer-dir ../model_save/my_tokenizer_wiki \
        --compare-with ../model_save/my_tokenizer_sp \
        --compare-with ../model_save/my_tokenizer_char
    
    # ä½¿ç”¨è‡ªå®šä¹‰æµ‹è¯•æ–‡ä»¶
    python evaluate_tokenizer.py \
        --tokenizer-dir ../model_save/my_tokenizer_wiki \
        --test-file ../data/test_corpus.txt
    
    # è¯¦ç»†æ¨¡å¼ï¼ˆæ˜¾ç¤ºæ¯ä¸ªæ ·æœ¬çš„åˆ†è¯ç»“æœï¼‰
    python evaluate_tokenizer.py \
        --tokenizer-dir ../model_save/my_tokenizer_wiki \
        --verbose
"""

import os
import sys
import argparse
from typing import List, Dict, Tuple
from collections import Counter
import json


def check_transformers():
    """æ£€æŸ¥å¹¶å¯¼å…¥ transformers"""
    try:
        from transformers import AutoTokenizer
        return AutoTokenizer
    except ImportError:
        return None


def load_tokenizer(tokenizer_dir: str):
    """åŠ è½½ tokenizer"""
    AutoTokenizer = check_transformers()
    if AutoTokenizer is None:
        raise ImportError("éœ€è¦ transformers åº“: pip install transformers")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
        return tokenizer
    except Exception as e:
        print(f"âŒ åŠ è½½ tokenizer å¤±è´¥: {e}")
        return None


def get_test_samples() -> List[Tuple[str, str]]:
    """è·å–æµ‹è¯•æ ·æœ¬ï¼ˆæ–‡æœ¬ï¼Œç±»åˆ«ï¼‰"""
    return [
        # çº¯ä¸­æ–‡
        ("äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚", "çº¯ä¸­æ–‡"),
        ("æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚", "çº¯ä¸­æ–‡"),
        ("æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚", "çº¯ä¸­æ–‡"),
        
        # çº¯è‹±æ–‡
        ("Artificial intelligence is a branch of computer science that aims to create systems capable of performing tasks that typically require human intelligence.", "çº¯è‹±æ–‡"),
        ("Machine learning is a subset of artificial intelligence that enables computers to learn and improve without being explicitly programmed.", "çº¯è‹±æ–‡"),
        
        # ä¸­è‹±æ··åˆ
        ("Python æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç‰¹åˆ«é€‚åˆ AI å’Œ machine learning å¼€å‘ã€‚", "ä¸­è‹±æ··åˆ"),
        ("Transformer æ¨¡å‹åœ¨ NLP é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼ŒBERT å’Œ GPT éƒ½æ˜¯åŸºäº Transformer æ¶æ„ã€‚", "ä¸­è‹±æ··åˆ"),
        ("ä½¿ç”¨ PyTorch æˆ– TensorFlow å¯ä»¥å¿«é€Ÿæ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚", "ä¸­è‹±æ··åˆ"),
        
        # åŒ…å«æ•°å­—
        ("2024å¹´ï¼Œå…¨çƒAIå¸‚åœºè§„æ¨¡é¢„è®¡å°†è¾¾åˆ°5000äº¿ç¾å…ƒï¼Œå¹´å¢é•¿ç‡è¶…è¿‡30%ã€‚", "åŒ…å«æ•°å­—"),
        ("GPT-3æœ‰1750äº¿ä¸ªå‚æ•°ï¼ŒGPT-4çš„å‚æ•°é‡æ›´å¤§ã€‚", "åŒ…å«æ•°å­—"),
        
        # åŒ…å«æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦
        ("ä»€ä¹ˆæ˜¯AIï¼Ÿå®ƒèƒ½åšä»€ä¹ˆï¼ŸAIçš„æœªæ¥åœ¨å“ªé‡Œï¼Ÿ", "åŒ…å«æ ‡ç‚¹"),
        ("ã€é‡è¦ã€‘è¯·æ³¨æ„ï¼šæ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®ï¼ï¼ï¼", "åŒ…å«æ ‡ç‚¹"),
        ("Email: test@example.com, Website: https://www.example.com", "ç‰¹æ®Šå­—ç¬¦"),
        
        # é•¿æ–‡æœ¬
        ("è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦é¢†åŸŸçš„åˆ†æ”¯å­¦ç§‘ã€‚æ­¤é¢†åŸŸæ¢è®¨å¦‚ä½•å¤„ç†åŠè¿ç”¨è‡ªç„¶è¯­è¨€ï¼›è‡ªç„¶è¯­è¨€å¤„ç†åŒ…æ‹¬å¤šæ–¹é¢å’Œæ­¥éª¤ï¼ŒåŸºæœ¬æœ‰è®¤çŸ¥ã€ç†è§£ã€ç”Ÿæˆç­‰éƒ¨åˆ†ã€‚", "é•¿æ–‡æœ¬"),
        
        # çŸ­æ–‡æœ¬
        ("ä½ å¥½", "çŸ­æ–‡æœ¬"),
        ("AI", "çŸ­æ–‡æœ¬"),
        ("123", "çŸ­æ–‡æœ¬"),
        
        # ä¸“ä¸šæœ¯è¯­
        ("Transformeræ¶æ„ä½¿ç”¨self-attentionæœºåˆ¶ï¼Œé€šè¿‡multi-head attentionå’Œposition encodingå®ç°åºåˆ—å»ºæ¨¡ã€‚", "ä¸“ä¸šæœ¯è¯­"),
        ("åå‘ä¼ æ’­ç®—æ³•ï¼ˆBackpropagationï¼‰æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚", "ä¸“ä¸šæœ¯è¯­"),
    ]


def evaluate_tokenizer(
    tokenizer,
    test_samples: List[Tuple[str, str]] = None,
    verbose: bool = False
) -> Dict:
    """
    è¯„ä¼° tokenizer
    
    Args:
        tokenizer: è¦è¯„ä¼°çš„ tokenizer
        test_samples: æµ‹è¯•æ ·æœ¬åˆ—è¡¨
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    if test_samples is None:
        test_samples = get_test_samples()
    
    results = {
        'vocab_size': len(tokenizer),
        'special_tokens': {},
        'samples': [],
        'statistics': {}
    }
    
    # 1. æ£€æŸ¥ç‰¹æ®Š token
    special_tokens = {
        'pad_token': tokenizer.pad_token,
        'unk_token': tokenizer.unk_token,
        'bos_token': getattr(tokenizer, 'bos_token', None),
        'eos_token': getattr(tokenizer, 'eos_token', None),
        'cls_token': getattr(tokenizer, 'cls_token', None),
        'sep_token': getattr(tokenizer, 'sep_token', None),
        'mask_token': getattr(tokenizer, 'mask_token', None),
    }
    results['special_tokens'] = special_tokens
    
    # 2. è¯„ä¼°æ¯ä¸ªæ ·æœ¬
    total_chars = 0
    total_tokens = 0
    total_unk = 0
    category_stats = {}
    
    for text, category in test_samples:
        # ç¼–ç 
        tokens = tokenizer.tokenize(text)
        token_ids = tokenizer.encode(text, add_special_tokens=False)
        
        # ç»Ÿè®¡
        num_chars = len(text)
        num_tokens = len(tokens)
        num_unk = sum(1 for t in tokens if t == tokenizer.unk_token or '[UNK]' in str(t))
        compression_ratio = num_chars / num_tokens if num_tokens > 0 else 0
        unk_ratio = num_unk / num_tokens if num_tokens > 0 else 0
        
        # è§£ç æµ‹è¯•
        decoded = tokenizer.decode(token_ids)
        is_reversible = (decoded.replace(' ', '') == text.replace(' ', ''))
        
        sample_result = {
            'text': text,
            'category': category,
            'num_chars': num_chars,
            'num_tokens': num_tokens,
            'num_unk': num_unk,
            'compression_ratio': compression_ratio,
            'unk_ratio': unk_ratio,
            'is_reversible': is_reversible,
            'tokens': tokens if verbose else None,
            'decoded': decoded if verbose else None,
        }
        
        results['samples'].append(sample_result)
        
        # ç´¯è®¡ç»Ÿè®¡
        total_chars += num_chars
        total_tokens += num_tokens
        total_unk += num_unk
        
        # åˆ†ç±»ç»Ÿè®¡
        if category not in category_stats:
            category_stats[category] = {
                'count': 0,
                'total_chars': 0,
                'total_tokens': 0,
                'total_unk': 0,
            }
        category_stats[category]['count'] += 1
        category_stats[category]['total_chars'] += num_chars
        category_stats[category]['total_tokens'] += num_tokens
        category_stats[category]['total_unk'] += num_unk
    
    # 3. è®¡ç®—æ€»ä½“ç»Ÿè®¡
    avg_compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0
    avg_unk_ratio = total_unk / total_tokens if total_tokens > 0 else 0
    
    results['statistics'] = {
        'total_samples': len(test_samples),
        'total_chars': total_chars,
        'total_tokens': total_tokens,
        'total_unk': total_unk,
        'avg_compression_ratio': avg_compression_ratio,
        'avg_unk_ratio': avg_unk_ratio,
        'category_stats': category_stats,
    }
    
    return results


def print_evaluation_report(results: Dict, tokenizer_name: str = "Tokenizer"):
    """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
    print("\n" + "="*80)
    print(f"ğŸ“Š {tokenizer_name} è¯„ä¼°æŠ¥å‘Š")
    print("="*80)
    
    # 1. åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ“š åŸºæœ¬ä¿¡æ¯:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {results['vocab_size']:,}")
    
    # 2. ç‰¹æ®Š token
    print(f"\nğŸ”– ç‰¹æ®Š Token:")
    for name, token in results['special_tokens'].items():
        if token:
            print(f"  {name}: {token}")
    
    # 3. æ€»ä½“ç»Ÿè®¡
    stats = results['statistics']
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"  æµ‹è¯•æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"  æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
    print(f"  æ€» token æ•°: {stats['total_tokens']:,}")
    print(f"  å¹³å‡å‹ç¼©ç‡: {stats['avg_compression_ratio']:.2f} å­—ç¬¦/token")
    print(f"  æœªçŸ¥è¯æ¯”ä¾‹: {stats['avg_unk_ratio']*100:.2f}%")
    
    # è¯„åˆ†
    score = calculate_score(stats['avg_compression_ratio'], stats['avg_unk_ratio'])
    print(f"\nâ­ ç»¼åˆè¯„åˆ†: {score:.1f}/100")
    print_score_interpretation(score)
    
    # 4. åˆ†ç±»ç»Ÿè®¡
    print(f"\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
    print(f"{'ç±»åˆ«':<12} {'æ ·æœ¬æ•°':<8} {'å¹³å‡å‹ç¼©ç‡':<12} {'æœªçŸ¥è¯æ¯”ä¾‹':<12}")
    print("-" * 50)
    
    for category, cat_stats in stats['category_stats'].items():
        avg_comp = cat_stats['total_chars'] / cat_stats['total_tokens'] if cat_stats['total_tokens'] > 0 else 0
        avg_unk = cat_stats['total_unk'] / cat_stats['total_tokens'] if cat_stats['total_tokens'] > 0 else 0
        print(f"{category:<12} {cat_stats['count']:<8} {avg_comp:<12.2f} {avg_unk*100:<12.2f}%")
    
    # 5. æ ·æœ¬è¯¦æƒ…ï¼ˆå¦‚æœæœ‰ï¼‰
    if results['samples'] and results['samples'][0].get('tokens'):
        print(f"\nğŸ“ æ ·æœ¬è¯¦æƒ…:")
        for i, sample in enumerate(results['samples'][:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"\n  æ ·æœ¬ {i} ({sample['category']}):")
            print(f"    åŸæ–‡: {sample['text'][:60]}{'...' if len(sample['text']) > 60 else ''}")
            print(f"    Token æ•°: {sample['num_tokens']}, å‹ç¼©ç‡: {sample['compression_ratio']:.2f}")
            print(f"    æœªçŸ¥è¯: {sample['num_unk']}, å¯é€†: {'âœ“' if sample['is_reversible'] else 'âœ—'}")
            if sample['tokens']:
                tokens_str = ' | '.join(sample['tokens'][:10])
                if len(sample['tokens']) > 10:
                    tokens_str += ' | ...'
                print(f"    Tokens: {tokens_str}")


def calculate_score(compression_ratio: float, unk_ratio: float) -> float:
    """
    è®¡ç®—ç»¼åˆè¯„åˆ†
    
    è¯„åˆ†æ ‡å‡†ï¼š
    - å‹ç¼©ç‡ï¼š2.0-3.0 æœ€ä½³ï¼ˆä¸­æ–‡ï¼‰ï¼Œ1.5-2.5 æœ€ä½³ï¼ˆè‹±æ–‡ï¼‰
    - æœªçŸ¥è¯æ¯”ä¾‹ï¼šè¶Šä½è¶Šå¥½ï¼Œ< 1% ä¼˜ç§€ï¼Œ< 5% è‰¯å¥½
    """
    # å‹ç¼©ç‡è¯„åˆ†ï¼ˆæ»¡åˆ† 60ï¼‰
    if 2.0 <= compression_ratio <= 3.0:
        compression_score = 60
    elif 1.5 <= compression_ratio < 2.0 or 3.0 < compression_ratio <= 3.5:
        compression_score = 50
    elif 1.0 <= compression_ratio < 1.5 or 3.5 < compression_ratio <= 4.0:
        compression_score = 40
    else:
        compression_score = 30
    
    # æœªçŸ¥è¯æ¯”ä¾‹è¯„åˆ†ï¼ˆæ»¡åˆ† 40ï¼‰
    if unk_ratio < 0.01:  # < 1%
        unk_score = 40
    elif unk_ratio < 0.05:  # < 5%
        unk_score = 30
    elif unk_ratio < 0.10:  # < 10%
        unk_score = 20
    else:
        unk_score = 10
    
    return compression_score + unk_score


def print_score_interpretation(score: float):
    """æ‰“å°è¯„åˆ†è§£é‡Š"""
    if score >= 90:
        print("  ğŸ‰ ä¼˜ç§€ï¼Tokenizer è®­ç»ƒè´¨é‡éå¸¸å¥½")
    elif score >= 75:
        print("  âœ… è‰¯å¥½ï¼Tokenizer è®­ç»ƒè´¨é‡ä¸é”™")
    elif score >= 60:
        print("  âš ï¸  ä¸€èˆ¬ï¼ŒTokenizer å¯èƒ½éœ€è¦æ”¹è¿›")
    else:
        print("  âŒ è¾ƒå·®ï¼Œå»ºè®®é‡æ–°è®­ç»ƒ Tokenizer")


def compare_tokenizers(tokenizer_dirs: List[str], verbose: bool = False):
    """å¯¹æ¯”å¤šä¸ª tokenizer"""
    print("\n" + "="*80)
    print("ğŸ” Tokenizer å¯¹æ¯”åˆ†æ")
    print("="*80)
    
    all_results = []
    test_samples = get_test_samples()
    
    for tokenizer_dir in tokenizer_dirs:
        print(f"\næ­£åœ¨è¯„ä¼°: {tokenizer_dir}")
        tokenizer = load_tokenizer(tokenizer_dir)
        if tokenizer is None:
            continue
        
        results = evaluate_tokenizer(tokenizer, test_samples, verbose=False)
        results['tokenizer_dir'] = tokenizer_dir
        all_results.append(results)
    
    if len(all_results) < 2:
        print("\nâŒ éœ€è¦è‡³å°‘ 2 ä¸ªæœ‰æ•ˆçš„ tokenizer æ‰èƒ½è¿›è¡Œå¯¹æ¯”")
        return
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print("\n" + "="*80)
    print("ğŸ“Š å¯¹æ¯”ç»“æœ")
    print("="*80)
    
    print(f"\n{'Tokenizer':<40} {'è¯æ±‡è¡¨':<10} {'å‹ç¼©ç‡':<10} {'æœªçŸ¥è¯%':<10} {'è¯„åˆ†':<10}")
    print("-" * 80)
    
    for result in all_results:
        name = os.path.basename(result['tokenizer_dir'])
        vocab_size = result['vocab_size']
        comp_ratio = result['statistics']['avg_compression_ratio']
        unk_ratio = result['statistics']['avg_unk_ratio'] * 100
        score = calculate_score(comp_ratio, result['statistics']['avg_unk_ratio'])
        
        print(f"{name:<40} {vocab_size:<10,} {comp_ratio:<10.2f} {unk_ratio:<10.2f} {score:<10.1f}")
    
    # æ‰¾å‡ºæœ€ä½³ tokenizer
    best_result = max(all_results, key=lambda r: calculate_score(
        r['statistics']['avg_compression_ratio'],
        r['statistics']['avg_unk_ratio']
    ))
    
    print(f"\nğŸ† æœ€ä½³ Tokenizer: {os.path.basename(best_result['tokenizer_dir'])}")
    
    # è¯¦ç»†æŠ¥å‘Š
    if verbose:
        for result in all_results:
            print_evaluation_report(result, os.path.basename(result['tokenizer_dir']))


def evaluate_on_file(tokenizer, test_file: str, max_samples: int = 100) -> Dict:
    """åœ¨æ–‡ä»¶ä¸Šè¯„ä¼° tokenizer"""
    print(f"\næ­£åœ¨ä»æ–‡ä»¶è¯»å–æµ‹è¯•æ ·æœ¬: {test_file}")
    
    test_samples = []
    with open(test_file, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= max_samples:
                break
            line = line.strip()
            if line:
                test_samples.append((line, "æ–‡ä»¶æ ·æœ¬"))
    
    print(f"âœ“ è¯»å–äº† {len(test_samples)} ä¸ªæ ·æœ¬")
    
    return evaluate_tokenizer(tokenizer, test_samples)


def main():
    parser = argparse.ArgumentParser(
        description='è¯„ä¼° Tokenizer è®­ç»ƒè´¨é‡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        '--tokenizer-dir',
        type=str,
        required=True,
        help='Tokenizer ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--compare-with',
        type=str,
        action='append',
        help='è¦å¯¹æ¯”çš„å…¶ä»– tokenizer ç›®å½•ï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰'
    )
    
    parser.add_argument(
        '--test-file',
        type=str,
        help='è‡ªå®šä¹‰æµ‹è¯•æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--max-samples',
        type=int,
        default=100,
        help='ä»æµ‹è¯•æ–‡ä»¶è¯»å–çš„æœ€å¤§æ ·æœ¬æ•°ï¼ˆé»˜è®¤: 100ï¼‰'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ¯ä¸ªæ ·æœ¬çš„åˆ†è¯ç»“æœï¼‰'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        help='ä¿å­˜è¯„ä¼°ç»“æœåˆ° JSON æ–‡ä»¶'
    )
    
    args = parser.parse_args()
    
    # å¯¹æ¯”æ¨¡å¼
    if args.compare_with:
        tokenizer_dirs = [args.tokenizer_dir] + args.compare_with
        compare_tokenizers(tokenizer_dirs, args.verbose)
        return
    
    # å•ä¸ªè¯„ä¼°æ¨¡å¼
    print(f"\næ­£åœ¨åŠ è½½ tokenizer: {args.tokenizer_dir}")
    tokenizer = load_tokenizer(args.tokenizer_dir)
    
    if tokenizer is None:
        sys.exit(1)
    
    print("âœ“ Tokenizer åŠ è½½æˆåŠŸ")
    
    # è¯„ä¼°
    if args.test_file:
        results = evaluate_on_file(tokenizer, args.test_file, args.max_samples)
    else:
        test_samples = get_test_samples()
        results = evaluate_tokenizer(tokenizer, test_samples, args.verbose)
    
    # æ‰“å°æŠ¥å‘Š
    print_evaluation_report(results, os.path.basename(args.tokenizer_dir))
    
    # ä¿å­˜ç»“æœ
    if args.output:
        # ç§»é™¤ä¸èƒ½åºåˆ—åŒ–çš„å†…å®¹
        output_results = {
            'vocab_size': results['vocab_size'],
            'special_tokens': results['special_tokens'],
            'statistics': results['statistics'],
        }
        
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == '__main__':
    main()
