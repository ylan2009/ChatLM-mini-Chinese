#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦æ¸…æ´—è¯­æ–™æ–‡ä»¶ - ä¸“é—¨è§£å†³ tokenizers åº“å´©æºƒé—®é¢˜

è¿™ä¸ªè„šæœ¬æ¯” fix_tokenizer_training.py æ›´æ¿€è¿›ï¼Œä¼šå»é™¤æ‰€æœ‰å¯èƒ½å¯¼è‡´é—®é¢˜çš„å­—ç¬¦

ä½¿ç”¨æ–¹æ³•ï¼š
    python deep_clean_corpus.py --input ../data/my_corpus_clean.txt --output ../data/my_corpus_deep_clean.txt
"""

import argparse
import os
import re
import unicodedata
from tqdm import tqdm


def remove_zero_width_chars(text: str) -> str:
    """å»é™¤é›¶å®½å­—ç¬¦"""
    zero_width_chars = [
        '\u200b',  # é›¶å®½ç©ºæ ¼
        '\u200c',  # é›¶å®½éè¿æ¥ç¬¦
        '\u200d',  # é›¶å®½è¿æ¥ç¬¦
        '\ufeff',  # é›¶å®½éæ–­ç©ºæ ¼ï¼ˆBOMï¼‰
        '\u2060',  # å­—è¿æ¥ç¬¦
        '\u180e',  # è’™å¤æ–‡å…ƒéŸ³åˆ†éš”ç¬¦
    ]
    for char in zero_width_chars:
        text = text.replace(char, '')
    return text


def remove_control_chars(text: str) -> str:
    """å»é™¤æ‰€æœ‰æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦ï¼‰"""
    # ä¿ç•™å¸¸ç”¨çš„ç©ºç™½å­—ç¬¦ï¼šç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦
    return ''.join(char for char in text if unicodedata.category(char)[0] != 'C' or char in [' ', '\t', '\n'])


def remove_problematic_unicode(text: str) -> str:
    """å»é™¤å¯èƒ½å¯¼è‡´é—®é¢˜çš„ Unicode å­—ç¬¦"""
    # å»é™¤ç§æœ‰ä½¿ç”¨åŒºå­—ç¬¦
    text = re.sub(r'[\ue000-\uf8ff]', '', text)  # ç§æœ‰ä½¿ç”¨åŒº
    text = re.sub(r'[\U000f0000-\U000ffffd]', '', text)  # è¡¥å……ç§æœ‰ä½¿ç”¨åŒº-A
    text = re.sub(r'[\U00100000-\U0010fffd]', '', text)  # è¡¥å……ç§æœ‰ä½¿ç”¨åŒº-B
    
    # å»é™¤æŸäº›ç‰¹æ®Šç¬¦å·
    text = re.sub(r'[\ufff0-\uffff]', '', text)  # ç‰¹æ®Šç”¨é€”å­—ç¬¦
    
    return text


def normalize_whitespace(text: str) -> str:
    """æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦"""
    # å°†æ‰€æœ‰ç©ºç™½å­—ç¬¦ç»Ÿä¸€ä¸ºæ™®é€šç©ºæ ¼
    text = re.sub(r'[\t\r\n\f\v]', ' ', text)
    # å»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r' +', ' ', text)
    return text.strip()


def deep_clean_line(line: str) -> str:
    """æ·±åº¦æ¸…æ´—å•è¡Œæ–‡æœ¬"""
    # 1. å»é™¤é›¶å®½å­—ç¬¦
    line = remove_zero_width_chars(line)
    
    # 2. å»é™¤æ§åˆ¶å­—ç¬¦
    line = remove_control_chars(line)
    
    # 3. å»é™¤é—®é¢˜ Unicode å­—ç¬¦
    line = remove_problematic_unicode(line)
    
    # 4. æ ‡å‡†åŒ–ä¸º NFC å½¢å¼ï¼ˆæ¨èçš„ Unicode æ ‡å‡†åŒ–å½¢å¼ï¼‰
    line = unicodedata.normalize('NFC', line)
    
    # 5. æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
    line = normalize_whitespace(line)
    
    return line


def is_valid_line(line: str, min_length: int = 10, max_length: int = 10000) -> bool:
    """
    æ£€æŸ¥æ–‡æœ¬è¡Œæ˜¯å¦æœ‰æ•ˆ
    
    Args:
        line: æ–‡æœ¬è¡Œ
        min_length: æœ€å°é•¿åº¦
        max_length: æœ€å¤§é•¿åº¦
    
    Returns:
        æ˜¯å¦æœ‰æ•ˆ
    """
    if not line:
        return False
    
    # é•¿åº¦æ£€æŸ¥
    if len(line) < min_length or len(line) > max_length:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆå­—ç¬¦ï¼ˆä¸­æ–‡ã€è‹±æ–‡æˆ–æ•°å­—ï¼‰
    if not re.search(r'[\u4e00-\u9fa5a-zA-Z0-9]', line):
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šçš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯èƒ½æ˜¯ä¹±ç ï¼‰
    special_char_count = len(re.findall(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€""''ï¼ˆï¼‰()ã€ã€‘\[\]ã€Šã€‹<>]', line))
    if special_char_count > len(line) * 0.3:  # ç‰¹æ®Šå­—ç¬¦è¶…è¿‡ 30%
        return False
    
    return True


def deep_clean_corpus(input_file: str, output_file: str, min_length: int = 10, max_length: int = 10000):
    """
    æ·±åº¦æ¸…æ´—è¯­æ–™æ–‡ä»¶
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        min_length: æœ€å°è¡Œé•¿åº¦
        max_length: æœ€å¤§è¡Œé•¿åº¦
    """
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
    
    print(f"ğŸ“– è¯»å–è¾“å…¥æ–‡ä»¶: {input_file}")
    
    # è·å–æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(input_file)
    file_size_mb = file_size / (1024 * 1024)
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # å¤„ç†æ–‡ä»¶
    print("ğŸ§¹ æ·±åº¦æ¸…æ´—æ–‡æœ¬...")
    print("   - å»é™¤é›¶å®½å­—ç¬¦")
    print("   - å»é™¤æ§åˆ¶å­—ç¬¦")
    print("   - å»é™¤é—®é¢˜ Unicode å­—ç¬¦")
    print("   - æ ‡å‡†åŒ–ä¸º NFC å½¢å¼")
    print("   - æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦")
    
    valid_lines = 0
    invalid_lines = 0
    
    with open(input_file, 'r', encoding='utf-8', errors='ignore') as f_in:
        with open(output_file, 'w', encoding='utf-8') as f_out:
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            for line in tqdm(f_in, desc="å¤„ç†è¿›åº¦"):
                # æ·±åº¦æ¸…æ´—
                line = deep_clean_line(line)
                
                # æ£€æŸ¥æœ‰æ•ˆæ€§
                if is_valid_line(line, min_length=min_length, max_length=max_length):
                    f_out.write(line + '\n')
                    valid_lines += 1
                else:
                    invalid_lines += 1
    
    print(f"\nâœ… æœ‰æ•ˆè¡Œæ•°: {valid_lines:,}")
    print(f"âŒ æ— æ•ˆè¡Œæ•°: {invalid_lines:,}")
    
    if valid_lines + invalid_lines > 0:
        print(f"ğŸ“‰ è¿‡æ»¤ç‡: {invalid_lines / (valid_lines + invalid_lines) * 100:.2f}%")
    
    output_size = os.path.getsize(output_file)
    output_size_mb = output_size / (1024 * 1024)
    print(f"âœ… è¾“å‡ºæ–‡ä»¶å¤§å°: {output_size_mb:.2f} MB")
    
    print("\nğŸ‰ æ·±åº¦æ¸…æ´—å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description="æ·±åº¦æ¸…æ´—è¯­æ–™æ–‡ä»¶ - è§£å†³ tokenizers åº“å´©æºƒé—®é¢˜",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æ·±åº¦æ¸…æ´—è¯­æ–™æ–‡ä»¶
  python deep_clean_corpus.py \\
    --input ../data/my_corpus_clean.txt \\
    --output ../data/my_corpus_deep_clean.txt
  
  # ç„¶åè®­ç»ƒ tokenizer
  python train_tokenizer.py \\
    --method t5-base \\
    --wiki-file ../data/my_corpus_deep_clean.txt \\
    --output-dir ../model_save/my_tokenizer_wiki \\
    --vocab-size 40960 \\
    --batch-size 500
        """
    )
    
    parser.add_argument(
        '--input', '-i',
        type=str,
        required=True,
        help='è¾“å…¥æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        required=True,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--min-length',
        type=int,
        default=10,
        help='æœ€å°è¡Œé•¿åº¦ï¼ˆé»˜è®¤ï¼š10ï¼‰'
    )
    
    parser.add_argument(
        '--max-length',
        type=int,
        default=10000,
        help='æœ€å¤§è¡Œé•¿åº¦ï¼ˆé»˜è®¤ï¼š10000ï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ‰§è¡Œæ·±åº¦æ¸…æ´—
    deep_clean_corpus(
        input_file=args.input,
        output_file=args.output,
        min_length=args.min_length,
        max_length=args.max_length
    )


if __name__ == '__main__':
    main()
