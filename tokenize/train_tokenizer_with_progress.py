#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸¦è¿›åº¦ç›‘æ§çš„ SentencePiece Tokenizer è®­ç»ƒè„šæœ¬

åŠŸèƒ½ï¼š
1. å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦
2. æ˜¾ç¤ºè®­ç»ƒæ—¥å¿—
3. é¢„ä¼°è®­ç»ƒæ—¶é—´
4. æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡

ä½¿ç”¨æ–¹æ³•ï¼š
    python train_tokenizer_with_progress.py \
        --input ../data/my_corpus_clean.txt \
        --output ../model_save/my_tokenizer_sp \
        --vocab-size 40960

    ä½¿ç”¨é‡‡æ ·50wçš„æ•°æ®é›†

    # é‡‡æ ·æ•°æ®
    shuf ../data/my_corpus_clean.txt | head -n 500000 > ../data/my_corpus_sampled.txt

    python train_tokenizer_with_progress.py \
        --input ../data/my_corpus_sampled.txt \
        --output ../model_save/my_tokenizer_sp \
        --vocab-size 40960
"""

import os
import sys
import time
import argparse
import threading
import subprocess
from pathlib import Path


def count_lines(file_path):
    """å¿«é€Ÿç»Ÿè®¡æ–‡ä»¶è¡Œæ•°"""
    print("ğŸ“Š æ­£åœ¨ç»Ÿè®¡æ•°æ®é‡...")
    line_count = 0
    with open(file_path, 'r', encoding='utf-8') as f:
        for _ in f:
            line_count += 1
    return line_count


def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´"""
    if seconds < 60:
        return f"{int(seconds)} ç§’"
    elif seconds < 3600:
        minutes = int(seconds / 60)
        secs = int(seconds % 60)
        return f"{minutes} åˆ† {secs} ç§’"
    else:
        hours = int(seconds / 3600)
        minutes = int((seconds % 3600) / 60)
        return f"{hours} å°æ—¶ {minutes} åˆ†"


def monitor_progress(start_time, estimated_time):
    """ç›‘æ§è®­ç»ƒè¿›åº¦ï¼ˆåŠ¨ç”»æ•ˆæœï¼‰"""
    spinner = ['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â ']
    idx = 0
    
    while threading.current_thread().is_alive():
        elapsed = time.time() - start_time
        progress_pct = min(100, (elapsed / estimated_time) * 100)
        
        # è¿›åº¦æ¡
        bar_length = 40
        filled = int(bar_length * progress_pct / 100)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
        
        # æ˜¾ç¤ºä¿¡æ¯
        sys.stdout.write(f'\r  {spinner[idx]} è®­ç»ƒä¸­... [{bar}] {progress_pct:.1f}% | å·²ç”¨æ—¶: {format_time(elapsed)}')
        sys.stdout.flush()
        
        idx = (idx + 1) % len(spinner)
        time.sleep(0.1)


def train_sentencepiece(
    input_file,
    output_dir,
    vocab_size=40960,
    model_type='unigram',
    character_coverage=0.9995
):
    """
    è®­ç»ƒ SentencePiece tokenizerï¼ˆå¸¦è¿›åº¦ç›‘æ§ï¼‰
    """
    print("\n" + "="*70)
    print("ğŸš€ SentencePiece Tokenizer è®­ç»ƒ")
    print("="*70)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    model_prefix = os.path.join(output_dir, 'sentencepiece')
    
    # æ­¥éª¤ 1: åˆ†ææ•°æ®
    print("\nğŸ“Š æ­¥éª¤ 1/5: åˆ†æè®­ç»ƒæ•°æ®")
    print("-" * 70)
    
    file_size_mb = os.path.getsize(input_file) / (1024 * 1024)
    line_count = count_lines(input_file)
    
    print(f"  âœ“ æ–‡ä»¶è·¯å¾„: {input_file}")
    print(f"  âœ“ æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")
    print(f"  âœ“ æ•°æ®è¡Œæ•°: {line_count:,} è¡Œ")
    
    # é¢„ä¼°è®­ç»ƒæ—¶é—´ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
    estimated_minutes = max(5, int(file_size_mb / 200 * 15))
    estimated_seconds = estimated_minutes * 60
    
    print(f"  âœ“ é¢„ä¼°æ—¶é—´: {estimated_minutes}-{estimated_minutes*2} åˆ†é’Ÿ")
    
    # æ­¥éª¤ 2: å‡†å¤‡è®­ç»ƒå‚æ•°
    print("\nâš™ï¸  æ­¥éª¤ 2/5: å‡†å¤‡è®­ç»ƒå‚æ•°")
    print("-" * 70)
    print(f"  â€¢ è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  â€¢ æ¨¡å‹ç±»å‹: {model_type}")
    print(f"  â€¢ å­—ç¬¦è¦†ç›–ç‡: {character_coverage}")
    print(f"  â€¢ è®­ç»ƒæ¨¡å¼: å¤§è¯­æ–™åº“æ¨¡å¼")
    print(f"  â€¢ çº¿ç¨‹æ•°: 16")
    
    # æ­¥éª¤ 3: å¼€å§‹è®­ç»ƒ
    print("\nğŸ”¥ æ­¥éª¤ 3/5: è®­ç»ƒ SentencePiece æ¨¡å‹")
    print("-" * 70)
    print("  ğŸ’¡ æç¤º: SentencePiece ä¼šè¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼Œè¯·å…³æ³¨ä¸‹æ–¹ä¿¡æ¯\n")
    
    # è®­ç»ƒå‚æ•°
    train_cmd = [
        'python', '-c',
        f'''
import sentencepiece as spm
spm.SentencePieceTrainer.train(
    input="{input_file}",
    model_prefix="{model_prefix}",
    vocab_size={vocab_size},
    model_type="{model_type}",
    character_coverage={character_coverage},
    pad_id=0,
    unk_id=1,
    bos_id=2,
    eos_id=3,
    pad_piece="[PAD]",
    unk_piece="[UNK]",
    bos_piece="[BOS]",
    eos_piece="[EOS]",
    user_defined_symbols="[CLS],[SEP],[MASK]",
    normalization_rule_name="nfkc",
    remove_extra_whitespaces=True,
    max_sentence_length=16384,
    num_threads=16,
    train_extremely_large_corpus=True
)
print("\\nâœ“ è®­ç»ƒå®Œæˆï¼")
'''
    ]
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # æ‰§è¡Œè®­ç»ƒï¼ˆæ˜¾ç¤ºå®æ—¶è¾“å‡ºï¼‰
    try:
        process = subprocess.Popen(
            train_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1
        )
        
        # å®æ—¶æ˜¾ç¤ºè¾“å‡º
        for line in process.stdout:
            print(f"  {line.rstrip()}")
        
        process.wait()
        
        if process.returncode != 0:
            raise RuntimeError("è®­ç»ƒå¤±è´¥")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        return None
    
    # è®¡ç®—è®­ç»ƒè€—æ—¶
    elapsed_time = time.time() - start_time
    
    print("\n" + "-" * 70)
    print(f"  âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"  â±  è®­ç»ƒè€—æ—¶: {format_time(elapsed_time)}")
    
    # æ­¥éª¤ 4: éªŒè¯æ¨¡å‹
    print("\nâœ… æ­¥éª¤ 4/5: éªŒè¯è®­ç»ƒç»“æœ")
    print("-" * 70)
    
    try:
        import sentencepiece as spm
        sp = spm.SentencePieceProcessor()
        sp.load(f'{model_prefix}.model')
        
        actual_vocab_size = sp.get_piece_size()
        print(f"  âœ“ æ¨¡å‹æ–‡ä»¶: {model_prefix}.model")
        print(f"  âœ“ è¯æ±‡è¡¨å¤§å°: {actual_vocab_size}")
        
        # æµ‹è¯•ç¼–ç 
        test_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼Hello, World!"
        tokens = sp.encode(test_text, out_type=str)
        print(f"  âœ“ æµ‹è¯•ç¼–ç : '{test_text}'")
        print(f"    â†’ {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
        
    except Exception as e:
        print(f"  âš ï¸  éªŒè¯å¤±è´¥: {e}")
    
    # æ­¥éª¤ 5: è½¬æ¢ä¸º Hugging Face æ ¼å¼
    print("\nğŸ”„ æ­¥éª¤ 5/5: è½¬æ¢ä¸º Hugging Face Tokenizer")
    print("-" * 70)
    
    try:
        from transformers import T5Tokenizer
        
        tokenizer = T5Tokenizer(
            vocab_file=f'{model_prefix}.model',
            eos_token='[EOS]',
            unk_token='[UNK]',
            pad_token='[PAD]',
            bos_token='[BOS]',
            extra_ids=0,
        )
        
        # æ·»åŠ ç‰¹æ®Š token
        special_tokens = {
            'additional_special_tokens': ['[CLS]', '[SEP]', '[MASK]']
        }
        tokenizer.add_special_tokens(special_tokens)
        
        # ä¿å­˜
        tokenizer.save_pretrained(output_dir)
        
        print(f"  âœ“ å·²è½¬æ¢ä¸º T5Tokenizer")
        print(f"  âœ“ å·²ä¿å­˜åˆ°: {output_dir}")
        print(f"    - tokenizer_config.json")
        print(f"    - sentencepiece.model")
        print(f"    - special_tokens_map.json")
        
    except Exception as e:
        print(f"  âš ï¸  è½¬æ¢å¤±è´¥: {e}")
        print(f"  ğŸ’¡ ä½† SentencePiece æ¨¡å‹å·²ä¿å­˜: {model_prefix}.model")
    
    # æ˜¾ç¤ºæ€»ç»“
    print("\n" + "="*70)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
    print(f"  â€¢ æ•°æ®é‡: {line_count:,} è¡Œ ({file_size_mb:.2f} MB)")
    print(f"  â€¢ è¯æ±‡è¡¨å¤§å°: {actual_vocab_size}")
    print(f"  â€¢ è®­ç»ƒè€—æ—¶: {format_time(elapsed_time)}")
    print(f"  â€¢ è¾“å‡ºç›®å½•: {output_dir}")
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"  # å¿«é€Ÿæµ‹è¯•")
    print(f"  python quick_test_tokenizer.py {output_dir}")
    print(f"\n  # å®Œæ•´è¯„ä¼°")
    print(f"  python evaluate_tokenizer.py --tokenizer-dir {output_dir}")
    print("="*70 + "\n")
    
    return output_dir


def main():
    parser = argparse.ArgumentParser(
        description='è®­ç»ƒ SentencePiece Tokenizerï¼ˆå¸¦è¿›åº¦ç›‘æ§ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•
  python train_tokenizer_with_progress.py \\
      --input ../data/my_corpus_clean.txt \\
      --output ../model_save/my_tokenizer_sp \\
      --vocab-size 40960

  # ä½¿ç”¨ BPE æ¨¡å‹
  python train_tokenizer_with_progress.py \\
      --input ../data/my_corpus_clean.txt \\
      --output ../model_save/my_tokenizer_bpe \\
      --vocab-size 40960 \\
      --model-type bpe

  # ä½¿ç”¨é‡‡æ ·æ•°æ®ï¼ˆå¿«é€Ÿè®­ç»ƒï¼‰
  shuf ../data/my_corpus_clean.txt | head -n 500000 > ../data/sampled.txt
  python train_tokenizer_with_progress.py \\
      --input ../data/sampled.txt \\
      --output ../model_save/my_tokenizer_sampled \\
      --vocab-size 40960
        '''
    )
    
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        required=True,
        help='è¾“å‡ºç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--vocab-size',
        type=int,
        default=40960,
        help='è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤: 40960ï¼‰'
    )
    
    parser.add_argument(
        '--model-type',
        type=str,
        default='unigram',
        choices=['unigram', 'bpe', 'char', 'word'],
        help='æ¨¡å‹ç±»å‹ï¼ˆé»˜è®¤: unigramï¼‰'
    )
    
    parser.add_argument(
        '--character-coverage',
        type=float,
        default=0.9995,
        help='å­—ç¬¦è¦†ç›–ç‡ï¼ˆé»˜è®¤: 0.9995ï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import sentencepiece
        import transformers
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("\nè¯·å®‰è£…ä¾èµ–:")
        print("  pip install sentencepiece transformers")
        sys.exit(1)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        train_sentencepiece(
            input_file=args.input,
            output_dir=args.output,
            vocab_size=args.vocab_size,
            model_type=args.model_type,
            character_coverage=args.character_coverage
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
