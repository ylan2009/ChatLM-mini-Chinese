# LLaMA-Factory è®­ç»ƒæŒ‡å—ï¼ˆ3Ã—RTX 3080 20GBï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£… LLaMA-Factory
pip install llmtuner

# å®‰è£… DeepSpeedï¼ˆæ¨èï¼Œç”¨äºä¼˜åŒ–æ˜¾å­˜ï¼‰
pip install deepspeed

# å®‰è£…å…¶ä»–ä¾èµ–
pip install transformers datasets torch pyyaml tensorboard
```

### 2. å‡†å¤‡æ•°æ®

ç¡®ä¿ä½ çš„æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼š
- `data/my_train_dataset.parquet` - è®­ç»ƒæ•°æ®
- `data/my_valid_dataset.parquet` - éªŒè¯æ•°æ®ï¼ˆå¯é€‰ï¼‰

æ•°æ®æ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»åŒ…å« `input` åˆ—ï¼ˆè¾“å…¥æ–‡æœ¬ï¼‰
- å¿…é¡»åŒ…å« `target` åˆ—ï¼ˆç›®æ ‡æ–‡æœ¬ï¼‰

### 3. å¯åŠ¨è®­ç»ƒ

**æœ€ç®€å•çš„æ–¹å¼ï¼ˆæ¨èï¼‰ï¼š**

```bash
bash run_llamafactory_3x3080.sh
```

ç„¶åé€‰æ‹©è®­ç»ƒæ–¹å¼ï¼ˆæ¨èé€‰æ‹© 1 æˆ– 3ï¼‰ã€‚

---

## ğŸ“‹ å‘½ä»¤è¡Œæ–¹å¼

### æ–¹å¼1: llamafactory-cliï¼ˆæœ€ç®€å•ï¼‰

```bash
# è®¾ç½®GPU
export CUDA_VISIBLE_DEVICES=0,1,2

# å¯åŠ¨è®­ç»ƒ
llamafactory-cli train llamafactory_config_3x3080.yaml
```

### æ–¹å¼2: accelerate launchï¼ˆæ›´çµæ´»ï¼‰

```bash
# é¦–æ¬¡ä½¿ç”¨éœ€è¦é…ç½®
accelerate config

# å¯åŠ¨è®­ç»ƒ
export CUDA_VISIBLE_DEVICES=0,1,2
accelerate launch \
    --multi_gpu \
    --num_processes=3 \
    -m llmtuner.cli train llamafactory_config_3x3080.yaml
```

### æ–¹å¼3: deepspeedï¼ˆæœ€ä¼˜æ˜¾å­˜åˆ©ç”¨ï¼Œæ¨èï¼ï¼‰

```bash
export CUDA_VISIBLE_DEVICES=0,1,2
deepspeed \
    --num_gpus=3 \
    -m llmtuner.cli train llamafactory_config_3x3080.yaml
```

### æ–¹å¼4: torchrunï¼ˆæ ‡å‡†DDPï¼‰

```bash
export CUDA_VISIBLE_DEVICES=0,1,2
torchrun \
    --nproc_per_node=3 \
    -m llmtuner.cli train llamafactory_config_3x3080.yaml
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### æ ¸å¿ƒé…ç½®ï¼ˆ`llamafactory_config_3x3080.yaml`ï¼‰

```yaml
# æ‰¹æ¬¡å¤§å°é…ç½®ï¼ˆé’ˆå¯¹3Ã—20GB GPUä¼˜åŒ–ï¼‰
per_device_train_batch_size: 8      # æ¯å¼ å¡batch=8
gradient_accumulation_steps: 16     # æ¢¯åº¦ç´¯ç§¯16æ­¥
# æœ‰æ•ˆbatch size = 8 Ã— 3 Ã— 16 = 384

# æ˜¾å­˜ä¼˜åŒ–
bf16: true                          # ä½¿ç”¨BF16æ··åˆç²¾åº¦
gradient_checkpointing: true        # æ¢¯åº¦æ£€æŸ¥ç‚¹
deepspeed: ds_config_zero2.json     # DeepSpeed ZeRO-2

# å†…å­˜ä¼˜åŒ–ï¼ˆé’ˆå¯¹12GBå†…å­˜ï¼‰
preprocessing_num_workers: 2        # é™ä½å†…å­˜å ç”¨
dataloader_num_workers: 0           # ç¦ç”¨å¤šè¿›ç¨‹åŠ è½½
dataloader_pin_memory: false        # ç¦ç”¨pin memory
```

### å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥è°ƒæ•´ï¼š

```yaml
# æ–¹æ¡ˆ1: å‡å°batch size
per_device_train_batch_size: 4      # 8 -> 4
gradient_accumulation_steps: 32     # 16 -> 32ï¼ˆä¿æŒæœ‰æ•ˆbatchä¸å˜ï¼‰

# æ–¹æ¡ˆ2: å‡å°åºåˆ—é•¿åº¦
cutoff_len: 256                     # 512 -> 256

# æ–¹æ¡ˆ3: ä½¿ç”¨LoRAå¾®è°ƒï¼ˆæ˜¾å­˜å ç”¨æ›´å°ï¼‰
finetuning_type: lora
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
```

---

## ğŸ“Š é¢„æœŸæ˜¾å­˜å ç”¨

### ä½¿ç”¨å½“å‰é…ç½®ï¼ˆbatch_size=8, bf16, gradient_checkpointingï¼‰

| é…ç½® | å•å¡æ˜¾å­˜å ç”¨ | æ€»æ˜¾å­˜å ç”¨ |
|------|------------|-----------|
| **ä¸ä½¿ç”¨DeepSpeed** | ~18GB | ~54GB |
| **ä½¿ç”¨DeepSpeed ZeRO-2** | ~15GB | ~45GB |
| **ä½¿ç”¨LoRA** | ~10GB | ~30GB |

ä½ çš„ç¡¬ä»¶ï¼ˆ3Ã—20GBï¼‰å®Œå…¨å¤Ÿç”¨ï¼âœ…

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³ï¼ˆOOMï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# æ–¹æ¡ˆ1: å‡å°batch size
# ç¼–è¾‘ llamafactory_config_3x3080.yaml
per_device_train_batch_size: 4  # æ”¹ä¸º4

# æ–¹æ¡ˆ2: ä½¿ç”¨DeepSpeed ZeRO-3ï¼ˆæ›´æ¿€è¿›çš„æ˜¾å­˜ä¼˜åŒ–ï¼‰
# åˆ›å»º ds_config_zero3.jsonï¼Œç„¶åä¿®æ”¹é…ç½®
deepspeed: ds_config_zero3.json

# æ–¹æ¡ˆ3: ä½¿ç”¨LoRAå¾®è°ƒ
finetuning_type: lora
```

### Q2: å†…å­˜ä¸è¶³ï¼ˆRAMï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# å‡å°‘æ•°æ®é¢„å¤„ç†è¿›ç¨‹
preprocessing_num_workers: 1  # æ”¹ä¸º1

# å‡å°‘æœ€å¤§æ ·æœ¬æ•°
max_samples: 1000000  # é™åˆ¶ä¸º100ä¸‡

# å¯ç”¨æ•°æ®æµå¼åŠ è½½
streaming: true
```

### Q3: NCCL åˆå§‹åŒ–å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# æ·»åŠ ç¯å¢ƒå˜é‡
export NCCL_SHM_DISABLE=1
export NCCL_TIMEOUT=3600
export NCCL_IB_DISABLE=1

# æˆ–ä½¿ç”¨Glooåç«¯
export ACCELERATE_USE_GLOO=1
```

### Q4: è®­ç»ƒé€Ÿåº¦æ…¢

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```yaml
# 1. å¢å¤§batch sizeï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
per_device_train_batch_size: 12

# 2. ç¦ç”¨è¯„ä¼°ï¼ˆè®­ç»ƒæ—¶ï¼‰
evaluation_strategy: "no"

# 3. å‡å°‘æ—¥å¿—é¢‘ç‡
logging_steps: 100
save_steps: 10000

# 4. ä½¿ç”¨æ›´å¿«çš„ä¼˜åŒ–å™¨
optim: adamw_torch  # æ¯”adafactorå¿«ï¼Œä½†å ç”¨æ›´å¤šæ˜¾å­˜
```

---

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

### ä½¿ç”¨ TensorBoard

```bash
# å¯åŠ¨ TensorBoard
tensorboard --logdir=./logs/llamafactory_3x3080

# åœ¨æµè§ˆå™¨æ‰“å¼€
# http://localhost:6006
```

### æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ

```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨ gpustat
pip install gpustat
gpustat -i 1
```

---

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### é’ˆå¯¹ä½ çš„ç¡¬ä»¶ï¼ˆ3Ã—3080 20GB + 12GB RAMï¼‰

1. **æ¨èé…ç½®**ï¼ˆå½“å‰é…ç½®ï¼‰ï¼š
   - `per_device_train_batch_size: 8`
   - `gradient_accumulation_steps: 16`
   - ä½¿ç”¨ DeepSpeed ZeRO-2
   - é¢„æœŸé€Ÿåº¦ï¼š~1000 samples/s

2. **æ¿€è¿›é…ç½®**ï¼ˆæœ€å¤§åŒ–GPUåˆ©ç”¨ï¼‰ï¼š
   - `per_device_train_batch_size: 12`
   - `gradient_accumulation_steps: 10`
   - ä½¿ç”¨ DeepSpeed ZeRO-2
   - é¢„æœŸé€Ÿåº¦ï¼š~1500 samples/s

3. **ä¿å®ˆé…ç½®**ï¼ˆæœ€ç¨³å®šï¼‰ï¼š
   - `per_device_train_batch_size: 4`
   - `gradient_accumulation_steps: 32`
   - ä¸ä½¿ç”¨ DeepSpeed
   - é¢„æœŸé€Ÿåº¦ï¼š~600 samples/s

---

## ğŸ“ å®Œæ•´è®­ç»ƒæµç¨‹

```bash
# 1. å®‰è£…ä¾èµ–
pip install llmtuner deepspeed

# 2. æ£€æŸ¥æ•°æ®
ls -lh data/my_train_dataset.parquet

# 3. å¯åŠ¨è®­ç»ƒï¼ˆæ¨èä½¿ç”¨DeepSpeedï¼‰
export CUDA_VISIBLE_DEVICES=0,1,2
deepspeed --num_gpus=3 -m llmtuner.cli train llamafactory_config_3x3080.yaml

# 4. ç›‘æ§è®­ç»ƒï¼ˆå¦å¼€ä¸€ä¸ªç»ˆç«¯ï¼‰
tensorboard --logdir=./logs/llamafactory_3x3080

# 5. è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹ä¿å­˜åœ¨
ls -lh ./model_save/llamafactory_3x3080_output/
```

---

## ğŸ†š ä¸å…¶ä»–æ–¹å¼å¯¹æ¯”

| ç‰¹æ€§ | LLaMA-Factory | Transformers Trainer | æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ |
|------|--------------|---------------------|-------------|
| **æ˜“ç”¨æ€§** | â­â­â­â­â­ | â­â­â­â­ | â­â­ |
| **é…ç½®æ–¹å¼** | YAMLæ–‡ä»¶ | Pythonä»£ç  | Pythonä»£ç  |
| **ä»£ç é‡** | ~100è¡Œ | ~200è¡Œ | ~850è¡Œ |
| **æ˜¾å­˜ä¼˜åŒ–** | è‡ªåŠ¨ä¼˜åŒ– | éœ€æ‰‹åŠ¨é…ç½® | éœ€æ‰‹åŠ¨å®ç° |
| **å­¦ä¹ æ›²çº¿** | æœ€å¹³ç¼“ | å¹³ç¼“ | é™¡å³­ |
| **æ¨èåº¦** | âœ… å¼ºçƒˆæ¨è | âœ… æ¨è | ä»…ç ”ç©¶ç”¨ |

---

## ğŸ’¡ æ€»ç»“

**å¯¹äºä½ çš„ç¡¬ä»¶é…ç½®ï¼ˆ3Ã—3080 20GB + 12GB RAMï¼‰ï¼Œæ¨èä½¿ç”¨ï¼š**

```bash
# æœ€ç®€å•çš„å¯åŠ¨æ–¹å¼
bash run_llamafactory_3x3080.sh
# ç„¶åé€‰æ‹© 3 (ä½¿ç”¨ deepspeed)
```

**æˆ–è€…ç›´æ¥å‘½ä»¤è¡Œï¼š**

```bash
export CUDA_VISIBLE_DEVICES=0,1,2
deepspeed --num_gpus=3 -m llmtuner.cli train llamafactory_config_3x3080.yaml
```

è¿™ä¸ªé…ç½®å·²ç»é’ˆå¯¹ä½ çš„ç¡¬ä»¶ä¼˜åŒ–è¿‡ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼ğŸš€
