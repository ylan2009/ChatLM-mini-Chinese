#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Tokenizer æµ‹è¯•è„šæœ¬
ç”¨æ³•:
    python test_tokenizer.py
    python test_tokenizer.py --tokenizer model_save/my_tokenizer_sp
"""

import sys
sys.path.extend(['.', '..'])

import argparse
from transformers import T5Tokenizer, PreTrainedTokenizerFast, AutoTokenizer
from config import PROJECT_ROOT


def test_tokenizer(tokenizer_path):
    """æµ‹è¯• tokenizer çš„å„é¡¹åŠŸèƒ½"""
    
    print(f"\n{'='*100}")
    print(f"ğŸ”¤ Tokenizer åŠŸèƒ½æµ‹è¯•")
    print(f"{'='*100}\n")
    
    # åŠ è½½ tokenizer
    print(f"ğŸ“‚ åŠ è½½ Tokenizer: {tokenizer_path}")
    try:
        tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)
        print(f"âœ… ä½¿ç”¨ T5Tokenizer åŠ è½½æˆåŠŸ")
    except Exception as e:
        try:
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
            print(f"âœ… ä½¿ç”¨ AutoTokenizer åŠ è½½æˆåŠŸ")
        except:
            tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)
            print(f"âœ… ä½¿ç”¨ PreTrainedTokenizerFast åŠ è½½æˆåŠŸ")
    
    print(f"\n{'â”€'*100}")
    print(f"ğŸ“Š Tokenizer åŸºæœ¬ä¿¡æ¯")
    print(f"{'â”€'*100}")
    print(f"   è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")
    print(f"   æ¨¡å‹æœ€å¤§é•¿åº¦: {tokenizer.model_max_length:,}")
    print(f"   Padding token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    print(f"   EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    print(f"   UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})")
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "æˆ‘å–œæ¬¢æœºå™¨å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
        "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºå»ç©",
        "ChatGPTæ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å‘å±•è¿…é€Ÿ",
    ]
    
    print(f"\n{'â”€'*100}")
    print(f"ğŸ§ª åˆ†è¯æµ‹è¯•")
    print(f"{'â”€'*100}\n")
    
    for idx, text in enumerate(test_cases, 1):
        print(f"æµ‹è¯• #{idx}")
        print(f"   åŸæ–‡: {text}")
        
        # åˆ†è¯
        tokens = tokenizer.tokenize(text)
        print(f"   åˆ†è¯: {tokens}")
        print(f"   Token æ•°: {len(tokens)}")
        
        # ç¼–ç 
        ids = tokenizer.encode(text, add_special_tokens=False)
        print(f"   Token IDs: {ids[:10]}{'...' if len(ids) > 10 else ''}")
        
        # è§£ç 
        decoded = tokenizer.decode(ids)
        print(f"   è§£ç : {decoded}")
        
        # éªŒè¯ä¸€è‡´æ€§
        if decoded.replace(' ', '') == text.replace(' ', ''):
            print(f"   âœ… ç¼–ç -è§£ç ä¸€è‡´")
        else:
            print(f"   âš ï¸  ç¼–ç -è§£ç ä¸ä¸€è‡´")
        
        print()
    
    # æµ‹è¯•ç‰¹æ®Šæƒ…å†µ
    print(f"{'â”€'*100}")
    print(f"ğŸ”¬ ç‰¹æ®Šæƒ…å†µæµ‹è¯•")
    print(f"{'â”€'*100}\n")
    
    # 1. é•¿æ–‡æœ¬
    long_text = "æœºå™¨å­¦ä¹ " * 100
    tokens = tokenizer.tokenize(long_text)
    print(f"1. é•¿æ–‡æœ¬æµ‹è¯•")
    print(f"   åŸæ–‡é•¿åº¦: {len(long_text)} å­—ç¬¦")
    print(f"   Token æ•°: {len(tokens)}")
    print(f"   å‹ç¼©æ¯”: {len(long_text) / len(tokens):.2f}x")
    print()
    
    # 2. ç”Ÿåƒ»è¯
    rare_text = "é‡å­çº ç¼ ç°è±¡"
    tokens = tokenizer.tokenize(rare_text)
    ids = tokenizer.encode(rare_text, add_special_tokens=False)
    unk_count = sum(1 for id in ids if id == tokenizer.unk_token_id)
    print(f"2. ç”Ÿåƒ»è¯æµ‹è¯•")
    print(f"   åŸæ–‡: {rare_text}")
    print(f"   åˆ†è¯: {tokens}")
    print(f"   <unk> æ•°é‡: {unk_count}")
    if unk_count == 0:
        print(f"   âœ… æ— æœªçŸ¥è¯")
    else:
        print(f"   âš ï¸  åŒ…å« {unk_count} ä¸ªæœªçŸ¥è¯")
    print()
    
    # 3. è‹±æ–‡æ··åˆ
    mixed_text = "æˆ‘ä½¿ç”¨Pythonè¿›è¡Œæœºå™¨å­¦ä¹ "
    tokens = tokenizer.tokenize(mixed_text)
    print(f"3. ä¸­è‹±æ··åˆæµ‹è¯•")
    print(f"   åŸæ–‡: {mixed_text}")
    print(f"   åˆ†è¯: {tokens}")
    print(f"   Token æ•°: {len(tokens)}")
    print()
    
    # 4. æ•°å­—å’Œç¬¦å·
    symbol_text = "2024å¹´ï¼ŒAIæŠ€æœ¯å‘å±•è¿…é€Ÿï¼"
    tokens = tokenizer.tokenize(symbol_text)
    print(f"4. æ•°å­—ç¬¦å·æµ‹è¯•")
    print(f"   åŸæ–‡: {symbol_text}")
    print(f"   åˆ†è¯: {tokens}")
    print(f"   Token æ•°: {len(tokens)}")
    print()
    
    # æ‰¹é‡ç¼–ç æµ‹è¯•
    print(f"{'â”€'*100}")
    print(f"ğŸ“¦ æ‰¹é‡ç¼–ç æµ‹è¯•")
    print(f"{'â”€'*100}\n")
    
    batch_texts = [
        "çŸ­æ–‡æœ¬",
        "è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰é•¿åº¦çš„æ–‡æœ¬ç¤ºä¾‹",
        "è¿™æ˜¯ä¸€ä¸ªæ›´é•¿çš„æ–‡æœ¬ç¤ºä¾‹ï¼Œç”¨äºæµ‹è¯•æ‰¹é‡ç¼–ç æ—¶çš„ padding åŠŸèƒ½"
    ]
    
    # ä¸å¸¦ padding
    print("ä¸å¸¦ padding:")
    encoded = tokenizer(batch_texts, padding=False)
    for i, ids in enumerate(encoded['input_ids']):
        print(f"   æ–‡æœ¬ {i+1}: é•¿åº¦ {len(ids)}, IDs: {ids[:5]}...")
    
    print("\nå¸¦ padding:")
    encoded = tokenizer(batch_texts, padding=True)
    for i, ids in enumerate(encoded['input_ids']):
        print(f"   æ–‡æœ¬ {i+1}: é•¿åº¦ {len(ids)}, IDs: {ids[:5]}...")
    
    print(f"\n{'='*100}")
    print(f"âœ… æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*100}\n")
    
    # æ€»ç»“
    print(f"ğŸ“‹ æ€»ç»“:")
    print(f"   - Tokenizer ç±»å‹: {type(tokenizer).__name__}")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")
    print(f"   - æ”¯æŒä¸­æ–‡: âœ…")
    print(f"   - æ”¯æŒè‹±æ–‡: âœ…")
    print(f"   - æ”¯æŒæ•°å­—ç¬¦å·: âœ…")
    print(f"   - æ‰¹é‡ç¼–ç : âœ…")
    print()


def compare_tokenizers():
    """å¯¹æ¯”ä¸åŒ tokenizer çš„æ•ˆæœ"""
    
    print(f"\n{'='*100}")
    print(f"ğŸ” å¯¹æ¯”ä¸åŒ Tokenizer")
    print(f"{'='*100}\n")
    
    text = "æˆ‘å–œæ¬¢æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ "
    
    # 1. ä½ çš„ tokenizer
    print(f"1ï¸âƒ£ ä½ çš„ SentencePiece Tokenizer")
    print(f"{'â”€'*100}")
    try:
        tokenizer = T5Tokenizer.from_pretrained(PROJECT_ROOT + '/model_save/my_tokenizer_sp')
        tokens = tokenizer.tokenize(text)
        print(f"   åŸæ–‡: {text}")
        print(f"   åˆ†è¯: {tokens}")
        print(f"   Token æ•°: {len(tokens)}")
        print(f"   âœ… åŠ è½½æˆåŠŸ\n")
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}\n")
    
    # 2. å­—ç¬¦çº§ï¼ˆæ¨¡æ‹Ÿï¼‰
    print(f"2ï¸âƒ£ å­—ç¬¦çº§åˆ†è¯ï¼ˆæ¨¡æ‹Ÿï¼‰")
    print(f"{'â”€'*100}")
    char_tokens = list(text)
    print(f"   åŸæ–‡: {text}")
    print(f"   åˆ†è¯: {char_tokens}")
    print(f"   Token æ•°: {len(char_tokens)}")
    print(f"   âš ï¸  åºåˆ—é•¿åº¦æ˜¯ SentencePiece çš„ {len(char_tokens)/len(tokens):.1f}x\n")
    
    # 3. è‹±æ–‡ tokenizerï¼ˆå¯¹æ¯”ï¼‰
    print(f"3ï¸âƒ£ GPT-2 Tokenizerï¼ˆè‹±æ–‡ï¼‰")
    print(f"{'â”€'*100}")
    try:
        from transformers import GPT2Tokenizer
        gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        gpt2_tokens = gpt2_tokenizer.tokenize(text)
        print(f"   åŸæ–‡: {text}")
        print(f"   åˆ†è¯: {gpt2_tokens[:20]}{'...' if len(gpt2_tokens) > 20 else ''}")
        print(f"   Token æ•°: {len(gpt2_tokens)}")
        print(f"   âŒ ä¸é€‚åˆä¸­æ–‡ï¼Œåºåˆ—é•¿åº¦æ˜¯ SentencePiece çš„ {len(gpt2_tokens)/len(tokens):.1f}x\n")
    except Exception as e:
        print(f"   âš ï¸  éœ€è¦å®‰è£…: pip install transformers\n")
    
    print(f"{'='*100}\n")


def main():
    parser = argparse.ArgumentParser(
        description='æµ‹è¯• Tokenizer åŠŸèƒ½',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æµ‹è¯•é»˜è®¤ tokenizer
  python test_tokenizer.py
  
  # æµ‹è¯•æŒ‡å®š tokenizer
  python test_tokenizer.py --tokenizer model_save/my_tokenizer_sp
  
  # å¯¹æ¯”ä¸åŒ tokenizer
  python test_tokenizer.py --compare
        """
    )
    
    parser.add_argument(
        '--tokenizer', '-t',
        type=str,
        default=PROJECT_ROOT + '/model_save/my_tokenizer_sp',
        help='Tokenizer è·¯å¾„'
    )
    
    parser.add_argument(
        '--compare', '-c',
        action='store_true',
        help='å¯¹æ¯”ä¸åŒ tokenizer'
    )
    
    args = parser.parse_args()
    
    # æµ‹è¯• tokenizer
    test_tokenizer(args.tokenizer)
    
    # å¯¹æ¯”
    if args.compare:
        compare_tokenizers()


if __name__ == '__main__':
    main()
