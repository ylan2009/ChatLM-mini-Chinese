import signal
import sys
import os
import time
import shutil
from typing import Union
import platform 
import gc

from psutil import virtual_memory, cpu_count
import numpy as np
from torch.utils.data import DataLoader
import torch 
from rich.progress import Progress, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn
from transformers import PreTrainedTokenizerFast
from torch_optimizer import Adafactor

# import accelerate
from accelerate import Accelerator
from accelerate.utils import set_seed

# import è‡ªå®šä¹‰ç±»å’Œå‡½æ•°
from model.chat_model import TextToTextModel
from utils.logger import Logger
from model.dataset import LowMemDataset
from config import TrainConfig, TrainConfigSFT, T5ModelConfig
from utils.functions import (
    get_bleu4_score, 
    save_model_config, 
    get_free_space_of_disk, 
    my_average,
    get_path_of_suffix_files,
    get_T5_config,
)

class ChatTrainerLowMem:
    """
    ä½å†…å­˜ç‰ˆæœ¬çš„è®­ç»ƒå™¨ï¼Œé’ˆå¯¹16Gå†…å­˜ç¯å¢ƒä¼˜åŒ–
    
    ä¸»è¦ä¼˜åŒ–ï¼š
    1. å¼ºåˆ¶å…³é—­æ•°æ®é›†å†…å­˜ç¼“å­˜ï¼ˆkeep_in_memory=Falseï¼‰
    2. ä½¿ç”¨æ›´å°çš„batch_sizeï¼ˆé»˜è®¤1-2ï¼‰
    3. å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°æ¥è¡¥å¿å°batch size
    4. ç¦ç”¨DataLoaderçš„num_workersï¼Œé¿å…å¤šè¿›ç¨‹å†…å­˜å¼€é”€
    5. å®šæœŸæ¸…ç†GPUå’ŒCPUç¼“å­˜
    6. å‡å°è¯„ä¼°batch_size
    """
    def __init__(self, train_config: Union[TrainConfig, TrainConfigSFT], model_config: T5ModelConfig, ) -> None:
        
        self.train_config = train_config
        self.model_config = model_config

        # file_name=Noneä¼šè‡ªåŠ¨ç”Ÿæˆä»¥å½“å‰æ—¥æœŸå‘½åçš„logæ–‡ä»¶å
        self.logger = Logger('chat_trainer_low_mem', std_out=True, save2file=True, file_name=None)

        self.model = None
        self.accelerator = None

        signal.signal(signal.SIGINT, self.process_exit_handler)

        self.is_win_platform = True if platform.system().lower() == 'windows' else False

        torch.manual_seed(train_config.seed)
        torch.cuda.manual_seed_all(train_config.seed)
    
    def log_memory_usage(self, stage: str = "") -> None:
        """è®°å½•å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        mem = virtual_memory()
        used_gb = (mem.total - mem.available) / (1024 ** 3)
        total_gb = mem.total / (1024 ** 3)
        percent = mem.percent
        
        msg = f"[å†…å­˜ç›‘æ§{(' - ' + stage) if stage else ''}] å·²ç”¨: {used_gb:.2f}GB / {total_gb:.2f}GB ({percent:.1f}%)"
        self.logger.info(msg, save_to_file=True)
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / (1024 ** 3)
                reserved = torch.cuda.memory_reserved(i) / (1024 ** 3)
                self.logger.info(f"  GPU {i}: å·²åˆ†é… {allocated:.2f}GB, å·²ä¿ç•™ {reserved:.2f}GB", save_to_file=True)
    
    def clear_memory(self) -> None:
        """æ¸…ç†å†…å­˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def process_exit_handler(self, signal_received, frame) -> None:
        '''
        è¿›ç¨‹é€€å‡ºæ—¶çš„æ“ä½œï¼Œä¿å­˜æ¨¡å‹
        '''
        if self.accelerator and self.model:
            ask = "you are pressed `ctrl+c`,  do you want to save checkpoint? Yes (y) or No (n)"
            self.accelerator.print(ask)
            ins = input()
            
            if ins.lower() in ('yes', 'y'):

                suffix =  'exit_save_{}'.format(str(time.strftime('%Y%m%d%H%M%S', time.localtime())))

                self.accelerator.wait_for_everyone()
                self.accelerator.save_state(output_dir=self.train_config.train_state_dir)

                self.accelerator.print('model ckeck point has been saved in {}'.format(self.train_config.train_state_dir))
        
            sys.exit(0)
        else:
            print('process not in trainingg, exit.')
            sys.exit(0)

    def save_model(self, suffix: Union[str, int]) -> None:
        '''ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶
        æ³¨æ„ï¼šsave_modelä¸èƒ½æ”¾åˆ°is_main_processé‡Œé¢
        e.g:
        >>> self.save_model(epoch) # åœ¨è¿™é‡Œä½¿ç”¨
        >>> if accelerator.is_main_process:
        >>>     do_somthing()
        '''
        if self.model and self.accelerator:

            # å…ˆwait_for_everyoneï¼Œå†ä¿å­˜
            self.accelerator.wait_for_everyone()

            if self.accelerator.is_main_process:
                save_path = self.train_config.model_file.format(suffix)
                save_dir = os.path.dirname(save_path)
                if save_dir:
                    os.makedirs(save_dir, exist_ok=True)

                # å…ˆåšä¸€æ¬¡æ¸…ç†ï¼Œé¿å…ç£ç›˜è¢« checkpoint å †æ»¡ï¼ˆæœ¬é¡¹ç›®å•ä¸ª .bin ~ 0.7-0.8GBï¼‰
                self._cleanup_checkpoints(keep_latest_n=self.train_config.keep_latest_n_ckp)

                # ç£ç›˜ç©ºé—´æ£€æŸ¥ï¼šè‡³å°‘é¢„ç•™ 2GBï¼Œé¿å…å†™åˆ°ä¸€åŠå¤±è´¥ï¼ˆä½ é‡åˆ°çš„å°±æ˜¯ç£ç›˜ 100% å¯¼è‡´å†™å¤±è´¥ï¼‰
                try:
                    free_gb = shutil.disk_usage(save_dir or ".").free / (1024 ** 3)
                except Exception:
                    free_gb = 0.0

                if free_gb < 2.0:
                    self.accelerator.print(
                        f"[WARN] ç£ç›˜å‰©ä½™ç©ºé—´ä¸è¶³({free_gb:.2f}GB)ï¼Œè·³è¿‡ä¿å­˜æ¨¡å‹: {save_path}"
                    )
                    return

                unwrap_model = self.accelerator.unwrap_model(self.model)
                model_dict = self.accelerator.get_state_dict(unwrap_model)

                # å†™å…¥å¤±è´¥æ—¶å¸¸è§æŠ¥é”™ï¼šPytorchStreamWriter failed writing file / unexpected pos
                # é€šå¸¸æ˜¯ç£ç›˜æ»¡æˆ–åº•å±‚æ–‡ä»¶ç³»ç»Ÿ I/O å¼‚å¸¸ã€‚è¿™é‡Œåšä¸¤å±‚é˜²æŠ¤ï¼š
                # 1) ä½¿ç”¨æ—§ç‰ˆåºåˆ—åŒ–ï¼Œé¿å… zip writer åœ¨éƒ¨åˆ†æ–‡ä»¶ç³»ç»Ÿä¸Šæ›´å®¹æ˜“è§¦å‘ unexpected pos
                # 2) ä¿å­˜å¤±è´¥æ—¶åˆ é™¤ä¸å®Œæ•´æ–‡ä»¶ï¼Œé¿å…ä¸‹æ¬¡ç»§ç»­æŠŠç£ç›˜å æ»¡
                try:
                    torch.save(
                        model_dict,
                        save_path,
                        _use_new_zipfile_serialization=False,
                    )
                except Exception as e:
                    # å°è¯•åˆ é™¤åŠæˆå“
                    try:
                        if os.path.exists(save_path):
                            os.remove(save_path)
                    except Exception:
                        pass
                    raise e

    def _cleanup_checkpoints(self, keep_latest_n: int = 8) -> None:
        """
        æ¸…ç†æ—§ checkpointï¼Œé¿å…ç£ç›˜ç©ºé—´è¢«å†™æ»¡ã€‚

        è§„åˆ™ï¼š
        - ä¿ç•™ `chat_small_t5.best.bin`
        - ä¿ç•™æœ€è¿‘ keep_latest_n ä¸ª `chat_small_t5.epoch_*_latest.bin`
        - ä¿ç•™åŒ…å« `exit_save` çš„åº”æ€¥ä¿å­˜
        """
        try:
            model_file_tpl = self.train_config.model_file.replace("\\", "/")
            model_dir = "/".join(model_file_tpl.split("/")[:-1]) or "."
            if not os.path.isdir(model_dir):
                return

            best_path = self.train_config.model_file.format("best")
            keep = set([best_path])

            candidates = []
            for fn in os.listdir(model_dir):
                if not fn.endswith(".bin"):
                    continue
                full = os.path.join(model_dir, fn)
                if "exit_save" in fn:
                    keep.add(full)
                    continue
                # åªæ¸…ç† epoch latest æ–‡ä»¶ï¼Œé¿å…è¯¯åˆ å…¶å®ƒäº§ç‰©
                if ".epoch_" in fn and fn.endswith("_latest.bin"):
                    candidates.append(full)

            # æŒ‰ä¿®æ”¹æ—¶é—´ä»æ–°åˆ°æ—§æ’åº
            candidates.sort(key=lambda p: os.path.getmtime(p), reverse=True)
            keep.update(candidates[:keep_latest_n])

            for p in candidates[keep_latest_n:]:
                if p not in keep and os.path.exists(p):
                    os.remove(p)
        except Exception:
            # æ¸…ç†å¤±è´¥ä¸å½±å“è®­ç»ƒ
            return

    
    def delete_early_checkpoint(self, epoch: int, keep_latest_n: int=3,) -> None:
        '''
        åˆ é™¤æœ€æ—©çš„æ¨¡å‹ï¼Œæœ€ä¿ç•™æœ€è¿‘keep_latest_nä¸ªæ¨¡å‹æ–‡ä»¶
        '''
        model_save_path = self.train_config.model_file
        model_save_path = model_save_path.replace('\\', '/')    # é’ˆå¯¹winçš„è·¯å¾„ï¼Œå°†\\æ›¿æ¢ä¸º/
        model_save_path = '/'.join(model_save_path.split('/')[0: -1])   # åˆ é™¤æœ«å°¾æ–‡ä»¶ååç¼€
        
        model_files = get_path_of_suffix_files(model_save_path, suffix='.bin', with_create_time=True)
        
        # è¿›ç¨‹å¼‚å¸¸é€€å‡ºä¿å­˜æ¨¡å‹æ–‡ä»¶ä¸åœ¨åˆ é™¤èŒƒå›´
        train_save_model_fils = []
        for item in model_files:
            if 'exit_save' not in item[0]:

                # å¤§äºå½“å‰epochçš„æ–‡ä»¶ä¸ä¸åˆ é™¤
                f_epoch = int(item[0].split('.')[-2])
                if epoch >= f_epoch:
                    print(epoch, f_epoch, item)
                    train_save_model_fils.append(item)

        train_save_model_fils.sort(key=lambda x: x[1])  # æŒ‰ç…§æ—¶é—´ä»å°åˆ°å¤§æ’åº

        if len(train_save_model_fils) <= keep_latest_n:
            return
        
        to_delete_files = train_save_model_fils[0: -keep_latest_n]
        for item in to_delete_files:
            os.remove(item[0])

    
    def train(self, is_keep_training: bool=False, is_finetune: bool=False) -> None:
        '''
        ä½å†…å­˜ç‰ˆæœ¬çš„è®­ç»ƒå‡½æ•°
        
        is_keep_training: æ˜¯å¦ä»æ–­ç‚¹å¤„åŠ è½½çŠ¶æ€ç»§ç»­è®­ç»ƒ
        is_finetune: æ˜¯å¦å¾®è°ƒï¼Œå¾®è°ƒçš„è¯å¯èƒ½éœ€è¦å†»ç»“éƒ¨åˆ†å‚æ•°
        '''
        log = self.logger
        train_config = self.train_config
        save_steps = self.train_config.save_steps
        logging_steps = self.train_config.logging_steps

        # ã€å…³é”®ä¼˜åŒ–1ã€‘æ¢¯åº¦ç´¯è®¡æ­¥æ•°ï¼šå¹³è¡¡å†…å­˜å’Œè®­ç»ƒæ•ˆæœ
        # æ ¹æ®å¯ç”¨å†…å­˜å’ŒGPUæ•°é‡æ™ºèƒ½è°ƒæ•´æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        unuse_mem_gb = virtual_memory().available / (1024 ** 3)
        num_gpus = torch.cuda.device_count()
        
        # ğŸš€ æ–°ç­–ç•¥ï¼šåœ¨ä¿è¯GPUæ˜¾å­˜å ç”¨çš„å‰æä¸‹ï¼Œä¼˜å…ˆé™ä½å†…å­˜ä½¿ç”¨
        # - å¯ç”¨å†…å­˜<10GBï¼šä½¿ç”¨å¤§æ¢¯åº¦ç´¯ç§¯ï¼ˆ4ï¼‰ï¼Œå‡å°‘å†…å­˜å ç”¨
        # - å¯ç”¨å†…å­˜>=10GBï¼šä½¿ç”¨é…ç½®çš„æ¢¯åº¦ç´¯ç§¯ï¼ˆ2ï¼‰ï¼Œå……åˆ†åˆ©ç”¨GPUæ˜¾å­˜
        if unuse_mem_gb < 10:
            # ä½å†…å­˜æ¨¡å¼ï¼šå¢å¤§æ¢¯åº¦ç´¯ç§¯ï¼Œå‡å°‘å†…å­˜å ç”¨
            # è™½ç„¶æ¢¯åº¦ç´¯ç§¯å¤§äº†ï¼Œä½†batch_sizeä¹Ÿä¼šç›¸åº”å¢å¤§ï¼ŒGPUæ˜¾å­˜å ç”¨ä¸å˜
            accumulation_steps = 4
        else:
            # å†…å­˜å……è¶³ï¼ˆ>=10GBï¼‰ï¼šä½¿ç”¨é…ç½®çš„æ¢¯åº¦ç´¯ç§¯ï¼Œå……åˆ†åˆ©ç”¨GPUæ˜¾å­˜
            accumulation_steps = train_config.gradient_accumulation_steps

        set_seed(train_config.seed)

        accelerator = Accelerator(
            mixed_precision=train_config.mixed_precision,       # æ··åˆç²¾åº¦
            gradient_accumulation_steps=accumulation_steps,     # æ¢¯åº¦ç´¯ç§¯
            project_dir=train_config.train_state_dir,
        )

        # ã€é‡è¦ã€‘æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦è·å–å†…å­˜ä¿¡æ¯ï¼Œä¸èƒ½åªåœ¨ä¸»è¿›ç¨‹ä¸­å®šä¹‰
        unuse_mem = virtual_memory().available / (1024 ** 3)  # å•ä½ï¼šGB
        unuse_disk = get_free_space_of_disk('./')

        if accelerator.is_main_process:
            log.info('=' * 80, save_to_file=True)
            log.info('ä½å†…å­˜æ¨¡å¼è®­ç»ƒ - é’ˆå¯¹16Gå†…å­˜ä¼˜åŒ–', save_to_file=True)
            log.info('=' * 80, save_to_file=True)
            log.info('cpu memory available: {:.2f} GB, disk space available: {:.2f} GB'.format(unuse_mem, unuse_disk), save_to_file=True)
            log.info('ä½¿ç”¨LowMemDataset: æ”¯æŒå¤šGPU + ä½å†…å­˜æ¨¡å¼ï¼ŒæŒ‰éœ€ä»ç£ç›˜è¯»å–æ•°æ®', save_to_file=True)
            log.info('gradient accumulation steps: {} (å¢åŠ ä»¥è¡¥å¿å°batch size)'.format(accumulation_steps), save_to_file=True)
            log.info('operation: {}, keep training: {}, loading datasets ...'.format('finetune' if is_finetune else 'train', is_keep_training))
            
            self.log_memory_usage("è®­ç»ƒå¼€å§‹å‰")
            
            # éªŒè¯æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(train_config.train_file):
                raise FileNotFoundError(
                    f'è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_config.train_file}\n'
                    f'è¯·å…ˆè¿è¡Œ: python prepare_sft_data.py ç”Ÿæˆè®­ç»ƒæ•°æ®'
                )
            if not os.path.exists(train_config.validation_file):
                raise FileNotFoundError(
                    f'éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_config.validation_file}\n'
                    f'è¯·å…ˆè¿è¡Œ: python prepare_sft_data.py ç”Ÿæˆè®­ç»ƒæ•°æ®'
                )
            
            # å¦‚æœæ˜¯å¾®è°ƒï¼ŒéªŒè¯é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦å­˜åœ¨
            if is_finetune and not os.path.exists(train_config.finetune_from_ckp_file):
                raise FileNotFoundError(
                    f'é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {train_config.finetune_from_ckp_file}\n'
                    f'è¯·å…ˆå®Œæˆé¢„è®­ç»ƒï¼Œæˆ–æ£€æŸ¥config.pyä¸­çš„finetune_from_ckp_fileè·¯å¾„æ˜¯å¦æ­£ç¡®\n'
                    f'é¢„è®­ç»ƒå‘½ä»¤: accelerate launch --multi_gpu --num_processes 2 ./train.py train'
                )

        # ã€å…³é”®ä¼˜åŒ–3ã€‘æ ¹æ®å†…å­˜æƒ…å†µåŠ¨æ€è°ƒæ•´num_workers
        # ğŸš€ å…³é”®ï¼šnum_workers=0 å¯ä»¥èŠ‚çœ 2-4GB å†…å­˜ï¼ˆé¿å…å¤šè¿›ç¨‹å¤åˆ¶æ•°æ®ï¼‰
        # ä½†ä¼šé™ä½æ•°æ®åŠ è½½é€Ÿåº¦ï¼Œéœ€è¦é€šè¿‡å¢å¤§ batch_size æ¥è¡¥å¿
        if unuse_mem < 10:
            # ä½å†…å­˜ï¼ˆ<10GBï¼‰ï¼šå¼ºåˆ¶ç¦ç”¨å¤šè¿›ç¨‹ï¼ŒèŠ‚çœ 2-4GB å†…å­˜
            num_workers = 0
        else:
            # å†…å­˜å……è¶³ï¼ˆ>=10GBï¼‰ï¼šå¯ç”¨å°‘é‡å¤šè¿›ç¨‹åŠ é€Ÿ
            # æ³¨æ„ï¼šæ¯ä¸ª worker ä¼šå¤åˆ¶ä¸€ä»½æ•°æ®ï¼Œå ç”¨é¢å¤–å†…å­˜
            gpu_cnt = torch.cuda.device_count()
            # ğŸš€ ä¼˜åŒ–ï¼šå‡å°‘ worker æ•°é‡ï¼ˆä» 8 é™åˆ° 4ï¼‰ï¼ŒèŠ‚çœå†…å­˜
            num_workers = min(4, int(1 * gpu_cnt)) if gpu_cnt > 0 else 0

        # ä½¿ç”¨LowMemDatasetï¼Œæ”¯æŒå¤šGPU + ä½å†…å­˜æ¨¡å¼
        # ultra_low_mem=True: æ¯æ¬¡è¯»å–æ—¶é‡æ–°æ‰“å¼€æ–‡ä»¶ï¼Œé¿å…PyArrowç¼“å­˜ç´¯ç§¯
        # è¿™ä¼šç¨å¾®é™ä½é€Ÿåº¦ï¼Œä½†èƒ½æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨ï¼ˆèŠ‚çœ 5-8GBï¼‰
        # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šé™ä½é˜ˆå€¼åˆ° 10GBï¼Œæ›´æ¿€è¿›åœ°å¯ç”¨è¶…ä½å†…å­˜æ¨¡å¼
        use_ultra_low_mem = unuse_mem < 10  # å¯ç”¨å†…å­˜<10GBæ—¶å¯ç”¨è¶…ä½å†…å­˜æ¨¡å¼
        
        if accelerator.is_main_process:
            log.info(f'ultra_low_memæ¨¡å¼: {use_ultra_low_mem} (å¯ç”¨å†…å­˜: {unuse_mem:.2f}GB)', save_to_file=True)
            if use_ultra_low_mem:
                log.info('  âš ï¸  è¶…ä½å†…å­˜æ¨¡å¼ä¼šé™ä½æ•°æ®åŠ è½½é€Ÿåº¦ï¼Œä½†å¯èŠ‚çœ 5-8GB å†…å­˜', save_to_file=True)
        
        train_dataset = LowMemDataset(
            parquet_file=train_config.train_file,
            tokenizer_dir=train_config.tokenizer_dir,
            max_seq_len=train_config.max_seq_len,
            ultra_low_mem=use_ultra_low_mem,
        )
        valid_dataset = LowMemDataset(
            parquet_file=train_config.validation_file,
            tokenizer_dir=train_config.tokenizer_dir,
            max_seq_len=train_config.max_seq_len,
            ultra_low_mem=use_ultra_low_mem,
        )

        # ã€å…³é”®ä¼˜åŒ–4ã€‘æ ¹æ®å¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´batch_size
        # ğŸš€ æ–°ç­–ç•¥ï¼šåœ¨ä½å†…å­˜æƒ…å†µä¸‹ï¼Œå¢å¤§ batch_size æ¥è¡¥å¿ num_workers=0 çš„é€Ÿåº¦æŸå¤±
        # åŸç†ï¼šnum_workers=0 èŠ‚çœäº† 2-4GB å†…å­˜ï¼Œå¯ä»¥ç”¨æ¥å¢å¤§ batch_size
        if unuse_mem < 10:
            # ä½å†…å­˜ï¼ˆ<10GBï¼‰ï¼šä½¿ç”¨é…ç½®çš„ batch_sizeï¼Œå……åˆ†åˆ©ç”¨ GPU æ˜¾å­˜
            # è™½ç„¶å†…å­˜ç´§å¼ ï¼Œä½†é€šè¿‡ ultra_low_mem=True + num_workers=0 èŠ‚çœäº†å†…å­˜
            batch_size = train_config.batch_size_per_gpu
            eval_batch_size = batch_size * 2
            if accelerator.is_main_process:
                log.info(f'âš ï¸  ä½å†…å­˜æ¨¡å¼ï¼ˆå¯ç”¨å†…å­˜<10GBï¼‰ï¼Œä½¿ç”¨ batch_size={batch_size}', save_to_file=True)
                log.info(f'  é€šè¿‡ ultra_low_mem=True + num_workers=0 èŠ‚çœå†…å­˜ï¼Œä¿æŒ GPU æ˜¾å­˜å ç”¨', save_to_file=True)
        else:
            # å†…å­˜å……è¶³ï¼ˆ>=10GBï¼‰ï¼šä½¿ç”¨é…ç½®çš„batch_sizeï¼Œå……åˆ†åˆ©ç”¨GPUæ˜¾å­˜
            batch_size = train_config.batch_size_per_gpu
            eval_batch_size = batch_size * 2
            if accelerator.is_main_process:
                log.info(f'âœ… å†…å­˜å……è¶³ï¼ˆå¯ç”¨å†…å­˜>=10GBï¼‰ï¼Œä½¿ç”¨é…ç½®çš„batch_size={batch_size}', save_to_file=True)

        if accelerator.is_main_process:
            log.info(f'batch_size_per_gpu: {batch_size} (åŸé…ç½®: {train_config.batch_size_per_gpu})', save_to_file=True)
            log.info(f'eval_batch_size: {eval_batch_size}', save_to_file=True)

        # æ ¹æ®å†…å­˜æƒ…å†µå†³å®šæ˜¯å¦å¯ç”¨pin_memory
        # ğŸš€ å…³é”®ï¼špin_memory ä¼šå ç”¨é¢å¤–å†…å­˜ï¼ˆçº¦ 1-2GBï¼‰ï¼Œä½å†…å­˜æ—¶ç¦ç”¨
        use_pin_memory = unuse_mem >= 10  # å†…å­˜å……è¶³æ—¶å¯ç”¨pin_memoryåŠ é€ŸGPUä¼ è¾“
        
        train_dataloader = DataLoader(
            train_dataset, 
            batch_size=batch_size,  
            shuffle=True,
            collate_fn=train_dataset.collate_fn,
            pin_memory=use_pin_memory,  # å†…å­˜å……è¶³æ—¶å¯ç”¨
            num_workers=num_workers,
        )
        valid_dataloader = DataLoader(
            valid_dataset, 
            batch_size=eval_batch_size,
            shuffle=False,
            collate_fn=valid_dataset.collate_fn,
            pin_memory=use_pin_memory,  # å†…å­˜å……è¶³æ—¶å¯ç”¨
            num_workers=num_workers,
        )

        device = accelerator.device
        log.info('using device: {} '.format(str(device)), save_to_file=True)
        

        # T5: All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
        tokenizer = train_dataset.tokenizer
        decoder_start_token_id = tokenizer.pad_token_id

        # for t5, set decoder_start_token_id = pad_token_id
        t5_config = get_T5_config(T5ModelConfig(), vocab_size=len(tokenizer), decoder_start_token_id=decoder_start_token_id, eos_token_id=tokenizer.eos_token_id)

        model = TextToTextModel(t5_config)

        # å¾®è°ƒåŠ è½½çš„æ¨¡å‹å¹¶å†»ç»“embeddingå’Œencoder
        if is_finetune:
            if accelerator.is_main_process:
                log.info(f'åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {train_config.finetune_from_ckp_file}', save_to_file=True)
            
            model.load_state_dict(torch.load(train_config.finetune_from_ckp_file, map_location='cpu'))
            
            # å†»ç»“embeddingå’Œencoderï¼Œåªè®­ç»ƒdecoder
            layers_to_freeze = [model.shared, model.encoder]
            
            trainable_params = 0
            total_params = 0
            
            for layer in layers_to_freeze:
                for param in layer.parameters():
                    param.requires_grad = False
            
            # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
            for param in model.parameters():
                total_params += param.numel()
                if param.requires_grad:
                    trainable_params += param.numel()
            
            if accelerator.is_main_process:
                log.info(f'SFTå¾®è°ƒ: å†»ç»“embeddingå’Œencoderï¼Œåªè®­ç»ƒdecoder', save_to_file=True)
                log.info(f'å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / æ€»å‚æ•°: {total_params:,} ({100*trainable_params/total_params:.2f}%)', save_to_file=True)

        # ä¿å­˜æ¨¡å‹é…ç½®ï¼Œæ–¹ä¾¿ä¿®æ”¹é…ç½®åæ¢å¤
        save_model_config(t5_config.to_diff_dict(), train_config.model_config_file)
        
        # T5è®­ç»ƒï¼Œè®ºæ–‡æ¨èä½¿ç”¨Adafactor
        optimizer = Adafactor(params=model.parameters(), lr=train_config.learn_rate)

        
        # è·å–å½“å‰æœºå™¨æœ‰å¤šå°‘ä¸ªGPUï¼Œé»˜è®¤å…¨éƒ¨ä½¿ç”¨
        num_gpus_used = accelerator.state.num_processes

        # å•æœºå¤šå¡ï¼Œæ¯ä¸ªstepæ€»å…±çš„batch_size = batch_size_per_gpu * num_gpus_used
        # total_batch_size åˆå§‹åŒ–ä¸ºbatch_size_per_gpuçœŸçš„åªæœ‰CPUçš„æƒ…å†µ
        total_batch_size = batch_size
        total_eval_batch_size = eval_batch_size
        if num_gpus_used >= 1:
            total_batch_size = num_gpus_used * batch_size
            total_eval_batch_size = num_gpus_used * eval_batch_size

        steps_per_epoch = int(np.ceil(len(train_dataset) // total_batch_size))
        eval_steps = int(np.ceil(len(valid_dataset) // total_eval_batch_size))

        if accelerator.is_main_process:
            log.info('train dataset size: {}, steps per epoch:{}; validation dataset size: {}, steps per validation: {}; datalodater num_workers: {}.'\
                    .format(len(train_dataset), steps_per_epoch, len(valid_dataset), eval_steps, num_workers), save_to_file=True)
            self.log_memory_usage("æ•°æ®é›†åŠ è½½å")

        
        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(
                optimizer=optimizer, 
                max_lr=train_config.div_factor * train_config.learn_rate, 
                epochs=train_config.epochs, 
                steps_per_epoch=int(np.ceil( len(train_dataset) / (batch_size * accumulation_steps) )),  # æ¢¯åº¦ç´¯ç§¯ç›¸å½“äºå¢å¤§äº†batch_size
                div_factor=train_config.div_factor,
                cycle_momentum=False,
            )
        
        model, optimizer, lr_scheduler, train_dataloader, valid_dataloader = accelerator.prepare(
                model, 
                optimizer,
                lr_scheduler, 
                train_dataloader, 
                valid_dataloader,
            )
        
        if is_keep_training:
            accelerator.load_state(input_dir=train_config.train_state_dir)
            accelerator.register_for_checkpointing(lr_scheduler)
        
        self.model = model
        self.accelerator = accelerator
        
        if accelerator.is_main_process:
            self.log_memory_usage("æ¨¡å‹åŠ è½½å")
        
        best_bleu4 = 0.0
        best_epoch = 0
        epoch_loss_list = []

        # æ·»åŠ è¿›åº¦æ¡ï¼Œåªåœ¨ä¸»è¿›ç¨‹æ›´æ–°
        if accelerator.is_main_process:
            progress = Progress(TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeRemainingColumn(),
                TimeElapsedColumn(),
                TextColumn("[bold blue]{task.fields[show_info]}"),
                refresh_per_second=1,  # æ¯1ç§’é’Ÿæ›´æ–°ä¸€æ¬¡ï¼Œä¸è¦é¢‘ç¹æ›´æ–°
                )
            
            epoch_progress = progress.add_task(description='epoch: ', show_info='', total=train_config.epochs)
            steps_progress = progress.add_task(description='steps: ', show_info='', \
                                                total=np.ceil(steps_per_epoch / logging_steps))
            eval_progress = progress.add_task(description='evaluate: ', show_info='', total=eval_steps, visible=False)

            self.progress = progress
            self.eval_progress = eval_progress

            progress.start()

        # end if

        for epoch in range(train_config.epochs):
            
            if accelerator.is_main_process:
                epoch_show_txt = 'epoch: {}/{}, avg_loss: {:.6f}, best_epoch: {}, best_bleu: {}'.format(
                    epoch, train_config.epochs, my_average(epoch_loss_list), best_epoch, best_bleu4
                )
                progress.update(epoch_progress, show_info=epoch_show_txt)
                progress.reset(steps_progress)

            epoch_loss_list = []
            model.train()

            # ã€å…³é”®ä¼˜åŒ–5ã€‘æ¯ä¸ªepochå¼€å§‹å‰æ¸…ç†å†…å­˜
            self.clear_memory()

            for step, batch_data in enumerate(train_dataloader):

                input_ids, input_mask = batch_data['input_ids'], batch_data['input_mask']
                target_ids = batch_data['target_ids']
                # for t5 model, all labels set to `-100` are ignored (masked)
                target_ids[target_ids == decoder_start_token_id] = -100

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=input_mask,
                    labels=target_ids,
                )

                loss = outputs.loss.mean() / accumulation_steps

                # attention here! loss.backward()
                accelerator.backward(loss) 

                # æ¢¯åº¦ç´¯è®¡
                if (step + 1) % accumulation_steps == 0:
                    accelerator.clip_grad_norm_(model.parameters(), 1.0)
                
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()
                
                # æ¯éš”save_stepsæ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
                if (step + 1) % save_steps == 0 or step == steps_per_epoch:
                    self.save_model('epoch_{}_latest'.format(epoch))
                    accelerator.save_state(output_dir=train_config.train_state_dir)
                
                # ==================================ä»¥ä¸‹è®°å½•lossåˆ°æ—¥å¿—============================================
                # æ¯næ­¥æ›´æ–°ä¸€æ¬¡ï¼Œé¿å…é¢‘ç¹çš„cpu-gpuæ•°æ®å¤åˆ¶
                # å‚è€ƒï¼šhttps://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#avoid-unnecessary-cpu-gpu-synchronization
                
                if step % logging_steps == 0 or step == steps_per_epoch:
                    
                    loss_cpu = loss.detach().item() * accumulation_steps
                    epoch_loss_list.append(loss_cpu)
                    
                    info_txt = 'training loss: epoch:{}, step:{}, loss:{}, device:{}'.\
                        format(epoch, step, loss_cpu, str(accelerator.device))
                    
                    log.info(info_txt, std_out=False, save_to_file=True) # ä¿å­˜ loss åˆ°æ–‡ä»¶

                    # æ›´æ–°è¿›åº¦æ¡
                    if accelerator.is_main_process:
                        step_show_txt = 'step: {}/{}, loss: {:.6f}'.format(step, steps_per_epoch, loss_cpu)
                        progress.advance(steps_progress, advance=1)
                        progress.update(steps_progress, show_info=step_show_txt)

                # ==================================ä»¥ä¸Šè®°å½•lossåˆ°æ—¥å¿—============================================
                
                # ã€å…³é”®ä¼˜åŒ–6ã€‘å®šæœŸæ¸…ç†å†…å­˜
                # æ›´é¢‘ç¹åœ°æ¸…ç†å†…å­˜ï¼Œæ¯50æ­¥æ¸…ç†ä¸€æ¬¡
                if step % 50 == 0:
                    self.clear_memory()
                
                # æ¯200æ­¥å¼ºåˆ¶æ¸…ç†å¹¶è®°å½•å†…å­˜ä½¿ç”¨
                if step % 200 == 0 and accelerator.is_main_process:
                    self.clear_memory()
                    self.log_memory_usage(f"Epoch {epoch} Step {step}")
            
            #  end for batch setps

            # ç­‰æ‰€æœ‰è®­ç»ƒè¿›ç¨‹å®Œæˆå†å¼€å§‹è¯„ä¼°
            accelerator.wait_for_everyone()

            # ã€å…³é”®ä¼˜åŒ–7ã€‘è¯„ä¼°å‰æ¸…ç†å†…å­˜
            self.clear_memory()

            model.eval()         
            
            cur_bleu4_score = self.evaluate(
                model=model,
                tokenizer=tokenizer,
                valid_dataloader=valid_dataloader,
                accelerator=accelerator,
                eval_steps=eval_steps,
                )

            # save model
            if cur_bleu4_score >= best_bleu4:

                best_bleu4 = cur_bleu4_score
                best_epoch = epoch
                self.save_model('best')
                accelerator.save_state(output_dir=train_config.train_state_dir)

            # æ¯ä¸ªepochæ‰“å°ä¸€ä¸‹æ—¥å¿—
            if accelerator.is_main_process:

                progress.advance(epoch_progress, advance=1)
                info_txt = 'epoch log: epoch:{}, avg_loss:{}, cur_bleu4:{}, best_bleu4:{}, best_epoch:{}'.\
                            format(epoch, my_average(epoch_loss_list), cur_bleu4_score, best_bleu4, best_epoch)
                self.print_and_log(info_txt, accelerator)
                self.log_memory_usage(f"Epoch {epoch} ç»“æŸ")


    def evaluate(self, 
                model: TextToTextModel, 
                tokenizer: PreTrainedTokenizerFast,
                valid_dataloader: DataLoader, 
                accelerator: Accelerator,
                eval_steps: int,
            ) -> float:
        
        '''
        è¯„ä¼°ï¼Œè¿”å›å¹³å‡çš„bleuåˆ†æ•°
        '''
        max_seq_len = self.train_config.max_seq_len
        batch_decode = tokenizer.batch_decode

        local_sum = 0.0
        local_cnt = 0

        if accelerator.is_main_process:
            self.progress.reset(self.eval_progress)
            self.progress.update(self.eval_progress, visible=True)

        max_eval_steps = eval_steps

        with torch.no_grad():
            for step, batch_data in enumerate(valid_dataloader):
                
                if step >= max_eval_steps: break
                
                if accelerator.is_main_process:
                    self.progress.advance(self.eval_progress, advance=1)
                    self.progress.update(self.eval_progress, show_info='step: {}/{}'.format(step, eval_steps))

                input_ids, input_mask = batch_data['input_ids'], batch_data['input_mask']
                target_ids = batch_data['target_ids']

                # ä½¿ç”¨greedy searchæ›¿ä»£beam searchï¼Œé€Ÿåº¦æå‡5å€ä»¥ä¸Š
                outputs = accelerator.unwrap_model(model).my_generate(
                    input_ids=input_ids,
                    attention_mask=input_mask,
                    max_seq_len=max_seq_len,
                    search_type='greedy',  # è¯„ä¼°æ—¶ä½¿ç”¨greedy searchï¼Œé€Ÿåº¦å¿«å¾ˆå¤š
                )

                # å„ rank åªå¤„ç†è‡ªå·±çš„ shardï¼ˆä¸åš gatherï¼‰
                outputs = outputs.detach().cpu().numpy()
                target_ids = target_ids.detach().cpu().numpy()

                outputs_txt = batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                target_txt = batch_decode(target_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)

                # é€æ ·æœ¬è®¡ç®— bleu4ï¼Œå¹¶åœ¨æœ¬åœ°ç´¯åŠ 
                for i in range(len(target_txt)):
                    local_sum += float(get_bleu4_score(reference=target_txt[i], outputs=outputs_txt[i]))
                    local_cnt += 1

        # æ±‡æ€»å„ rank çš„ sum / count
        device = accelerator.device
        local_sum_t = torch.tensor(local_sum, device=device, dtype=torch.float32)
        local_cnt_t = torch.tensor(local_cnt, device=device, dtype=torch.float32)
        global_sum_t = accelerator.reduce(local_sum_t, reduction="sum")
        global_cnt_t = accelerator.reduce(local_cnt_t, reduction="sum")
        avg_bleu4_score = (global_sum_t / torch.clamp(global_cnt_t, min=1.0)).item()

        if accelerator.is_main_process:
            self.progress.update(self.eval_progress, show_info='bleu4 score: {}'.format(avg_bleu4_score))
            self.progress.update(self.eval_progress, visible=False)

        return avg_bleu4_score

    def test(self, best_epoch: int=0) -> None:
        '''
        '''
        import os 

        train_config = self.train_config
        log = self.logger

        # args for dataloader
        num_workers = 0  # ä½å†…å­˜æ¨¡å¼å¼ºåˆ¶0

        test_dataset = LowMemDataset(
            parquet_file=train_config.train_file,
            tokenizer_dir=train_config.tokenizer_dir,
            max_seq_len=train_config.max_seq_len,
        )
        
        test_dataloader = DataLoader(
            test_dataset, 
            batch_size=min(train_config.batch_size_per_gpu, 2),  # æœ€å¤§2
            shuffle=False,
            collate_fn=test_dataset.collate_fn,
            pin_memory=False,
            num_workers=num_workers,
        )

        log.info('test dataset size: {}.'.format(len(test_dataset)), save_to_file=True)

        set_seed(train_config.seed)
        accelerator = Accelerator(mixed_precision=train_config.mixed_precision)
        device = accelerator.device
        log.info('using device: {} '.format(str(device)), save_to_file=True)

         # è·å–å½“å‰è¿è¡Œä½¿ç”¨äº†å¤šå°‘ä¸ªGPU
        num_gpus_used = accelerator.state.num_processes

        # å•æœºå¤šå¡ï¼Œæ¯ä¸ªstepæ€»å…±çš„batch_size = batch_size_per_gpu * num_gpus_used
        # total_batch_size åˆå§‹åŒ–ä¸ºbatch_size_per_gpuçœŸçš„åªæœ‰CPUçš„æƒ…å†µ
        total_batch_size = min(train_config.batch_size_per_gpu, 2)
        if num_gpus_used >= 1:
            total_batch_size = num_gpus_used * min(train_config.batch_size_per_gpu, 2)

        # T5: All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
        tokenizer = test_dataset.tokenizer

        model_file = train_config.model_file.format(best_epoch)
        if os.path.isdir(model_file):
            # ä¼ å…¥æ–‡ä»¶å¤¹åˆ™ from_pretrained
            model = TextToTextModel.from_pretrained(model_file)
        else:
            # load_state_dict
            t5_config = get_T5_config(T5ModelConfig(), vocab_size=len(tokenizer), decoder_start_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)
            model = TextToTextModel(t5_config)
            model.load_state_dict(torch.load(model_file, map_location='cpu')) # set cpu for no exception
       
        model, test_dataloader = accelerator.prepare(
                model, 
                test_dataloader,
            )
        
        steps = int(np.ceil(len(test_dataset) // total_batch_size))

        local_sum = 0.0
        local_cnt = 0
        batch_decode = tokenizer.batch_decode
        max_seq_len = self.train_config.max_seq_len
        model.eval()

        if accelerator.is_main_process:
            progress = Progress(TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeRemainingColumn(),
                TimeElapsedColumn(),
                TextColumn("[bold blue]{task.fields[show_info]}"),
                refresh_per_second=1.0,
                )
                
            steps_progress = progress.add_task(description='steps: ', show_info='', total=steps)
            progress.start()
            
        with torch.no_grad():
            for step, batch_data in enumerate(test_dataloader):

                if accelerator.is_main_process:
                    progress.advance(steps_progress, advance=1)
                    progress.update(steps_progress, show_info='step: {}/{}'.format(step, steps))

                input_ids, input_mask = batch_data['input_ids'], batch_data['input_mask']
                target_ids = batch_data['target_ids']

                outputs = accelerator.unwrap_model(model).my_generate(
                    input_ids=input_ids,
                    attention_mask=input_mask,
                    max_seq_len=max_seq_len,
                )

                # æµ‹è¯•é˜¶æ®µåŒç†ï¼šä¸åšæ¯æ­¥ gatherï¼Œé¿å…ä¸åŒ rank é€Ÿåº¦å·®å¯¼è‡´ NCCL è¶…æ—¶
                outputs = outputs.detach().cpu().numpy()
                target_ids = target_ids.detach().cpu().numpy()
                
                outputs_txt = batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                target_txt = batch_decode(target_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)

                for i in range(len(target_txt)):
                    local_sum += float(get_bleu4_score(reference=target_txt[i], outputs=outputs_txt[i]))
                    local_cnt += 1

        device = accelerator.device
        local_sum_t = torch.tensor(local_sum, device=device, dtype=torch.float32)
        local_cnt_t = torch.tensor(local_cnt, device=device, dtype=torch.float32)
        global_sum_t = accelerator.reduce(local_sum_t, reduction="sum")
        global_cnt_t = accelerator.reduce(local_cnt_t, reduction="sum")
        avg_bleu4_score = (global_sum_t / torch.clamp(global_cnt_t, min=1.0)).item()
        if accelerator.is_main_process:
            progress.update(steps_progress, show_info='bleu4 score: {}'.format(avg_bleu4_score))

        info_txt = 'test_dataset_size: {}, avg_bleu4_score:{}.'.format(len(test_dataset), avg_bleu4_score)
        log.info(info_txt, save_to_file=True)

        return avg_bleu4_score

    
    def print_and_log(self, info: str, accelerator: Accelerator=None) -> None:
        '''
        ä½¿ç”¨accelerator.print, å¦åˆ™å¤šè¿›ç¨‹æ‰“å°ä¼šå¼‚å¸¸
        '''
        if not accelerator:
            print(info)
        else:
            accelerator.print(info)
        self.logger.info(info, std_out=False, save_to_file=True)

if __name__ == '__main__':
    
    train_config = TrainConfig()
    model_config = T5ModelConfig()

    chat_trainer = ChatTrainerLowMem(train_config=train_config, model_config=model_config)

    chat_trainer.train()
