#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é¢„è®­ç»ƒæ¨¡å‹è¯„ä¼°è„šæœ¬
ç”¨æ³•:
    python evaluate_pretrain.py                           # ä½¿ç”¨é»˜è®¤æ¨¡å‹å’Œæµ‹è¯•é›†
    python evaluate_pretrain.py --model model_save/checkpoint-1000  # æŒ‡å®šæ¨¡å‹
    python evaluate_pretrain.py --generate                # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
"""

import sys
sys.path.extend(['.', '..'])

import argparse
import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, PreTrainedTokenizerFast
from fastparquet import ParquetFile
import numpy as np
from tqdm import tqdm
from config import PROJECT_ROOT


def calculate_perplexity(model, tokenizer, dataset_path, max_samples=100, max_length=512):
    """è®¡ç®—å›°æƒ‘åº¦"""
    print(f"\n{'='*100}")
    print(f"ğŸ“Š è®¡ç®—å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰")
    print(f"{'='*100}\n")
    
    model.eval()
    device = next(model.parameters()).device
    
    total_loss = 0
    total_tokens = 0
    num_samples = 0
    
    # è¯»å–æ•°æ®é›†
    pf = ParquetFile(dataset_path)
    
    with torch.no_grad():
        for chunk in pf:
            for rows in chunk.iter_row_groups():
                for i in tqdm(range(len(rows)), desc="è®¡ç®—ä¸­"):
                    if num_samples >= max_samples:
                        break
                    
                    # è·å–æ–‡æœ¬
                    prompt = rows['prompt'][i] if 'prompt' in rows.columns else ""
                    response = rows['response'][i] if 'response' in rows.columns else ""
                    text = f"{prompt}\n{response}"
                    
                    # Tokenize
                    inputs = tokenizer(
                        text,
                        max_length=max_length,
                        truncation=True,
                        return_tensors="pt"
                    )
                    
                    input_ids = inputs['input_ids'].to(device)
                    
                    # è®¡ç®—æŸå¤±
                    outputs = model(input_ids, labels=input_ids)
                    loss = outputs.loss
                    
                    # ç´¯è®¡
                    total_loss += loss.item() * input_ids.size(1)
                    total_tokens += input_ids.size(1)
                    num_samples += 1
                
                if num_samples >= max_samples:
                    break
            
            if num_samples >= max_samples:
                break
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œå›°æƒ‘åº¦
    avg_loss = total_loss / total_tokens
    perplexity = np.exp(avg_loss)
    
    print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœï¼š")
    print(f"   - æ ·æœ¬æ•°é‡: {num_samples}")
    print(f"   - æ€» Token æ•°: {total_tokens:,}")
    print(f"   - å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"   - å›°æƒ‘åº¦ (PPL): {perplexity:.2f}")
    
    # è¯„ä¼°ç­‰çº§
    if perplexity < 10:
        grade = "ğŸŒŸ ä¼˜ç§€"
    elif perplexity < 30:
        grade = "âœ… è‰¯å¥½"
    elif perplexity < 100:
        grade = "âš ï¸  ä¸€èˆ¬"
    else:
        grade = "âŒ è¾ƒå·®"
    
    print(f"   - è¯„ä¼°ç­‰çº§: {grade}")
    print(f"\n{'='*100}\n")
    
    return perplexity, avg_loss


def generate_text(model, tokenizer, prompts=None, max_length=100, temperature=0.8, top_p=0.9):
    """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ"""
    print(f"\n{'='*100}")
    print(f"âœï¸  æ–‡æœ¬ç”Ÿæˆæµ‹è¯•")
    print(f"{'='*100}\n")
    
    if prompts is None:
        # é»˜è®¤æµ‹è¯•æç¤º
        prompts = [
            "ä»Šå¤©å¤©æ°”å¾ˆ",
            "æœºå™¨å­¦ä¹ æ˜¯",
            "ä»å‰æœ‰åº§å±±",
            "Pythonæ˜¯ä¸€ç§",
            "äººå·¥æ™ºèƒ½çš„åº”ç”¨åŒ…æ‹¬",
            "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«åœ¨äº",
        ]
    
    model.eval()
    device = next(model.parameters()).device
    
    for idx, prompt in enumerate(prompts, 1):
        print(f"{'â”€'*100}")
        print(f"æµ‹è¯• #{idx}")
        print(f"{'â”€'*100}")
        print(f"ğŸ“ æç¤º: {prompt}")
        
        # Tokenize
        inputs = tokenizer(prompt, return_tensors="pt")
        input_ids = inputs['input_ids'].to(device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # è§£ç 
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"ğŸ¤– ç”Ÿæˆ: {generated_text}")
        print()
    
    print(f"{'='*100}\n")


def compare_checkpoints(model_paths, tokenizer_path, test_prompts):
    """å¯¹æ¯”ä¸åŒ checkpoint çš„æ•ˆæœ"""
    print(f"\n{'='*100}")
    print(f"ğŸ” å¯¹æ¯”ä¸åŒ Checkpoint")
    print(f"{'='*100}\n")
    
    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)
    
    for model_path in model_paths:
        print(f"\nğŸ“¦ æ¨¡å‹: {model_path}")
        print(f"{'â”€'*100}")
        
        try:
            model = AutoModelForCausalLM.from_pretrained(model_path)
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = model.to(device)
            
            # ç”Ÿæˆæµ‹è¯•
            for prompt in test_prompts:
                inputs = tokenizer(prompt, return_tensors="pt")
                input_ids = inputs['input_ids'].to(device)
                
                with torch.no_grad():
                    outputs = model.generate(
                        input_ids,
                        max_length=50,
                        temperature=0.8,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                    )
                
                generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"   æç¤º: {prompt}")
                print(f"   ç”Ÿæˆ: {generated}\n")
            
            del model
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {e}\n")
    
    print(f"{'='*100}\n")


def main():
    parser = argparse.ArgumentParser(
        description='è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹æ•ˆæœ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è®¡ç®—å›°æƒ‘åº¦
  python evaluate_pretrain.py --perplexity
  
  # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
  python evaluate_pretrain.py --generate
  
  # å®Œæ•´è¯„ä¼°
  python evaluate_pretrain.py --perplexity --generate
  
  # æŒ‡å®šæ¨¡å‹
  python evaluate_pretrain.py --model model_save/checkpoint-1000 --generate
  
  # å¯¹æ¯”ä¸åŒ checkpoint
  python evaluate_pretrain.py --compare
        """
    )
    
    parser.add_argument(
        '--model', '-m',
        type=str,
        default=PROJECT_ROOT + '/model_save',
        help='æ¨¡å‹è·¯å¾„ (é»˜è®¤: model_save)'
    )
    
    parser.add_argument(
        '--tokenizer', '-t',
        type=str,
        default=PROJECT_ROOT + '/model_save/my_tokenizer_sp',
        help='Tokenizer è·¯å¾„ (é»˜è®¤: model_save/my_tokenizer_sp)'
    )
    
    parser.add_argument(
        '--dataset', '-d',
        type=str,
        default=PROJECT_ROOT + '/data/my_test_dataset.parquet',
        help='æµ‹è¯•æ•°æ®é›†è·¯å¾„ (é»˜è®¤: data/my_test_dataset.parquet)'
    )
    
    parser.add_argument(
        '--perplexity', '-p',
        action='store_true',
        help='è®¡ç®—å›°æƒ‘åº¦'
    )
    
    parser.add_argument(
        '--generate', '-g',
        action='store_true',
        help='æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ'
    )
    
    parser.add_argument(
        '--compare', '-c',
        action='store_true',
        help='å¯¹æ¯”ä¸åŒ checkpoint'
    )
    
    parser.add_argument(
        '--max-samples',
        type=int,
        default=100,
        help='è®¡ç®—å›°æƒ‘åº¦æ—¶ä½¿ç”¨çš„æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤: 100)'
    )
    
    parser.add_argument(
        '--max-length',
        type=int,
        default=100,
        help='ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ (é»˜è®¤: 100)'
    )
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æ“ä½œï¼Œé»˜è®¤éƒ½åš
    if not (args.perplexity or args.generate or args.compare):
        args.perplexity = True
        args.generate = True
    
    print(f"\n{'='*100}")
    print(f"ğŸš€ é¢„è®­ç»ƒæ¨¡å‹è¯„ä¼°")
    print(f"{'='*100}")
    print(f"\nğŸ“¦ æ¨¡å‹è·¯å¾„: {args.model}")
    print(f"ğŸ”¤ Tokenizer: {args.tokenizer}")
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {args.dataset}")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    try:
        print(f"\nâ³ åŠ è½½æ¨¡å‹...")
        tokenizer = PreTrainedTokenizerFast.from_pretrained(args.tokenizer)
        model = AutoModelForCausalLM.from_pretrained(args.model)
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # è®¡ç®—å›°æƒ‘åº¦
        if args.perplexity:
            if Path(args.dataset).exists():
                calculate_perplexity(
                    model, 
                    tokenizer, 
                    args.dataset,
                    max_samples=args.max_samples
                )
            else:
                print(f"âš ï¸  æµ‹è¯•æ•°æ®é›†ä¸å­˜åœ¨: {args.dataset}")
        
        # æµ‹è¯•ç”Ÿæˆ
        if args.generate:
            generate_text(
                model,
                tokenizer,
                max_length=args.max_length
            )
        
        # å¯¹æ¯” checkpoint
        if args.compare:
            # æŸ¥æ‰¾æ‰€æœ‰ checkpoint
            model_dir = Path(args.model).parent
            checkpoints = sorted(model_dir.glob("checkpoint-*"))
            
            if checkpoints:
                print(f"\næ‰¾åˆ° {len(checkpoints)} ä¸ª checkpoint")
                test_prompts = ["ä»Šå¤©å¤©æ°”å¾ˆ", "æœºå™¨å­¦ä¹ æ˜¯"]
                compare_checkpoints(
                    [str(cp) for cp in checkpoints[-3:]],  # æœ€å3ä¸ª
                    args.tokenizer,
                    test_prompts
                )
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ° checkpoint")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
