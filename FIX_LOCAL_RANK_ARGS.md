# ğŸš¨ --local_rank å‚æ•°è§£æé”™è¯¯ä¿®å¤

## âŒ é”™è¯¯ä¿¡æ¯

```python
Traceback (most recent call last):
  File "/tmp/deepspeed_train.py", line 11, in <module>
    main()
  File "/home/rongtw/anaconda3/envs/chatlm/lib/python3.10/site-packages/llmtuner/cli.py", line 75, in main
    raise NotImplementedError("Unknown command: {}".format(command))
NotImplementedError: Unknown command: --local_rank=0
```

---

## ğŸ” é—®é¢˜åˆ†æ

### é”™è¯¯åŸå› 

**DeepSpeed/Torchrun è‡ªåŠ¨æ·»åŠ äº† `--local_rank` å‚æ•°ï¼Œä½† `llmtuner.cli.main()` æŠŠå®ƒå½“ä½œå‘½ä»¤è€Œä¸æ˜¯å‚æ•°ï¼**

### å‘½ä»¤è¡Œå‚æ•°é¡ºåº

```bash
# DeepSpeed å®é™…æ‰§è¡Œçš„å‘½ä»¤
python /tmp/deepspeed_train.py --local_rank=0 train llamafactory_config_3x3080.yaml
#                               ^^^^^^^^^^^^^^ ^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
#                               DeepSpeedå‚æ•°   å‘½ä»¤   é…ç½®æ–‡ä»¶
```

### å‚æ•°è§£ææµç¨‹

| æ­¥éª¤ | æœŸæœ› | å®é™… | ç»“æœ |
|------|------|------|------|
| 1. è¯»å–ç¬¬ä¸€ä¸ªå‚æ•° | `train` | `--local_rank=0` | âŒ é”™è¯¯ |
| 2. è¯†åˆ«ä¸ºå‘½ä»¤ | âœ… | âŒ | NotImplementedError |

### ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿ

**DeepSpeed/Torchrun çš„è¡Œä¸ºï¼š**

```bash
# ä½ çš„å¯åŠ¨å‘½ä»¤
deepspeed --num_gpus=3 script.py train config.yaml

# DeepSpeed å®é™…æ‰§è¡Œï¼ˆæ¯ä¸ªè¿›ç¨‹ï¼‰
python script.py --local_rank=0 train config.yaml  # GPU 0
python script.py --local_rank=1 train config.yaml  # GPU 1
python script.py --local_rank=2 train config.yaml  # GPU 2
```

**llmtuner.cli.main() çš„æœŸæœ›ï¼š**

```python
# llmtuner/cli.py
def main():
    command = sys.argv[1]  # æœŸæœ›æ˜¯ 'train'
    if command == 'train':
        # ...
    else:
        raise NotImplementedError(f"Unknown command: {command}")
```

**å†²çªï¼š**
- DeepSpeed ä¼ å…¥ï¼š`['script.py', '--local_rank=0', 'train', 'config.yaml']`
- main() è¯»å–ï¼š`sys.argv[1]` = `'--local_rank=0'` âŒ
- æœŸæœ›è¯»å–ï¼š`sys.argv[1]` = `'train'` âœ…

---

## âœ… è§£å†³æ–¹æ¡ˆ

### ğŸ¯ æ–¹æ¡ˆ1: è¿‡æ»¤ --local_rank å‚æ•°ï¼ˆæ¨èï¼‰â­â­â­â­â­

**åŸç†ï¼š** åœ¨è°ƒç”¨ `main()` ä¹‹å‰ï¼Œä» `sys.argv` ä¸­ç§»é™¤ `--local_rank` å‚æ•°

**åŒ…è£…è„šæœ¬ï¼š**

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DeepSpeed è®­ç»ƒå¯åŠ¨è„šæœ¬
è§£å†³ --local_rank å‚æ•°è§£æé—®é¢˜
"""
import sys
import os

# è¿‡æ»¤æ‰ DeepSpeed è‡ªåŠ¨æ·»åŠ çš„ --local_rank å‚æ•°
filtered_args = []
skip_next = False
for i, arg in enumerate(sys.argv[1:], 1):
    if skip_next:
        skip_next = False
        continue
    if arg.startswith('--local_rank'):
        if '=' not in arg and i < len(sys.argv) - 1:
            skip_next = True  # è·³è¿‡ä¸‹ä¸€ä¸ªå‚æ•°ï¼ˆå€¼ï¼‰
        continue  # è·³è¿‡ --local_rank
    filtered_args.append(arg)

# æ›¿æ¢ sys.argv
sys.argv = [sys.argv[0]] + filtered_args

# å¯¼å…¥å¹¶è¿è¡Œ
from llmtuner.cli import main

if __name__ == "__main__":
    main()
```

**å·¥ä½œåŸç†ï¼š**

```python
# åŸå§‹å‚æ•°
sys.argv = ['script.py', '--local_rank=0', 'train', 'config.yaml']

# è¿‡æ»¤å
sys.argv = ['script.py', 'train', 'config.yaml']

# main() è¯»å–
command = sys.argv[1]  # 'train' âœ…
```

**ä¸ºä»€ä¹ˆå¯ä»¥ç§»é™¤ --local_rankï¼Ÿ**

- âœ… DeepSpeed/PyTorch ä¼šè‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ï¼š`LOCAL_RANK`, `RANK`, `WORLD_SIZE`
- âœ… LLaMA-Factory ä»ç¯å¢ƒå˜é‡è¯»å–è¿™äº›ä¿¡æ¯ï¼Œä¸éœ€è¦å‘½ä»¤è¡Œå‚æ•°
- âœ… ç§»é™¤ `--local_rank` ä¸å½±å“åˆ†å¸ƒå¼è®­ç»ƒ

---

### ğŸ¯ æ–¹æ¡ˆ2: ä½¿ç”¨ llamafactory-cliï¼ˆæœ€ç®€å•ï¼‰â­â­â­â­â­

**llamafactory-cli å·²ç»å¤„ç†äº†è¿™ä¸ªé—®é¢˜ï¼š**

```bash
# ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·
llamafactory-cli train llamafactory_config_3x3080.yaml
```

**ä¼˜ç‚¹ï¼š**
- âœ… ä¸éœ€è¦åŒ…è£…è„šæœ¬
- âœ… è‡ªåŠ¨å¤„ç†æ‰€æœ‰å‚æ•°
- âœ… æ”¯æŒæ‰€æœ‰å¯åŠ¨æ–¹å¼

---

### ğŸ¯ æ–¹æ¡ˆ3: ä½¿ç”¨ accelerate launchï¼ˆæ”¯æŒ -mï¼‰â­â­â­â­

**accelerate ä¼šæ­£ç¡®å¤„ç†å‚æ•°ï¼š**

```bash
accelerate launch \
    --multi_gpu \
    --num_processes=3 \
    -m llmtuner.cli train config.yaml
```

**ä¼˜ç‚¹ï¼š**
- âœ… ä¸éœ€è¦åŒ…è£…è„šæœ¬
- âœ… å‚æ•°å¤„ç†æ­£ç¡®
- âœ… çµæ´»é…ç½®

---

## ğŸ”§ å·²ä¿®å¤çš„å¯åŠ¨è„šæœ¬

æˆ‘å·²ç»ä¿®å¤äº† `run_llamafactory_3x3080.sh`ï¼Œç°åœ¨åŒ…è£…è„šæœ¬ä¼šè‡ªåŠ¨è¿‡æ»¤ `--local_rank` å‚æ•°ï¼š

### ä¿®å¤å†…å®¹

#### 1. DeepSpeed åŒ…è£…è„šæœ¬ï¼ˆæ–¹å¼3ï¼‰

```bash
# ä¿®å¤å‰ï¼ˆé”™è¯¯ï¼‰
cat > /tmp/deepspeed_train.py << 'EOF'
#!/usr/bin/env python
import sys
from llmtuner.cli import main

if __name__ == "__main__":
    main()
EOF
# âŒ ç›´æ¥è°ƒç”¨ main()ï¼Œ--local_rank å‚æ•°å¯¼è‡´è§£æå¤±è´¥

# ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰
cat > /tmp/deepspeed_train.py << 'EOF'
#!/usr/bin/env python
import sys

# è¿‡æ»¤æ‰ --local_rank å‚æ•°
filtered_args = []
skip_next = False
for i, arg in enumerate(sys.argv[1:], 1):
    if skip_next:
        skip_next = False
        continue
    if arg.startswith('--local_rank'):
        if '=' not in arg and i < len(sys.argv) - 1:
            skip_next = True
        continue
    filtered_args.append(arg)

sys.argv = [sys.argv[0]] + filtered_args

from llmtuner.cli import main

if __name__ == "__main__":
    main()
EOF
# âœ… è¿‡æ»¤ --local_rank å‚æ•°åå†è°ƒç”¨ main()
```

#### 2. Torchrun åŒ…è£…è„šæœ¬ï¼ˆæ–¹å¼4ï¼‰

åŒæ ·çš„ä¿®å¤åº”ç”¨åˆ° torchrun åŒ…è£…è„šæœ¬ã€‚

---

## ğŸš€ ç°åœ¨å¯ä»¥è¿è¡Œäº†

### Step 1: é‡æ–°è¿è¡Œå¯åŠ¨è„šæœ¬

```bash
cd /data3/ChatLM-mini-Chinese
bash run_llamafactory_3x3080.sh
```

### Step 2: é€‰æ‹©è®­ç»ƒæ–¹å¼

```
è¯·é€‰æ‹©è®­ç»ƒæ–¹å¼:
  1) ä½¿ç”¨ llamafactory-cli (æ¨èï¼Œæœ€ç®€å•)
  2) ä½¿ç”¨ accelerate launch (æ›´çµæ´»)
  3) ä½¿ç”¨ deepspeed (æœ€ä¼˜æ˜¾å­˜åˆ©ç”¨)      â† ç°åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼
  4) ä½¿ç”¨ torchrun (æ ‡å‡†DDP)            â† ç°åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼

è¯·è¾“å…¥é€‰é¡¹ [1-4]: 3
```

### Step 3: å¼€å§‹è®­ç»ƒ

è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
1. âœ… åˆ›å»ºåŒ…è£…è„šæœ¬
2. âœ… è¿‡æ»¤ `--local_rank` å‚æ•°
3. âœ… æ­£ç¡®è°ƒç”¨ `main()`
4. âœ… å¼€å§‹è®­ç»ƒ

---

## ğŸ“Š å‚æ•°å¤„ç†å¯¹æ¯”

### åŸå§‹å‚æ•°ï¼ˆDeepSpeed ä¼ å…¥ï¼‰

```python
sys.argv = [
    '/tmp/deepspeed_train.py',
    '--local_rank=0',      # â† DeepSpeed æ·»åŠ 
    'train',               # â† å‘½ä»¤
    'config.yaml'          # â† é…ç½®æ–‡ä»¶
]
```

### è¿‡æ»¤åçš„å‚æ•°

```python
sys.argv = [
    '/tmp/deepspeed_train.py',
    'train',               # â† å‘½ä»¤ï¼ˆæ­£ç¡®ä½ç½®ï¼‰
    'config.yaml'          # â† é…ç½®æ–‡ä»¶
]
```

### main() è§£æ

```python
def main():
    command = sys.argv[1]  # 'train' âœ…
    if command == 'train':
        # å¼€å§‹è®­ç»ƒ âœ…
```

---

## ğŸ’¡ æ·±å…¥ç†è§£

### åˆ†å¸ƒå¼è®­ç»ƒçš„å‚æ•°ä¼ é€’

#### ç¯å¢ƒå˜é‡æ–¹å¼ï¼ˆæ¨èï¼‰âœ…

```bash
# DeepSpeed/PyTorch è‡ªåŠ¨è®¾ç½®
export LOCAL_RANK=0
export RANK=0
export WORLD_SIZE=3

# ç¨‹åºè¯»å–
import os
local_rank = int(os.environ.get('LOCAL_RANK', 0))
```

**ä¼˜ç‚¹ï¼š**
- âœ… ä¸æ±¡æŸ“å‘½ä»¤è¡Œå‚æ•°
- âœ… æ ‡å‡†åŒ–
- âœ… æ‰€æœ‰æ¡†æ¶éƒ½æ”¯æŒ

#### å‘½ä»¤è¡Œå‚æ•°æ–¹å¼ï¼ˆæ—§æ–¹å¼ï¼‰âŒ

```bash
# ä¼ é€’å‚æ•°
python script.py --local_rank=0

# ç¨‹åºè§£æ
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--local_rank', type=int, default=0)
```

**ç¼ºç‚¹ï¼š**
- âŒ ä¸å…¶ä»–å‚æ•°å†²çª
- âŒ éœ€è¦æ‰‹åŠ¨è§£æ
- âŒ ä¸åŒæ¡†æ¶å®ç°ä¸åŒ

### LLaMA-Factory çš„å®ç°

**LLaMA-Factory ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼š**

```python
# llmtuner å†…éƒ¨
import os
import torch.distributed as dist

if dist.is_available() and dist.is_initialized():
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
```

**æ‰€ä»¥å¯ä»¥å®‰å…¨ç§»é™¤ --local_rank å‚æ•°ï¼**

---

## ğŸ”’ é˜²æ­¢å†æ¬¡å‡ºç°

### è§„åˆ™1: ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·

```bash
# âœ… æ¨èï¼ˆè‡ªåŠ¨å¤„ç†æ‰€æœ‰å‚æ•°ï¼‰
llamafactory-cli train config.yaml

# âŒ é¿å…ï¼ˆéœ€è¦æ‰‹åŠ¨å¤„ç†å‚æ•°ï¼‰
deepspeed script.py train config.yaml
```

### è§„åˆ™2: ä½¿ç”¨åŒ…è£…è„šæœ¬æ—¶è¿‡æ»¤å‚æ•°

```python
# âœ… æ­£ç¡®ï¼ˆè¿‡æ»¤ --local_rankï¼‰
import sys
filtered_args = [arg for arg in sys.argv[1:] if not arg.startswith('--local_rank')]
sys.argv = [sys.argv[0]] + filtered_args
from llmtuner.cli import main
main()

# âŒ é”™è¯¯ï¼ˆç›´æ¥è°ƒç”¨ï¼‰
from llmtuner.cli import main
main()
```

### è§„åˆ™3: ç†è§£å¯åŠ¨å™¨çš„è¡Œä¸º

| å¯åŠ¨å™¨ | æ·»åŠ å‚æ•° | è®¾ç½®ç¯å¢ƒå˜é‡ | éœ€è¦è¿‡æ»¤ |
|--------|---------|------------|---------|
| `deepspeed` | âœ… `--local_rank` | âœ… | âœ… éœ€è¦ |
| `torchrun` | âœ… `--local_rank` | âœ… | âœ… éœ€è¦ |
| `accelerate` | âŒ | âœ… | âŒ ä¸éœ€è¦ |
| `llamafactory-cli` | âŒ | âœ… | âŒ ä¸éœ€è¦ |

---

## ğŸ“ å¿«é€Ÿå‚è€ƒ

### åˆ›å»ºåŒ…è£…è„šæœ¬ï¼ˆå¸¦å‚æ•°è¿‡æ»¤ï¼‰

```bash
# åˆ›å»º DeepSpeed åŒ…è£…è„šæœ¬
cat > deepspeed_train.py << 'EOF'
#!/usr/bin/env python
import sys

# è¿‡æ»¤ --local_rank å‚æ•°
filtered_args = []
skip_next = False
for i, arg in enumerate(sys.argv[1:], 1):
    if skip_next:
        skip_next = False
        continue
    if arg.startswith('--local_rank'):
        if '=' not in arg and i < len(sys.argv) - 1:
            skip_next = True
        continue
    filtered_args.append(arg)

sys.argv = [sys.argv[0]] + filtered_args

from llmtuner.cli import main

if __name__ == "__main__":
    main()
EOF

# ä½¿ç”¨
deepspeed --num_gpus=3 deepspeed_train.py train config.yaml
```

### å„ç§å¯åŠ¨æ–¹å¼

```bash
# æ–¹å¼1: llamafactory-cliï¼ˆæ¨èï¼Œæ— éœ€å¤„ç†å‚æ•°ï¼‰
llamafactory-cli train config.yaml

# æ–¹å¼2: accelerateï¼ˆæ— éœ€å¤„ç†å‚æ•°ï¼‰
accelerate launch -m llmtuner.cli train config.yaml

# æ–¹å¼3: deepspeedï¼ˆéœ€è¦åŒ…è£…è„šæœ¬è¿‡æ»¤å‚æ•°ï¼‰
deepspeed --num_gpus=3 wrapper.py train config.yaml

# æ–¹å¼4: torchrunï¼ˆéœ€è¦åŒ…è£…è„šæœ¬è¿‡æ»¤å‚æ•°ï¼‰
torchrun --nproc_per_node=3 wrapper.py train config.yaml
```

---

## ğŸ¯ æ€»ç»“

**é—®é¢˜ï¼š** DeepSpeed æ·»åŠ çš„ `--local_rank` å‚æ•°å¯¼è‡´ `llmtuner.cli.main()` è§£æå¤±è´¥

**åŸå› ï¼š** `main()` æœŸæœ›ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å‘½ä»¤ï¼ˆ`train`ï¼‰ï¼Œä½†æ”¶åˆ°çš„æ˜¯ `--local_rank=0`

**è§£å†³æ–¹æ¡ˆï¼š**
1. ä½¿ç”¨ `llamafactory-cli`ï¼ˆæœ€ç®€å•ï¼‰
2. æˆ–è€…åœ¨åŒ…è£…è„šæœ¬ä¸­è¿‡æ»¤ `--local_rank` å‚æ•°

**æ‰§è¡Œå‘½ä»¤ï¼š**
```bash
cd /data3/ChatLM-mini-Chinese
bash run_llamafactory_3x3080.sh
# é€‰æ‹©æ–¹å¼ 3ï¼ˆdeepspeedï¼‰
```

**ç°åœ¨åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼** ğŸ‰

---

## ğŸ“ è¿˜æœ‰é—®é¢˜ï¼Ÿ

å¦‚æœè¿˜æœ‰é”™è¯¯ï¼Œè¯·æä¾›ï¼š
1. å®Œæ•´çš„é”™è¯¯ä¿¡æ¯
2. ä½¿ç”¨çš„å¯åŠ¨æ–¹å¼ï¼ˆ1-4ï¼‰
3. æµ‹è¯•åŒ…è£…è„šæœ¬

```bash
# æµ‹è¯•åŒ…è£…è„šæœ¬
cat > /tmp/test_wrapper.py << 'EOF'
#!/usr/bin/env python
import sys

print("åŸå§‹å‚æ•°:", sys.argv)

# è¿‡æ»¤ --local_rank
filtered_args = []
skip_next = False
for i, arg in enumerate(sys.argv[1:], 1):
    if skip_next:
        skip_next = False
        continue
    if arg.startswith('--local_rank'):
        if '=' not in arg and i < len(sys.argv) - 1:
            skip_next = True
        continue
    filtered_args.append(arg)

sys.argv = [sys.argv[0]] + filtered_args
print("è¿‡æ»¤åå‚æ•°:", sys.argv)

from llmtuner.cli import main
main()
EOF

# æ‰‹åŠ¨æµ‹è¯•
python /tmp/test_wrapper.py --local_rank=0 train config.yaml
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [FIX_RELATIVE_IMPORT.md](FIX_RELATIVE_IMPORT.md) - ç›¸å¯¹å¯¼å…¥é”™è¯¯ä¿®å¤
- [FIX_DEEPSPEED_LAUNCH.md](FIX_DEEPSPEED_LAUNCH.md) - DeepSpeed å¯åŠ¨é—®é¢˜
- [FIX_TRL_CONFLICT.md](FIX_TRL_CONFLICT.md) - ä¾èµ–å†²çªä¿®å¤
