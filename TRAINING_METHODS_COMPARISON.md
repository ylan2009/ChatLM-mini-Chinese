# è®­ç»ƒæ–¹å¼å¯¹æ¯”è¯´æ˜

## ğŸ“Œ é‡è¦æ¾„æ¸…

é¡¹ç›®ä¸­æœ‰**ä¸‰ç§ä¸åŒçš„è®­ç»ƒå®ç°**ï¼Œå®ƒä»¬çš„åŒºåˆ«å¦‚ä¸‹ï¼š

---

## 1ï¸âƒ£ `train_low_mem.py` - åŸå§‹æ‰‹åŠ¨è®­ç»ƒå¾ªç¯

**ç‰¹ç‚¹**ï¼š
- âœ… å®Œå…¨æ‰‹åŠ¨å®ç°è®­ç»ƒå¾ªç¯
- âœ… ä½¿ç”¨ `accelerate` å¤„ç†åˆ†å¸ƒå¼
- âœ… è‡ªå®šä¹‰ä½å†…å­˜ä¼˜åŒ–
- âœ… æ‰‹åŠ¨å®ç°è¯„ä¼°ã€ä¿å­˜ã€æ—¥å¿—

**ä»£ç ç»“æ„**ï¼š
```python
from accelerate import Accelerator

accelerator = Accelerator()

# æ‰‹åŠ¨è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
```

**ä¼˜ç‚¹**ï¼š
- å®Œå…¨æ§åˆ¶è®­ç»ƒæµç¨‹
- å¯ä»¥å®ç°ä»»ä½•è‡ªå®šä¹‰é€»è¾‘
- é€‚åˆç ”ç©¶å’Œå®éªŒ

**ç¼ºç‚¹**ï¼š
- ä»£ç é‡å¤§ï¼ˆ~850è¡Œï¼‰
- éœ€è¦æ‰‹åŠ¨å¤„ç†å¾ˆå¤šç»†èŠ‚
- ç»´æŠ¤æˆæœ¬é«˜

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦å®Œå…¨è‡ªå®šä¹‰è®­ç»ƒé€»è¾‘
- ç ”ç©¶æ–°çš„è®­ç»ƒæ–¹æ³•
- å¯¹è®­ç»ƒè¿‡ç¨‹æœ‰ç‰¹æ®Šè¦æ±‚

---

## 2ï¸âƒ£ `train_with_transformers_trainer.py` - Transformers Trainer API

**ç‰¹ç‚¹**ï¼š
- âœ… ä½¿ç”¨ Transformers çš„ `Trainer` API
- âœ… è‡ªåŠ¨å¤„ç†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ã€ä¿å­˜
- âœ… æ”¯æŒå¤šç§åˆ†å¸ƒå¼æ–¹å¼ï¼ˆDDPã€DataParallelï¼‰
- âŒ **ä¸æ˜¯** LLaMA-Factoryï¼ˆåªæ˜¯é£æ ¼ç±»ä¼¼ï¼‰

**ä»£ç ç»“æ„**ï¼š
```python
from transformers import Trainer, TrainingArguments

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()  # è‡ªåŠ¨å¤„ç†æ‰€æœ‰è®­ç»ƒé€»è¾‘
```

**ä¼˜ç‚¹**ï¼š
- ä»£ç ç®€æ´ï¼ˆ~200è¡Œï¼‰
- è‡ªåŠ¨å¤„ç†å¤§éƒ¨åˆ†ç»†èŠ‚
- æ˜“äºç»´æŠ¤å’Œæ‰©å±•
- æ— éœ€é¢å¤–ä¾èµ–ï¼ˆåªéœ€ transformersï¼‰

**ç¼ºç‚¹**ï¼š
- çµæ´»æ€§ç•¥ä½äºæ‰‹åŠ¨å¾ªç¯
- æŸäº›è‡ªå®šä¹‰éœ€æ±‚éœ€è¦ç»§æ‰¿å’Œé‡å†™

**é€‚ç”¨åœºæ™¯**ï¼š
- æ ‡å‡†çš„æ¨¡å‹è®­ç»ƒä»»åŠ¡
- å¿«é€Ÿå®éªŒå’Œè¿­ä»£
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

**âš ï¸ å‘½åè¯´æ˜**ï¼š
- åŸå `train_with_llamafactory.py` **å¾ˆè¯¯å¯¼**
- å·²é‡å‘½åä¸º `train_with_transformers_trainer.py`
- å®ƒä½¿ç”¨çš„æ˜¯ **Transformers Trainer**ï¼Œä¸æ˜¯ LLaMA-Factory

---

## 3ï¸âƒ£ `train_with_real_llamafactory.py` - çœŸæ­£çš„ LLaMA-Factory

**ç‰¹ç‚¹**ï¼š
- âœ… ä½¿ç”¨çœŸæ­£çš„ LLaMA-Factory åº“ï¼ˆ`llmtuner`ï¼‰
- âœ… é…ç½®é©±åŠ¨ï¼ˆYAMLé…ç½®æ–‡ä»¶ï¼‰
- âœ… æ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼ï¼ˆé¢„è®­ç»ƒã€SFTã€LoRAã€RLHFï¼‰
- âœ… å†…ç½®æœ€ä½³å®è·µå’Œä¼˜åŒ–

**ä»£ç ç»“æ„**ï¼š
```python
from llmtuner import run_exp

# ä½¿ç”¨é…ç½®æ–‡ä»¶
run_exp(args={"config_file": "config.yaml"})

# æˆ–ä½¿ç”¨å‘½ä»¤è¡Œ
# llamafactory-cli train config.yaml
```

**ä¼˜ç‚¹**ï¼š
- æœ€ç®€å•ï¼ˆé…ç½®æ–‡ä»¶é©±åŠ¨ï¼‰
- å†…ç½®å¤§é‡æœ€ä½³å®è·µ
- æ”¯æŒå¤šç§è®­ç»ƒèŒƒå¼
- ç¤¾åŒºæ”¯æŒå’Œæ–‡æ¡£å®Œå–„

**ç¼ºç‚¹**ï¼š
- éœ€è¦é¢å¤–å®‰è£… `llmtuner`
- çµæ´»æ€§æœ€ä½ï¼ˆå—é™äºé…ç½®é€‰é¡¹ï¼‰
- å­¦ä¹ é…ç½®æ–‡ä»¶æ ¼å¼

**é€‚ç”¨åœºæ™¯**ï¼š
- æ ‡å‡†çš„LLMå¾®è°ƒä»»åŠ¡
- éœ€è¦å¿«é€Ÿä¸Šæ‰‹
- å›¢é˜Ÿåä½œï¼ˆç»Ÿä¸€é…ç½®ï¼‰

---

## ğŸ“Š ä¸‰ç§æ–¹å¼å¯¹æ¯”

| ç‰¹æ€§ | æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ | Transformers Trainer | çœŸæ­£çš„ LLaMA-Factory |
|------|-------------|---------------------|---------------------|
| **ä»£ç é‡** | ~850è¡Œ | ~200è¡Œ | ~100è¡Œï¼ˆä¸»è¦æ˜¯é…ç½®ï¼‰ |
| **çµæ´»æ€§** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **æ˜“ç”¨æ€§** | â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **ç»´æŠ¤æˆæœ¬** | é«˜ | ä¸­ | ä½ |
| **å­¦ä¹ æ›²çº¿** | é™¡å³­ | å¹³ç¼“ | æœ€å¹³ç¼“ |
| **é¢å¤–ä¾èµ–** | accelerate | æ—  | llmtuner |
| **è‡ªå®šä¹‰èƒ½åŠ›** | å®Œå…¨æ§åˆ¶ | è¾ƒå¼º | å—é™äºé…ç½® |
| **åˆ†å¸ƒå¼æ”¯æŒ** | Accelerator | è‡ªåŠ¨æ£€æµ‹ | è‡ªåŠ¨æ£€æµ‹ |
| **é€‚åˆäººç¾¤** | ç ”ç©¶è€… | å·¥ç¨‹å¸ˆ | å¿«é€Ÿå¼€å‘è€… |

---

## ğŸ¯ å¦‚ä½•é€‰æ‹©ï¼Ÿ

### åœºæ™¯1: ç ”ç©¶æ–°çš„è®­ç»ƒæ–¹æ³•
â†’ ä½¿ç”¨ **`train_low_mem.py`**ï¼ˆæ‰‹åŠ¨è®­ç»ƒå¾ªç¯ï¼‰

### åœºæ™¯2: æ ‡å‡†çš„æ¨¡å‹è®­ç»ƒ
â†’ ä½¿ç”¨ **`train_with_transformers_trainer.py`**ï¼ˆTransformers Trainerï¼‰

### åœºæ™¯3: å¿«é€Ÿå¾®è°ƒLLM
â†’ ä½¿ç”¨ **`train_with_real_llamafactory.py`**ï¼ˆçœŸæ­£çš„ LLaMA-Factoryï¼‰

### åœºæ™¯4: ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
â†’ ä½¿ç”¨ **`train_with_transformers_trainer.py`**ï¼ˆç¨³å®šæ€§å’Œçµæ´»æ€§å¹³è¡¡ï¼‰

---

## ğŸ”§ å®é™…ä½¿ç”¨ç¤ºä¾‹

### ä½¿ç”¨ Transformers Trainerï¼ˆæ¨èï¼‰

```bash
# å•GPU
python train_with_transformers_trainer.py

# å¤šGPUï¼ˆtorchrunï¼‰
torchrun --nproc_per_node=2 train_with_transformers_trainer.py

# å¤šGPUï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
CUDA_VISIBLE_DEVICES=0,1 python train_with_transformers_trainer.py
```

### ä½¿ç”¨çœŸæ­£çš„ LLaMA-Factory

```bash
# æ–¹å¼1: Pythonè„šæœ¬
python train_with_real_llamafactory.py

# æ–¹å¼2: å‘½ä»¤è¡Œï¼ˆæ¨èï¼‰
llamafactory-cli train llamafactory_config.yaml

# æ–¹å¼3: å¤šGPU
accelerate launch --multi_gpu --num_processes=2 train_with_real_llamafactory.py
```

### ä½¿ç”¨æ‰‹åŠ¨è®­ç»ƒå¾ªç¯

```bash
# éœ€è¦ä½¿ç”¨ accelerate
accelerate launch --multi_gpu --num_processes=2 train_low_mem.py
```

---

## â“ å¸¸è§è¯¯è§£

### è¯¯è§£1: "å¿…é¡»ç”¨ accelerate æ‰èƒ½å¤šGPUè®­ç»ƒ"
âŒ **é”™è¯¯**ï¼Transformers Trainer æ”¯æŒå¤šç§æ–¹å¼ï¼š
- torchrunï¼ˆæ¨èï¼‰
- è‡ªåŠ¨æ£€æµ‹
- accelerate
- DeepSpeed

### è¯¯è§£2: "LLaMA-Factory å°±æ˜¯ Transformers Trainer"
âŒ **é”™è¯¯**ï¼å®ƒä»¬æ˜¯ä¸åŒçš„ï¼š
- **Transformers Trainer**: Hugging Face çš„é€šç”¨è®­ç»ƒAPI
- **LLaMA-Factory**: åŸºäº Trainer çš„é«˜çº§å°è£…ï¼Œä¸“æ³¨äºLLMå¾®è°ƒ

### è¯¯è§£3: "æ‰‹åŠ¨è®­ç»ƒå¾ªç¯æ€§èƒ½æ›´å¥½"
âŒ **ä¸ä¸€å®š**ï¼Trainer å†…éƒ¨ä¹Ÿæ˜¯ä¼˜åŒ–è¿‡çš„ï¼Œæ€§èƒ½å·®å¼‚å¾ˆå°ã€‚æ‰‹åŠ¨å¾ªç¯çš„ä¼˜åŠ¿åœ¨äº**çµæ´»æ€§**ï¼Œä¸æ˜¯æ€§èƒ½ã€‚

---

## ğŸ“ æ€»ç»“

1. **`train_low_mem.py`**: åŸå§‹é¡¹ç›®çš„å®ç°ï¼Œæ‰‹åŠ¨è®­ç»ƒå¾ªç¯
2. **`train_with_transformers_trainer.py`**: ä½¿ç”¨ Transformers Trainerï¼ˆä¹‹å‰è¯¯å‘½åä¸º llamafactoryï¼‰
3. **`train_with_real_llamafactory.py`**: çœŸæ­£ä½¿ç”¨ LLaMA-Factory åº“

**æ¨èæ–°æ‰‹ä½¿ç”¨**: `train_with_transformers_trainer.py`ï¼ˆå¹³è¡¡äº†çµæ´»æ€§å’Œæ˜“ç”¨æ€§ï¼‰

**æ¨èå¿«é€Ÿå¼€å‘**: `train_with_real_llamafactory.py`ï¼ˆæœ€ç®€å•ï¼‰

**æ¨èç ”ç©¶å®éªŒ**: `train_low_mem.py`ï¼ˆæœ€çµæ´»ï¼‰
