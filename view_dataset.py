#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æŸ¥çœ‹é¢„è®­ç»ƒæ•°æ®é›†æ ·æœ¬çš„ç®€å•è„šæœ¬
ç”¨æ³•:
    python view_dataset.py                                    # æŸ¥çœ‹è®­ç»ƒé›†å‰5æ¡
    python view_dataset.py --file data/my_test_dataset.parquet --num 10  # æŸ¥çœ‹æµ‹è¯•é›†å‰10æ¡
    python view_dataset.py --random --num 3                   # éšæœºæŸ¥çœ‹3æ¡
    python view_dataset.py --stats                            # åªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
"""

import sys
sys.path.extend(['.', '..'])

import argparse
from pathlib import Path
from fastparquet import ParquetFile
import random
from config import PROJECT_ROOT


def format_text(text, max_length=200):
    """æ ¼å¼åŒ–æ–‡æœ¬ï¼Œé™åˆ¶æ˜¾ç¤ºé•¿åº¦"""
    if text is None:
        return "[None]"
    text_str = str(text).strip()
    if len(text_str) > max_length:
        return text_str[:max_length] + "..."
    return text_str


def get_dataset_stats(file_path):
    """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\n{'='*100}")
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
    print(f"{'='*100}\n")
    
    pf = ParquetFile(file_path)
    
    # è·å–åˆ—å
    first_chunk = next(pf.iter_row_groups())
    columns = first_chunk.columns.tolist()
    print(f"ğŸ“‹ åˆ—å: {columns}")
    
    # ç»Ÿè®¡æ€»è¡Œæ•°
    total_rows = 0
    prompt_lengths = []
    response_lengths = []
    
    for chunk in ParquetFile(file_path):
        for rows in chunk.iter_row_groups():
            total_rows += len(rows)
            
            # ç»Ÿè®¡é•¿åº¦
            if 'prompt' in columns:
                for val in rows['prompt']:
                    if val:
                        prompt_lengths.append(len(str(val)))
            
            if 'response' in columns:
                for val in rows['response']:
                    if val:
                        response_lengths.append(len(str(val)))
    
    print(f"ğŸ“ˆ æ€»æ ·æœ¬æ•°: {total_rows:,}")
    
    if prompt_lengths:
        print(f"\nğŸ“ Prompt ç»Ÿè®¡:")
        print(f"   - å¹³å‡é•¿åº¦: {sum(prompt_lengths)/len(prompt_lengths):.0f} å­—ç¬¦")
        print(f"   - æœ€å°é•¿åº¦: {min(prompt_lengths)} å­—ç¬¦")
        print(f"   - æœ€å¤§é•¿åº¦: {max(prompt_lengths)} å­—ç¬¦")
    
    if response_lengths:
        print(f"\nğŸ’¬ Response ç»Ÿè®¡:")
        print(f"   - å¹³å‡é•¿åº¦: {sum(response_lengths)/len(response_lengths):.0f} å­—ç¬¦")
        print(f"   - æœ€å°é•¿åº¦: {min(response_lengths)} å­—ç¬¦")
        print(f"   - æœ€å¤§é•¿åº¦: {max(response_lengths)} å­—ç¬¦")
    
    print(f"\n{'='*100}\n")
    
    return total_rows, columns


def view_samples(file_path, num_samples=5, random_sample=False, max_text_length=200):
    """æŸ¥çœ‹æ•°æ®é›†æ ·æœ¬"""
    print(f"\n{'='*100}")
    print(f"ğŸ“ æ–‡ä»¶: {file_path}")
    print(f"{'='*100}\n")
    
    if not Path(file_path).exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
        return
    
    try:
        pf = ParquetFile(file_path)
        
        # è·å–åˆ—åå’Œæ€»è¡Œæ•°
        first_chunk = next(pf.iter_row_groups())
        columns = first_chunk.columns.tolist()
        
        # ç»Ÿè®¡æ€»è¡Œæ•°
        total_rows = 0
        for chunk in ParquetFile(file_path):
            for rows in chunk.iter_row_groups():
                total_rows += len(rows)
        
        print(f"ğŸ“‹ åˆ—å: {columns}")
        print(f"ğŸ“ˆ æ€»æ ·æœ¬æ•°: {total_rows:,}\n")
        
        # æ”¶é›†æ ·æœ¬
        samples = []
        current_row = 0
        
        # å¦‚æœæ˜¯éšæœºé‡‡æ ·ï¼Œå…ˆç”Ÿæˆéšæœºç´¢å¼•
        if random_sample:
            sample_indices = sorted(random.sample(range(total_rows), min(num_samples, total_rows)))
            sample_indices_set = set(sample_indices)
            print(f"ğŸ² éšæœºé‡‡æ · {len(sample_indices)} æ¡æ ·æœ¬\n")
        else:
            sample_indices_set = set(range(min(num_samples, total_rows)))
            print(f"ğŸ“– æ˜¾ç¤ºå‰ {min(num_samples, total_rows)} æ¡æ ·æœ¬\n")
        
        # è¯»å–æ ·æœ¬
        for chunk in ParquetFile(file_path):
            for rows in chunk.iter_row_groups():
                for i in range(len(rows)):
                    if current_row in sample_indices_set:
                        sample = {'row_num': current_row + 1}
                        for col in columns:
                            sample[col] = rows[col][i]
                        samples.append(sample)
                        
                        if len(samples) >= num_samples:
                            break
                    current_row += 1
                
                if len(samples) >= num_samples:
                    break
            
            if len(samples) >= num_samples:
                break
        
        # æ˜¾ç¤ºæ ·æœ¬
        print(f"{'='*100}")
        print(f"ğŸ“ æ•°æ®æ ·æœ¬")
        print(f"{'='*100}\n")
        
        for idx, sample in enumerate(samples, 1):
            print(f"{'â”€'*100}")
            print(f"æ ·æœ¬ #{sample['row_num']}")
            print(f"{'â”€'*100}")
            
            for col in columns:
                value = sample.get(col)
                formatted_value = format_text(value, max_text_length)
                print(f"\nã€{col}ã€‘")
                print(f"{formatted_value}")
            
            print()
        
        print(f"{'='*100}\n")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def main():
    parser = argparse.ArgumentParser(
        description='æŸ¥çœ‹é¢„è®­ç»ƒæ•°æ®é›†æ ·æœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æŸ¥çœ‹è®­ç»ƒé›†å‰5æ¡
  python view_dataset.py
  
  # æŸ¥çœ‹æµ‹è¯•é›†å‰10æ¡
  python view_dataset.py --file data/my_test_dataset.parquet --num 10
  
  # éšæœºæŸ¥çœ‹3æ¡
  python view_dataset.py --random --num 3
  
  # åªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
  python view_dataset.py --stats
  
  # æŸ¥çœ‹å®Œæ•´æ–‡æœ¬ï¼ˆä¸æˆªæ–­ï¼‰
  python view_dataset.py --num 2 --max-length 0
        """
    )
    
    parser.add_argument(
        '--file', '-f',
        type=str,
        default=PROJECT_ROOT + '/data/my_train_dataset.parquet',
        help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/my_train_dataset.parquet)'
    )
    
    parser.add_argument(
        '--num', '-n',
        type=int,
        default=5,
        help='æŸ¥çœ‹çš„æ ·æœ¬æ•°é‡ (é»˜è®¤: 5)'
    )
    
    parser.add_argument(
        '--random', '-r',
        action='store_true',
        help='éšæœºé‡‡æ ·ï¼ˆé»˜è®¤æ˜¾ç¤ºå‰Næ¡ï¼‰'
    )
    
    parser.add_argument(
        '--stats', '-s',
        action='store_true',
        help='åªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼Œä¸æ˜¾ç¤ºæ ·æœ¬'
    )
    
    parser.add_argument(
        '--max-length', '-m',
        type=int,
        default=200,
        help='æ–‡æœ¬æ˜¾ç¤ºçš„æœ€å¤§é•¿åº¦ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶ (é»˜è®¤: 200)'
    )
    
    args = parser.parse_args()
    
    # å¤„ç†æ–‡ä»¶è·¯å¾„
    file_path = args.file
    if not file_path.startswith('/'):
        file_path = PROJECT_ROOT + '/' + file_path
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if args.stats:
        get_dataset_stats(file_path)
    else:
        # æ˜¾ç¤ºæ ·æœ¬
        max_length = None if args.max_length == 0 else args.max_length
        view_samples(file_path, args.num, args.random, max_length or 999999)


if __name__ == '__main__':
    main()
