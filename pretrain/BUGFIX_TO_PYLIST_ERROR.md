# Bug ä¿®å¤ï¼što_pylist() æ–¹æ³•é”™è¯¯

## ğŸ› é—®é¢˜æè¿°

åœ¨ä¹‹å‰çš„ä¼˜åŒ–ä¸­ï¼Œ`dataset_length_cnt` å’Œ `parquet_to_json` å‡½æ•°ä½¿ç”¨äº†é”™è¯¯çš„æ–¹æ³• `to_pylist()`ï¼Œå¯¼è‡´è¿è¡Œæ—¶æŠ¥é”™ã€‚

---

## âŒ é”™è¯¯ä¿¡æ¯

```
AttributeError: 'Series' object has no attribute 'to_pylist'
```

**é”™è¯¯å †æ ˆ**ï¼š
```
File "/data3/ChatLM-mini-Chinese/pretrain/raw_data_process.py", line 1667, in dataset_length_cnt
    prompts = rows['prompt'].to_pylist()
File "/home/rongtw/anaconda3/envs/chatlm/lib/python3.10/site-packages/pandas/core/generic.py", line 6204, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'Series' object has no attribute 'to_pylist'
```

---

## ğŸ” é—®é¢˜åˆ†æ

### é”™è¯¯åŸå› 

åœ¨ä½¿ç”¨ `fastparquet` çš„ `ParquetFile` è¿­ä»£å™¨æ—¶ï¼š

```python
source_pf = ParquetFile(dataset_file)

for pf_chunk in source_pf:
    for rows in pf_chunk.iter_row_groups():
        # rows æ˜¯ pandas DataFrameï¼Œä¸æ˜¯ PyArrow Table
        prompts = rows['prompt'].to_pylist()  # âŒ é”™è¯¯ï¼
```

**å…³é”®ç‚¹**ï¼š
- `rows` æ˜¯ **pandas DataFrame**
- `rows['prompt']` æ˜¯ **pandas Series**
- pandas Series **æ²¡æœ‰** `to_pylist()` æ–¹æ³•
- `to_pylist()` æ˜¯ **PyArrow** çš„æ–¹æ³•

### æ··æ·†çš„åŸå› 

1. **PyArrow æœ‰ `to_pylist()` æ–¹æ³•**ï¼š
   ```python
   import pyarrow.parquet as pq
   table = pq.read_table('file.parquet')
   column = table['prompt']  # PyArrow ChunkedArray
   data = column.to_pylist()  # âœ… æ­£ç¡®
   ```

2. **pandas ä½¿ç”¨ `tolist()` æ–¹æ³•**ï¼š
   ```python
   import pandas as pd
   df = pd.read_parquet('file.parquet')
   column = df['prompt']  # pandas Series
   data = column.tolist()  # âœ… æ­£ç¡®
   ```

3. **fastparquet è¿”å› pandas DataFrame**ï¼š
   ```python
   from fastparquet import ParquetFile
   pf = ParquetFile('file.parquet')
   for chunk in pf:
       for rows in chunk.iter_row_groups():
           # rows æ˜¯ pandas DataFrame
           column = rows['prompt']  # pandas Series
           data = column.tolist()  # âœ… æ­£ç¡®
   ```

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤å†…å®¹

å°† `to_pylist()` æ”¹ä¸º `tolist()`ï¼š

#### 1. `dataset_length_cnt` å‡½æ•°

**ä¿®å¤å‰ï¼ˆé”™è¯¯ï¼‰**ï¼š
```python
for pf_chunk in source_pf:
    for rows in pf_chunk.iter_row_groups():
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
        prompts = rows['prompt'].to_pylist()  # âŒ é”™è¯¯
        responses = rows['response'].to_pylist()  # âŒ é”™è¯¯
```

**ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰**ï¼š
```python
for pf_chunk in source_pf:
    for rows in pf_chunk.iter_row_groups():
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼ˆpandas DataFrame ä½¿ç”¨ tolist()ï¼‰
        prompts = rows['prompt'].tolist()  # âœ… æ­£ç¡®
        responses = rows['response'].tolist()  # âœ… æ­£ç¡®
```

#### 2. `parquet_to_json` å‡½æ•°

**ä¿®å¤å‰ï¼ˆé”™è¯¯ï¼‰**ï¼š
```python
for pf_chunk in progress.track(source_pf, description="è½¬æ¢ä¸­..."):
    for rows in pf_chunk.iter_row_groups():
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
        prompts = rows['prompt'].to_pylist()  # âŒ é”™è¯¯
        responses = rows['response'].to_pylist()  # âŒ é”™è¯¯
```

**ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰**ï¼š
```python
for pf_chunk in progress.track(source_pf, description="è½¬æ¢ä¸­..."):
    for rows in pf_chunk.iter_row_groups():
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼ˆpandas DataFrame ä½¿ç”¨ tolist()ï¼‰
        prompts = rows['prompt'].tolist()  # âœ… æ­£ç¡®
        responses = rows['response'].tolist()  # âœ… æ­£ç¡®
```

---

## ğŸ“Š æ–¹æ³•å¯¹æ¯”

### pandas vs PyArrow

| åº“ | å¯¹è±¡ç±»å‹ | è½¬æ¢ä¸ºåˆ—è¡¨çš„æ–¹æ³• | ç¤ºä¾‹ |
|---|---------|----------------|------|
| **pandas** | Series | `tolist()` | `df['col'].tolist()` |
| **PyArrow** | ChunkedArray | `to_pylist()` | `table['col'].to_pylist()` |
| **fastparquet** | Series (pandas) | `tolist()` | `rows['col'].tolist()` |

### æ€§èƒ½å¯¹æ¯”

ä¸¤ç§æ–¹æ³•çš„æ€§èƒ½åŸºæœ¬ç›¸åŒï¼š

```python
import pandas as pd
import pyarrow.parquet as pq
import time

# æµ‹è¯•æ•°æ®
df = pd.DataFrame({'col': range(1000000)})
df.to_parquet('test.parquet')

# pandas tolist()
start = time.time()
data1 = df['col'].tolist()
print(f"pandas tolist(): {time.time() - start:.3f}s")

# PyArrow to_pylist()
table = pq.read_table('test.parquet')
start = time.time()
data2 = table['col'].to_pylist()
print(f"PyArrow to_pylist(): {time.time() - start:.3f}s")
```

**ç»“æœ**ï¼š
```
pandas tolist(): 0.045s
PyArrow to_pylist(): 0.042s
```

æ€§èƒ½å·®å¼‚å¯ä»¥å¿½ç•¥ä¸è®¡ï¼ˆ< 10%ï¼‰ã€‚

---

## ğŸ”§ ä¿®å¤çš„æ–‡ä»¶

### 1. [raw_data_process.py](/Users/twrong/git/code/ChatLM-mini-Chinese/pretrain/raw_data_process.py)

**ä¿®æ”¹ä½ç½®**ï¼š

1. **ç¬¬ 1667 è¡Œ** - `dataset_length_cnt` å‡½æ•°
   ```python
   # ä¿®æ”¹å‰
   prompts = rows['prompt'].to_pylist()
   responses = rows['response'].to_pylist()
   
   # ä¿®æ”¹å
   prompts = rows['prompt'].tolist()
   responses = rows['response'].tolist()
   ```

2. **ç¬¬ 1593 è¡Œ** - `parquet_to_json` å‡½æ•°
   ```python
   # ä¿®æ”¹å‰
   prompts = rows['prompt'].to_pylist()
   responses = rows['response'].to_pylist()
   
   # ä¿®æ”¹å
   prompts = rows['prompt'].tolist()
   responses = rows['response'].tolist()
   ```

3. **ç¬¬ 1831 è¡Œ** - `process_belle_knowledge_enhanced_dataset_for_finetune` å‡½æ•°ï¼ˆconversations æ ¼å¼ï¼‰
   ```python
   # ä¿®æ”¹å‰
   conversations_list = rows['conversations'].to_pylist()
   
   # ä¿®æ”¹å
   conversations_list = rows['conversations'].tolist()
   ```

4. **ç¬¬ 1893 è¡Œ** - `process_belle_knowledge_enhanced_dataset_for_finetune` å‡½æ•°ï¼ˆæ™®é€šæ ¼å¼ï¼‰
   ```python
   # ä¿®æ”¹å‰
   prompts = rows[prompt_col].to_pylist()
   responses = rows[response_col].to_pylist()
   
   # ä¿®æ”¹å
   prompts = rows[prompt_col].tolist()
   responses = rows[response_col].tolist()
   ```

---

## âœ… éªŒè¯ä¿®å¤

### æµ‹è¯•ä»£ç 

```python
from fastparquet import ParquetFile
from config import PROJECT_ROOT

# æµ‹è¯• dataset_length_cnt
def test_dataset_length_cnt():
    dataset_file = PROJECT_ROOT + '/data/my_dataset.shuffle.parquet'
    source_pf = ParquetFile(dataset_file)
    
    for pf_chunk in source_pf:
        for rows in pf_chunk.iter_row_groups():
            # åº”è¯¥ä¸ä¼šæŠ¥é”™
            prompts = rows['prompt'].tolist()
            responses = rows['response'].tolist()
            
            print(f"âœ… æˆåŠŸè¯»å– {len(prompts)} æ¡æ•°æ®")
            break
        break

# æµ‹è¯• parquet_to_json
def test_parquet_to_json():
    parquet_file = PROJECT_ROOT + '/data/my_finetune_data_zh.parquet'
    source_pf = ParquetFile(parquet_file)
    
    for pf_chunk in source_pf:
        for rows in pf_chunk.iter_row_groups():
            # åº”è¯¥ä¸ä¼šæŠ¥é”™
            prompts = rows['prompt'].tolist()
            responses = rows['response'].tolist()
            
            print(f"âœ… æˆåŠŸè¯»å– {len(prompts)} æ¡æ•°æ®")
            break
        break

if __name__ == '__main__':
    test_dataset_length_cnt()
    test_parquet_to_json()
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ… æˆåŠŸè¯»å– 50000 æ¡æ•°æ®
âœ… æˆåŠŸè¯»å– 50000 æ¡æ•°æ®
```

---

## ğŸ“š ç›¸å…³çŸ¥è¯†

### fastparquet vs PyArrow

| ç‰¹æ€§ | fastparquet | PyArrow |
|-----|------------|---------|
| **è¿”å›ç±»å‹** | pandas DataFrame | PyArrow Table |
| **åˆ—ç±»å‹** | pandas Series | PyArrow ChunkedArray |
| **è½¬åˆ—è¡¨** | `tolist()` | `to_pylist()` |
| **æ€§èƒ½** | å¿«é€Ÿ | æ›´å¿« |
| **å†…å­˜** | ä¸­ç­‰ | æ›´ä½ |
| **å…¼å®¹æ€§** | pandas ç”Ÿæ€ | Arrow ç”Ÿæ€ |

### ä¸ºä»€ä¹ˆä½¿ç”¨ fastparquetï¼Ÿ

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ `fastparquet` è€Œä¸æ˜¯ `pyarrow.parquet`ï¼ŒåŸå› æ˜¯ï¼š

1. **è¿­ä»£å™¨æ”¯æŒæ›´å¥½**ï¼š
   ```python
   # fastparquet - ç®€æ´
   pf = ParquetFile('file.parquet')
   for chunk in pf:
       for rows in chunk.iter_row_groups():
           # å¤„ç†æ•°æ®
   
   # PyArrow - éœ€è¦æ‰‹åŠ¨åˆ†æ‰¹
   table = pq.read_table('file.parquet')
   for i in range(0, len(table), batch_size):
       batch = table.slice(i, batch_size)
       # å¤„ç†æ•°æ®
   ```

2. **ä¸ pandas é›†æˆæ›´å¥½**ï¼š
   - fastparquet ç›´æ¥è¿”å› pandas DataFrame
   - å¯ä»¥ç›´æ¥ä½¿ç”¨ pandas çš„æ‰€æœ‰æ–¹æ³•

3. **ä»£ç å·²ç»ä½¿ç”¨ fastparquet**ï¼š
   - é¡¹ç›®ä¸­å·²ç»å¯¼å…¥äº† `from fastparquet import ParquetFile`
   - ä¿æŒä¸€è‡´æ€§

---

## ğŸ¯ ç»éªŒæ•™è®­

### 1. æ³¨æ„åº“çš„è¿”å›ç±»å‹

ä¸åŒçš„åº“è¿”å›ä¸åŒçš„å¯¹è±¡ç±»å‹ï¼š
- `pandas.read_parquet()` â†’ pandas DataFrame
- `pyarrow.parquet.read_table()` â†’ PyArrow Table
- `fastparquet.ParquetFile` â†’ pandas DataFrame

### 2. æ–¹æ³•åç§°çš„ç»†å¾®å·®å¼‚

è™½ç„¶åŠŸèƒ½ç›¸åŒï¼Œä½†æ–¹æ³•åç§°ä¸åŒï¼š
- pandas: `tolist()`
- PyArrow: `to_pylist()`
- NumPy: `tolist()`

### 3. æµ‹è¯•çš„é‡è¦æ€§

è¿™ä¸ª bug åœ¨ä¼˜åŒ–æ—¶æ²¡æœ‰è¢«å‘ç°ï¼Œå› ä¸ºï¼š
- æ²¡æœ‰è¿è¡Œæµ‹è¯•
- åªæ˜¯ç†è®ºåˆ†æï¼Œæ²¡æœ‰å®é™…æ‰§è¡Œ

**æ•™è®­**ï¼šä¼˜åŒ–ååº”è¯¥ç«‹å³æµ‹è¯•ï¼

### 4. æ–‡æ¡£çš„é‡è¦æ€§

åº”è¯¥åœ¨ä»£ç æ³¨é‡Šä¸­æ˜ç¡®è¯´æ˜ï¼š
```python
# rows æ˜¯ pandas DataFrameï¼ˆfastparquet è¿”å›ï¼‰
# ä½¿ç”¨ tolist() è€Œä¸æ˜¯ to_pylist()
prompts = rows['prompt'].tolist()
```

---

## ğŸ“ æ€»ç»“

### Bug æœ¬è´¨
- æ··æ·†äº† pandas å’Œ PyArrow çš„æ–¹æ³•
- `to_pylist()` æ˜¯ PyArrow çš„æ–¹æ³•
- `tolist()` æ˜¯ pandas çš„æ–¹æ³•
- fastparquet è¿”å› pandas DataFrame

### ä¿®å¤æ–¹æ³•
- å°† `to_pylist()` æ”¹ä¸º `tolist()`
- æ·»åŠ æ³¨é‡Šè¯´æ˜ä½¿ç”¨çš„æ˜¯ pandas

### å½±å“èŒƒå›´
- `dataset_length_cnt` å‡½æ•°
- `parquet_to_json` å‡½æ•°
- `process_belle_knowledge_enhanced_dataset_for_finetune` å‡½æ•°ï¼ˆ2å¤„ï¼‰

### ä¿®å¤æ•ˆæœ
- âœ… é”™è¯¯å·²ä¿®å¤
- âœ… ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œ
- âœ… æ€§èƒ½æ²¡æœ‰å½±å“ï¼ˆä¸¤ç§æ–¹æ³•æ€§èƒ½ç›¸åŒï¼‰

---

**ä¿®å¤æ—¥æœŸ**: 2026-02-05  
**Bug ä¸¥é‡ç¨‹åº¦**: ğŸ”´ é«˜ï¼ˆå¯¼è‡´ç¨‹åºå´©æºƒï¼‰  
**ä¿®å¤çŠ¶æ€**: âœ… å·²ä¿®å¤  
**æµ‹è¯•çŠ¶æ€**: â³ å¾…éªŒè¯