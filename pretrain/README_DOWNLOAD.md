# é¢„è®­ç»ƒæ•°æ®é›†ä¸‹è½½å’Œå¤„ç†æŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•ä½¿ç”¨ `download_and_process_datasets.py` è„šæœ¬ä»è¿œç«¯ä¸‹è½½å¹¶å¤„ç†é¢„è®­ç»ƒæ‰€éœ€çš„æ•°æ®é›†ã€‚

## ğŸ“‹ æ•°æ®é›†æ¦‚è§ˆ

æœ¬é¡¹ç›®ä½¿ç”¨ä»¥ä¸‹å…¬å¼€æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼š

| æ•°æ®é›† | æè¿° | åŸå§‹æ•°é‡ | æ¸…æ´—åæ•°é‡ | æ¥æº |
|--------|------|----------|------------|------|
| webtext2019zh | ç¤¾åŒºé—®ç­”æ•°æ®é›† | 410ä¸‡ | 260ä¸‡ | [nlp_chinese_corpus](https://github.com/brightmart/nlp_chinese_corpus) |
| baike_qa2019 | ç™¾ç§‘ç±»é—®ç­” | 140ä¸‡ | 130ä¸‡ | [ç™¾åº¦AI Studio](https://aistudio.baidu.com/datasetdetail/107726) |
| chinese_medical | åŒ»è¯é¢†åŸŸé—®ç­” | 79ä¸‡ | 79ä¸‡ | [Chinese-medical-dialogue-data](https://github.com/Toyhom/Chinese-medical-dialogue-data) |
| zhihu_kol | çŸ¥ä¹é—®ç­”æ•°æ® | 100ä¸‡ | 97ä¸‡ | [Zhihu-KOL](https://huggingface.co/datasets/wangrui6/Zhihu-KOL) |
| belle | BELLEæŒ‡ä»¤è®­ç»ƒæ•°æ® | 370ä¸‡ | 338ä¸‡ | [BelleGroup](https://huggingface.co/BelleGroup) |
| wiki | ç»´åŸºç™¾ç§‘è¯æ¡ | - | 119ä¸‡ | [zhwiki](https://dumps.wikimedia.org/zhwiki/) |

**æ€»è®¡**: çº¦1023ä¸‡æ¡æ•°æ®ï¼ˆé¢„è®­ç»ƒé›†930ä¸‡ + è¯„ä¼°é›†2.5ä¸‡ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install requests tqdm ujson pandas pyarrow fastparquet datasets opencc-python-reimplemented colorlog rich matplotlib
```

### 2. ä¸‹è½½æ‰€æœ‰æ•°æ®é›†

```bash
cd pretrain
python download_and_process_datasets.py --download-all
```

### 3. å¤„ç†æ•°æ®é›†

```bash
python download_and_process_datasets.py --process
```

### 4. ä¸€é”®ä¸‹è½½å¹¶å¤„ç†

```bash
python download_and_process_datasets.py --download-all --process
```

## ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜

### ä¸‹è½½ç‰¹å®šæ•°æ®é›†

å¦‚æœåªæƒ³ä¸‹è½½éƒ¨åˆ†æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨ `--download` å‚æ•°ï¼š

```bash
# åªä¸‹è½½webtext2019zhå’Œbaike_qa
python download_and_process_datasets.py --download webtext2019zh baike_qa

# ä¸‹è½½é™¤wikiå¤–çš„æ‰€æœ‰æ•°æ®é›†
python download_and_process_datasets.py --download webtext2019zh baike_qa chinese_medical belle zhihu_kol
```

### è·³è¿‡ç»´åŸºç™¾ç§‘æ•°æ®é›†

ç»´åŸºç™¾ç§‘æ•°æ®æ–‡ä»¶è¾ƒå¤§ï¼ˆçº¦2.7GBï¼‰ï¼Œå¦‚æœä¸éœ€è¦å¯ä»¥è·³è¿‡ï¼š

```bash
python download_and_process_datasets.py --download-all --skip-wiki
```

### åªå¤„ç†å·²ä¸‹è½½çš„æ•°æ®é›†

å¦‚æœå·²ç»æ‰‹åŠ¨ä¸‹è½½äº†æ•°æ®é›†ï¼Œå¯ä»¥ç›´æ¥å¤„ç†ï¼š

```bash
python download_and_process_datasets.py --process
```

## ğŸ“ ç›®å½•ç»“æ„

ä¸‹è½½å’Œå¤„ç†åçš„ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```
ChatLM-mini-Chinese/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data/                          # åŸå§‹æ•°æ®é›†
â”‚   â”‚   â”œâ”€â”€ web_text_zh_train.json
â”‚   â”‚   â”œâ”€â”€ web_text_zh_valid.json
â”‚   â”‚   â”œâ”€â”€ web_text_zh_test.json
â”‚   â”‚   â”œâ”€â”€ baike_qa_train.json
â”‚   â”‚   â”œâ”€â”€ baike_qa_valid.json
â”‚   â”‚   â”œâ”€â”€ chinese_medical_dialogue_datasets/
â”‚   â”‚   â”œâ”€â”€ bell_open_source/
â”‚   â”‚   â”‚   â”œâ”€â”€ Belle_open_source_1M.json
â”‚   â”‚   â”‚   â”œâ”€â”€ train_2M_CN.json
â”‚   â”‚   â”‚   â””â”€â”€ train_3.5M_CN.json
â”‚   â”‚   â”œâ”€â”€ zhihu-kol/
â”‚   â”‚   â””â”€â”€ zhwiki-latest-pages-articles-multistream.xml.bz2
â”‚   â”‚
â”‚   â”œâ”€â”€ my_data/                           # å¤„ç†åçš„æ•°æ®é›†
â”‚   â”‚   â”œâ”€â”€ my_web_text_zh.parquet
â”‚   â”‚   â”œâ”€â”€ my_baike_qa.parquet
â”‚   â”‚   â”œâ”€â”€ my_chinese_medical_dialogue.parquet
â”‚   â”‚   â”œâ”€â”€ zhihu_kol.parquet
â”‚   â”‚   â”œâ”€â”€ my_belll_3M_cn.parquet
â”‚   â”‚   â””â”€â”€ wiki_zh_simple.parquet
â”‚   â”‚
â”‚   â”œâ”€â”€ my_dataset.parquet                 # åˆå¹¶åçš„æ•°æ®é›†
â”‚   â”œâ”€â”€ my_dataset.shuffle.parquet         # æ‰“ä¹±åçš„æ•°æ®é›†
â”‚   â”œâ”€â”€ my_train_dataset.parquet           # è®­ç»ƒé›†
â”‚   â”œâ”€â”€ my_test_dataset.parquet            # æµ‹è¯•é›†
â”‚   â”œâ”€â”€ my_valid_dataset.parquet           # éªŒè¯é›†
â”‚   â”œâ”€â”€ my_corpus.txt                      # æ–‡æœ¬æ ¼å¼ï¼ˆç”¨äºè®­ç»ƒtokenizerï¼‰
â”‚   â””â”€â”€ my_finetune_data_zh.parquet        # å¾®è°ƒæ•°æ®é›†
â”‚
â””â”€â”€ logs/
    â”œâ”€â”€ download_datasets.log              # ä¸‹è½½æ—¥å¿—
    â””â”€â”€ raw_data_process.log               # å¤„ç†æ—¥å¿—
```

## ğŸ”§ æ•°æ®å¤„ç†æµç¨‹

è„šæœ¬ä¼šè‡ªåŠ¨æ‰§è¡Œä»¥ä¸‹å¤„ç†æ­¥éª¤ï¼š

1. **æ•°æ®æ¸…æ´—**: 
   - åˆ é™¤é‡å¤çš„æ ‡ç‚¹ç¬¦å·
   - å°†è‹±æ–‡æ ‡ç‚¹è½¬æ¢ä¸ºä¸­æ–‡æ ‡ç‚¹
   - åˆ é™¤è¿‡çŸ­çš„é—®ç­”å¯¹
   - è¿‡æ»¤ä½è´¨é‡æ•°æ®

2. **æ ¼å¼ç»Ÿä¸€**: 
   - ç»Ÿä¸€è½¬æ¢ä¸º `{prompt, response}` æ ¼å¼
   - ä¿å­˜ä¸º parquet æ ¼å¼ï¼ˆé«˜æ•ˆå‹ç¼©ï¼‰

3. **æ•°æ®åˆå¹¶**: 
   - åˆå¹¶æ‰€æœ‰å¤„ç†åçš„æ•°æ®é›†
   - é™åˆ¶æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤512å­—ç¬¦ï¼‰

4. **å»é‡**: 
   - ä½¿ç”¨MinHashç®—æ³•å»é™¤é‡å¤æ–‡æ¡£
   - ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š0.85

5. **æ•°æ®æ‰“ä¹±**: 
   - éšæœºæ‰“ä¹±æ•°æ®é¡ºåº
   - å›ºå®šéšæœºç§å­ï¼š23333

6. **æ•°æ®åˆ’åˆ†**: 
   - è®­ç»ƒé›†ï¼š91%
   - æµ‹è¯•é›†ï¼š8.75%
   - éªŒè¯é›†ï¼š0.25%

7. **æ ¼å¼è½¬æ¢**: 
   - ç”Ÿæˆæ–‡æœ¬æ ¼å¼ï¼ˆç”¨äºè®­ç»ƒtokenizerï¼‰
   - ç”ŸæˆJSONæ ¼å¼ï¼ˆç”¨äºå…¶ä»–ç”¨é€”ï¼‰

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. ç½‘ç»œé—®é¢˜

æŸäº›æ•°æ®é›†æ‰˜ç®¡åœ¨HuggingFaceä¸Šï¼Œå›½å†…è®¿é—®å¯èƒ½è¾ƒæ…¢ã€‚å»ºè®®ï¼š

- ä½¿ç”¨ä»£ç†æˆ–é•œåƒç«™
- è®¾ç½®HuggingFaceé•œåƒï¼š
  ```bash
  export HF_ENDPOINT=https://hf-mirror.com
  ```

### 2. ç£ç›˜ç©ºé—´

ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼š

- åŸå§‹æ•°æ®ï¼šçº¦10GB
- å¤„ç†åæ•°æ®ï¼šçº¦5GB
- æ€»è®¡éœ€è¦ï¼šçº¦15-20GB

### 3. å¤„ç†æ—¶é—´

å®Œæ•´å¤„ç†æ‰€æœ‰æ•°æ®é›†å¯èƒ½éœ€è¦ï¼š

- ä¸‹è½½æ—¶é—´ï¼š1-3å°æ—¶ï¼ˆå–å†³äºç½‘ç»œé€Ÿåº¦ï¼‰
- å¤„ç†æ—¶é—´ï¼š2-4å°æ—¶ï¼ˆå–å†³äºCPUæ€§èƒ½ï¼‰

### 4. ç»´åŸºç™¾ç§‘æ•°æ®

ç»´åŸºç™¾ç§‘æ•°æ®éœ€è¦é¢å¤–å¤„ç†ï¼š

1. ä¸‹è½½ bz2 æ–‡ä»¶
2. ä½¿ç”¨ WikiExtractor æå–æ–‡æœ¬
3. è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡

è¯¦ç»†æ­¥éª¤è¯·å‚è€ƒ `tokenize/process_zhwiki.py`

### 5. æ‰‹åŠ¨ä¸‹è½½

å¦‚æœè‡ªåŠ¨ä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†å¹¶æ”¾ç½®åˆ°å¯¹åº”ç›®å½•ï¼š

- webtext2019zh: ä» [HuggingFace](https://huggingface.co/datasets/silver/webtext2019zh) ä¸‹è½½
- baike_qa: ä» [ç™¾åº¦AI Studio](https://aistudio.baidu.com/datasetdetail/107726) ä¸‹è½½
- chinese_medical: ä» [GitHub](https://github.com/Toyhom/Chinese-medical-dialogue-data) ä¸‹è½½
- belle: ä» [HuggingFace BelleGroup](https://huggingface.co/BelleGroup) ä¸‹è½½
- zhihu_kol: ä» [HuggingFace](https://huggingface.co/datasets/wangrui6/Zhihu-KOL) ä¸‹è½½
- wiki: ä» [Wikimedia](https://dumps.wikimedia.org/zhwiki/) ä¸‹è½½

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ä¸‹è½½é€Ÿåº¦å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

A: å¯ä»¥ä½¿ç”¨ä»£ç†æˆ–è€…æ‰‹åŠ¨ä¸‹è½½åæ”¾åˆ°å¯¹åº”ç›®å½•ã€‚

### Q2: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

A: è„šæœ¬ä½¿ç”¨æµå¼å¤„ç†ï¼Œå†…å­˜å ç”¨è¾ƒå°ã€‚å¦‚æœä»ç„¶ä¸è¶³ï¼Œå¯ä»¥ï¼š
- å‡å° `groups_cnt` å‚æ•°ï¼ˆé»˜è®¤50000ï¼‰
- åˆ†æ‰¹å¤„ç†æ•°æ®é›†

### Q3: æŸä¸ªæ•°æ®é›†ä¸‹è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

A: å¯ä»¥å•ç‹¬ä¸‹è½½è¯¥æ•°æ®é›†ï¼š
```bash
python download_and_process_datasets.py --download <dataset_name>
```

### Q4: å¦‚ä½•éªŒè¯æ•°æ®é›†æ˜¯å¦æ­£ç¡®ï¼Ÿ

A: è„šæœ¬ä¼šåœ¨å¤„ç†å®Œæˆåè‡ªåŠ¨ç»Ÿè®¡æ•°æ®é‡ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨è¿è¡Œï¼š
```python
from raw_data_process import count_my_parquet_data
count_my_parquet_data(PROJECT_ROOT + '/data/')
```

### Q5: å‡ºç° "cannot import name 'Logger'" é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

A: è¿™æ˜¯å› ä¸ºç³»ç»Ÿä¸­å®‰è£…äº†ç¬¬ä¸‰æ–¹ `logger` åŒ…å¯¼è‡´å‘½åå†²çªã€‚è§£å†³æ–¹æ¡ˆï¼š
```bash
# æ–¹æ¡ˆ1: å¸è½½å†²çªçš„åŒ…ï¼ˆå¦‚æœä¸éœ€è¦ï¼‰
pip uninstall logger

# æ–¹æ¡ˆ2: å·²ç»ä¿®å¤ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°ä»£ç 
git pull origin main
```

è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ [IMPORT_FIX.md](IMPORT_FIX.md)

## ğŸ“Š æ•°æ®ç»Ÿè®¡

å¤„ç†å®Œæˆåï¼Œå¯ä»¥æŸ¥çœ‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼š

```bash
# æŸ¥çœ‹æ—¥å¿—
cat ../logs/download_datasets.log
cat ../logs/raw_data_process.log

# æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨
# æ•°æ®é•¿åº¦åˆ†å¸ƒå›¾ä¼šä¿å­˜åœ¨ img/sentence_length.png
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [åŸå§‹æ•°æ®å¤„ç†è„šæœ¬](raw_data_process.py)
- [ç»´åŸºç™¾ç§‘å¤„ç†è„šæœ¬](../tokenize/process_zhwiki.py)
- [é¡¹ç›®ä¸»é¡µ](https://github.com/your-repo/ChatLM-mini-Chinese)

## ğŸ“ è®¸å¯è¯

å„æ•°æ®é›†éµå¾ªå…¶åŸå§‹è®¸å¯è¯ï¼Œè¯·åœ¨ä½¿ç”¨å‰æŸ¥çœ‹ç›¸åº”çš„è®¸å¯è¯ä¿¡æ¯ã€‚

## ğŸ¤ è´¡çŒ®

å¦‚æœå‘ç°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿æäº¤Issueæˆ–Pull Requestã€‚
