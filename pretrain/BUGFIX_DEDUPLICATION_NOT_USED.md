# æ•°æ®å¤„ç†æµç¨‹ Bug ä¿®å¤è¯´æ˜

## ğŸ› å‘ç°çš„é—®é¢˜

### é—®é¢˜æè¿°
åœ¨æ•°æ®å¤„ç†æµç¨‹ä¸­å‘ç°äº†ä¸€ä¸ªä¸¥é‡çš„ bugï¼š**å»é‡åçš„æ•°æ®æ²¡æœ‰è¢«åç»­æ­¥éª¤ä½¿ç”¨**ã€‚

### é—®é¢˜è¯¦æƒ…

#### åŸå§‹æµç¨‹ï¼ˆæœ‰é—®é¢˜ï¼‰ï¼š
```
1. merge_dataset_as_single_file()
   â†“ ç”Ÿæˆ: my_dataset.parquet

2. remove_dataset_duplicate_rows()
   â†“ è¯»å–: my_dataset.parquet
   â†“ ç”Ÿæˆ: my_dataset_no_dulpticates.parquet  âœ… å»é‡åçš„æ•°æ®

3. shuffle_parquet_dataset()
   â†“ è¯»å–: my_dataset.parquet  âŒ é”™è¯¯ï¼ä½¿ç”¨äº†åŸå§‹æ•°æ®
   â†“ ç”Ÿæˆ: my_dataset.shuffle.parquet

4. split_train_valid_test_datasets()
   â†“ è¯»å–: my_dataset.shuffle.parquet  âŒ åŒ…å«é‡å¤æ•°æ®
   â†“ ç”Ÿæˆ: train/valid/test æ•°æ®é›†
```

### é—®é¢˜å½±å“

1. **âŒ å»é‡æ“ä½œç™½åšäº†**
   - `remove_dataset_duplicate_rows` èŠ±è´¹äº†å¤§é‡æ—¶é—´ï¼ˆ10-15å°æ—¶ï¼‰
   - ç”Ÿæˆçš„å»é‡æ–‡ä»¶ `my_dataset_no_dulpticates.parquet` æ²¡æœ‰è¢«ä½¿ç”¨
   - æµªè´¹äº†è®¡ç®—èµ„æºå’Œæ—¶é—´

2. **âŒ è®­ç»ƒæ•°æ®åŒ…å«é‡å¤**
   - æœ€ç»ˆçš„è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†éƒ½åŒ…å«é‡å¤æ•°æ®
   - å¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆ
   - å½±å“æ¨¡å‹è®­ç»ƒæ•ˆæœ

3. **âŒ æ•°æ®ç»Ÿè®¡ä¸å‡†ç¡®**
   - æŠ¥å‘Šçš„å»é‡ç‡æ˜¯æ­£ç¡®çš„
   - ä½†å®é™…ä½¿ç”¨çš„æ•°æ®æ²¡æœ‰å»é‡
   - ç»Ÿè®¡ä¿¡æ¯ä¸å®é™…ä¸ç¬¦

### é—®é¢˜åŸå› 

åœ¨ `download_and_process_datasets.py` çš„ `process_all_datasets()` å‡½æ•°ä¸­ï¼š

```python
# ç¬¬ 6 æ­¥ï¼šå»é‡
remove_dataset_duplicate_rows(groups_cnt=50000)
# ç”Ÿæˆ: my_dataset_no_dulpticates.parquet

# ç¬¬ 7 æ­¥ï¼šæ‰“ä¹±ï¼ˆé”™è¯¯çš„ä»£ç ï¼‰
shuffle_parquet_dataset(
    parquet_file=PROJECT_ROOT + '/data/my_dataset.parquet',  # âŒ ä½¿ç”¨äº†åŸå§‹æ–‡ä»¶
    shuffle_file=PROJECT_ROOT + '/data/my_dataset.shuffle.parquet',
    seed=23333
)
```

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤åçš„æµç¨‹

```
1. merge_dataset_as_single_file()
   â†“ ç”Ÿæˆ: my_dataset.parquet

2. remove_dataset_duplicate_rows()
   â†“ è¯»å–: my_dataset.parquet
   â†“ ç”Ÿæˆ: my_dataset_no_dulpticates.parquet  âœ… å»é‡åçš„æ•°æ®

3. shuffle_parquet_dataset()
   â†“ è¯»å–: my_dataset_no_dulpticates.parquet  âœ… æ­£ç¡®ï¼ä½¿ç”¨å»é‡åçš„æ•°æ®
   â†“ ç”Ÿæˆ: my_dataset.shuffle.parquet

4. split_train_valid_test_datasets()
   â†“ è¯»å–: my_dataset.shuffle.parquet  âœ… ä¸åŒ…å«é‡å¤æ•°æ®
   â†“ ç”Ÿæˆ: train/valid/test æ•°æ®é›†
```

### ä¿®å¤ä»£ç 

ä¿®æ”¹ `download_and_process_datasets.py` ä¸­çš„ç¬¬ 7 æ­¥ï¼š

```python
# 7. æ‰“ä¹±æ•°æ®ï¼ˆä½¿ç”¨å»é‡åçš„æ•°æ®é›†ï¼‰
log.info("æ‰“ä¹±æ•°æ®é›†...", save_to_file=True)
shuffle_parquet_dataset(
    parquet_file=PROJECT_ROOT + '/data/my_dataset_no_dulpticates.parquet',  # âœ… ä½¿ç”¨å»é‡åçš„æ–‡ä»¶
    shuffle_file=PROJECT_ROOT + '/data/my_dataset.shuffle.parquet',
    seed=23333
)
```

---

## ğŸ“Š ä¿®å¤æ•ˆæœ

### ä¿®å¤å‰ vs ä¿®å¤å

| æŒ‡æ ‡ | ä¿®å¤å‰ï¼ˆé”™è¯¯ï¼‰ | ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰ |
|------|--------------|--------------|
| **å»é‡æ“ä½œ** | æ‰§è¡Œä½†æœªä½¿ç”¨ | æ‰§è¡Œå¹¶ä½¿ç”¨ âœ… |
| **è®­ç»ƒæ•°æ®** | åŒ…å«é‡å¤ âŒ | ä¸åŒ…å«é‡å¤ âœ… |
| **æ•°æ®è´¨é‡** | ä½ | é«˜ âœ… |
| **æ¨¡å‹æ•ˆæœ** | å¯èƒ½è¿‡æ‹Ÿåˆ | æ­£å¸¸ âœ… |
| **æ—¶é—´æµªè´¹** | 10-15å°æ—¶ç™½è´¹ | æ— æµªè´¹ âœ… |

### æ•°æ®é‡å˜åŒ–ç¤ºä¾‹

å‡è®¾åŸå§‹æ•°æ®æœ‰ 500 ä¸‡æ¡ï¼Œå»é‡ç‡ä¸º 15%ï¼š

```
ä¿®å¤å‰ï¼ˆé”™è¯¯æµç¨‹ï¼‰ï¼š
â”œâ”€ my_dataset.parquet: 500ä¸‡æ¡
â”œâ”€ my_dataset_no_dulpticates.parquet: 425ä¸‡æ¡ï¼ˆæœªä½¿ç”¨ï¼‰
â”œâ”€ my_dataset.shuffle.parquet: 500ä¸‡æ¡ï¼ˆåŒ…å«é‡å¤ï¼‰
â””â”€ è®­ç»ƒé›†: 500ä¸‡æ¡ï¼ˆåŒ…å«é‡å¤ï¼‰âŒ

ä¿®å¤åï¼ˆæ­£ç¡®æµç¨‹ï¼‰ï¼š
â”œâ”€ my_dataset.parquet: 500ä¸‡æ¡
â”œâ”€ my_dataset_no_dulpticates.parquet: 425ä¸‡æ¡ï¼ˆå·²ä½¿ç”¨ï¼‰âœ…
â”œâ”€ my_dataset.shuffle.parquet: 425ä¸‡æ¡ï¼ˆä¸åŒ…å«é‡å¤ï¼‰
â””â”€ è®­ç»ƒé›†: 425ä¸‡æ¡ï¼ˆä¸åŒ…å«é‡å¤ï¼‰âœ…
```

---

## ğŸ” å¦‚ä½•éªŒè¯ä¿®å¤

### 1. æ£€æŸ¥æ–‡ä»¶å¤§å°

ä¿®å¤åï¼Œ`my_dataset.shuffle.parquet` çš„å¤§å°åº”è¯¥ä¸ `my_dataset_no_dulpticates.parquet` ç›¸åŒï¼š

```bash
ls -lh /path/to/data/*.parquet
```

**é¢„æœŸç»“æœ**ï¼š
```
my_dataset.parquet                  # è¾ƒå¤§ï¼ˆåŒ…å«é‡å¤ï¼‰
my_dataset_no_dulpticates.parquet   # è¾ƒå°ï¼ˆå»é‡åï¼‰
my_dataset.shuffle.parquet          # ä¸ no_dulpticates å¤§å°ç›¸åŒ âœ…
```

### 2. æ£€æŸ¥è¡Œæ•°

```python
import pyarrow.parquet as pq

# è¯»å–æ–‡ä»¶è¡Œæ•°
original = pq.read_table('my_dataset.parquet').num_rows
dedup = pq.read_table('my_dataset_no_dulpticates.parquet').num_rows
shuffle = pq.read_table('my_dataset.shuffle.parquet').num_rows

print(f"åŸå§‹æ•°æ®: {original:,} è¡Œ")
print(f"å»é‡å: {dedup:,} è¡Œ")
print(f"æ‰“ä¹±å: {shuffle:,} è¡Œ")

# éªŒè¯
assert dedup == shuffle, "æ‰“ä¹±åçš„æ•°æ®åº”è¯¥ä¸å»é‡åçš„æ•°æ®è¡Œæ•°ç›¸åŒï¼"
print("âœ… éªŒè¯é€šè¿‡ï¼")
```

**é¢„æœŸè¾“å‡º**ï¼š
```
åŸå§‹æ•°æ®: 5,000,000 è¡Œ
å»é‡å: 4,250,000 è¡Œ
æ‰“ä¹±å: 4,250,000 è¡Œ
âœ… éªŒè¯é€šè¿‡ï¼
```

### 3. æ£€æŸ¥æ—¥å¿—

æŸ¥çœ‹å¤„ç†æ—¥å¿—ï¼Œç¡®è®¤å»é‡ç‡ï¼š

```bash
grep "å»é‡ç‡" logs/download_datasets.log
```

**é¢„æœŸè¾“å‡º**ï¼š
```
å»é‡ç‡: 15.00%
```

---

## âš ï¸ é‡è¦æé†’

### å¦‚æœä½ å·²ç»è¿è¡Œè¿‡åŸå§‹ä»£ç 

å¦‚æœä½ å·²ç»ä½¿ç”¨æœ‰ bug çš„ä»£ç å¤„ç†è¿‡æ•°æ®ï¼Œéœ€è¦ï¼š

1. **åˆ é™¤é”™è¯¯çš„æ–‡ä»¶**ï¼š
   ```bash
   rm /path/to/data/my_dataset.shuffle.parquet
   rm /path/to/data/my_train_data.parquet
   rm /path/to/data/my_valid_data.parquet
   rm /path/to/data/my_test_data.parquet
   ```

2. **é‡æ–°è¿è¡Œåç»­æ­¥éª¤**ï¼š
   ```bash
   # ä»æ‰“ä¹±æ•°æ®å¼€å§‹é‡æ–°è¿è¡Œ
   python download_and_process_datasets.py --process
   ```
   
   æˆ–è€…æ‰‹åŠ¨è¿è¡Œï¼š
   ```python
   from raw_data_process import (
       shuffle_parquet_dataset,
       split_train_valid_test_datasets,
   )
   
   # 7. æ‰“ä¹±æ•°æ®ï¼ˆä½¿ç”¨å»é‡åçš„æ•°æ®ï¼‰
   shuffle_parquet_dataset(
       parquet_file=PROJECT_ROOT + '/data/my_dataset_no_dulpticates.parquet',
       shuffle_file=PROJECT_ROOT + '/data/my_dataset.shuffle.parquet',
       seed=23333
   )
   
   # 8. åˆ’åˆ†æ•°æ®é›†
   split_train_valid_test_datasets(
       source_parquet_file=PROJECT_ROOT + '/data/my_dataset.shuffle.parquet',
       max_len=320,
       groups_cnt=50000
   )
   ```

3. **éªŒè¯ä¿®å¤**ï¼š
   ä½¿ç”¨ä¸Šé¢çš„éªŒè¯æ–¹æ³•ç¡®è®¤æ•°æ®æ­£ç¡®

### å¦‚æœä½ è¿˜æ²¡æœ‰è¿è¡Œè¿‡

ç›´æ¥ä½¿ç”¨ä¿®å¤åçš„ä»£ç å³å¯ï¼Œæ— éœ€é¢å¤–æ“ä½œã€‚

---

## ğŸ“ ç›¸å…³æ–‡ä»¶

### ä¿®æ”¹çš„æ–‡ä»¶

1. **[download_and_process_datasets.py](/Users/twrong/git/code/ChatLM-mini-Chinese/pretrain/download_and_process_datasets.py)**
   - ä¿®æ”¹äº†ç¬¬ 7 æ­¥çš„ `shuffle_parquet_dataset` è°ƒç”¨
   - ä»ä½¿ç”¨ `my_dataset.parquet` æ”¹ä¸º `my_dataset_no_dulpticates.parquet`

### æ¶‰åŠçš„å‡½æ•°

1. **`remove_dataset_duplicate_rows`** (raw_data_process.py)
   - è¾“å…¥: `my_dataset.parquet`
   - è¾“å‡º: `my_dataset_no_dulpticates.parquet`

2. **`shuffle_parquet_dataset`** (raw_data_process.py)
   - è¾“å…¥: `my_dataset_no_dulpticates.parquet` âœ…ï¼ˆä¿®å¤åï¼‰
   - è¾“å‡º: `my_dataset.shuffle.parquet`

3. **`split_train_valid_test_datasets`** (raw_data_process.py)
   - è¾“å…¥: `my_dataset.shuffle.parquet`
   - è¾“å‡º: `my_train_data.parquet`, `my_valid_data.parquet`, `my_test_data.parquet`

---

## ğŸ¯ æ€»ç»“

### Bug æœ¬è´¨
- æ•°æ®å¤„ç†æµç¨‹ä¸­çš„æ–‡ä»¶è·¯å¾„é”™è¯¯
- å»é‡åçš„æ•°æ®æ²¡æœ‰è¢«åç»­æ­¥éª¤ä½¿ç”¨

### ä¿®å¤æ–¹æ³•
- ä¿®æ”¹ `shuffle_parquet_dataset` çš„è¾“å…¥æ–‡ä»¶è·¯å¾„
- ä» `my_dataset.parquet` æ”¹ä¸º `my_dataset_no_dulpticates.parquet`

### ä¿®å¤æ•ˆæœ
- âœ… å»é‡æ“ä½œä¸å†ç™½è´¹
- âœ… è®­ç»ƒæ•°æ®ä¸åŒ…å«é‡å¤
- âœ… æ•°æ®è´¨é‡æå‡
- âœ… æ¨¡å‹è®­ç»ƒæ•ˆæœæ›´å¥½

### éªŒè¯æ–¹æ³•
- æ£€æŸ¥æ–‡ä»¶å¤§å°
- æ£€æŸ¥è¡Œæ•°
- æ£€æŸ¥æ—¥å¿—

---

**ä¿®å¤æ—¥æœŸ**: 2026-02-05  
**Bug ä¸¥é‡ç¨‹åº¦**: ğŸ”´ é«˜ï¼ˆå½±å“æ•°æ®è´¨é‡å’Œè®­ç»ƒæ•ˆæœï¼‰  
**ä¿®å¤çŠ¶æ€**: âœ… å·²ä¿®å¤  
**æµ‹è¯•çŠ¶æ€**: â³ å¾…éªŒè¯