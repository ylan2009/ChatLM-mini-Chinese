# HuggingFace æ•°æ®é›†ä¸‹è½½æ–¹å¼æ›´æ–°è¯´æ˜

## ğŸ“ æ›´æ–°å†…å®¹

æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œå·²å°†æ‰€æœ‰ HuggingFace æ•°æ®é›†çš„ä¸‹è½½æ–¹å¼ä»"ç›´æ¥ä¸‹è½½ URL æ–‡ä»¶"æ”¹ä¸º"ä½¿ç”¨ `datasets` åº“ä¸‹è½½"ï¼Œå‚è€ƒ `download_belle_sft_dataset.py` çš„å®ç°æ–¹å¼ã€‚

## ğŸ”„ ä¸»è¦ä¿®æ”¹

### 1. ä¿®æ”¹çš„æ•°æ®é›†

ä»¥ä¸‹æ•°æ®é›†çš„ä¸‹è½½æ–¹å¼å·²æ›´æ–°ï¼š

| æ•°æ®é›† | HuggingFace è·¯å¾„ | åŸä¸‹è½½æ–¹å¼ | æ–°ä¸‹è½½æ–¹å¼ |
|--------|------------------|------------|------------|
| webtext2019zh | `silver/webtext2019zh` | ç›´æ¥ä¸‹è½½ JSON æ–‡ä»¶ | `load_dataset()` |
| baike_qa | `silver/baike_qa2019` | ç›´æ¥ä¸‹è½½ JSON æ–‡ä»¶ | `load_dataset()` |
| belle | `BelleGroup/train_*M_CN` | ç›´æ¥ä¸‹è½½ JSON æ–‡ä»¶ | `load_dataset()` |
| zhihu_kol | `wangrui6/Zhihu-KOL` | å·²ä½¿ç”¨ `load_dataset()` | æ— å˜åŒ– |

**æœªä¿®æ”¹çš„æ•°æ®é›†**ï¼š
- `chinese_medical`: ä» GitHub ä¸‹è½½ ZIP æ–‡ä»¶ï¼ˆä¸åœ¨ HuggingFace ä¸Šï¼‰
- `wiki`: ä½¿ç”¨é¡¹ç›®ä¸­å·²æœ‰çš„ `data/wiki.simple.txt` æ–‡ä»¶

### 2. é…ç½®æ–‡ä»¶ä¿®æ”¹

#### ä¿®æ”¹å‰ï¼ˆä½¿ç”¨ URLï¼‰

```python
DATASETS_CONFIG = {
    'webtext2019zh': {
        'urls': {
            'train': 'https://huggingface.co/datasets/silver/webtext2019zh/resolve/main/web_text_zh_train.json',
            'valid': 'https://huggingface.co/datasets/silver/webtext2019zh/resolve/main/web_text_zh_valid.json',
            'test': 'https://huggingface.co/datasets/silver/webtext2019zh/resolve/main/web_text_zh_test.json',
        },
        'save_dir': PROJECT_ROOT + '/data/raw_data/',
    },
    'belle': {
        'urls': {
            'belle_1m': 'https://huggingface.co/datasets/BelleGroup/train_1M_CN/resolve/main/Belle_open_source_1M.json',
            'belle_2m': 'https://huggingface.co/datasets/BelleGroup/train_2M_CN/resolve/main/train_2M_CN.json',
            # ...
        },
        'save_dir': PROJECT_ROOT + '/data/raw_data/bell_open_source/',
    },
}
```

#### ä¿®æ”¹åï¼ˆä½¿ç”¨ datasets åº“ï¼‰

```python
DATASETS_CONFIG = {
    'webtext2019zh': {
        'note': 'webtext2019zhæ•°æ®é›†éœ€è¦ä»HuggingFaceä¸‹è½½ï¼Œä½¿ç”¨datasetsåº“',
        'hf_dataset': 'silver/webtext2019zh',
        'save_dir': PROJECT_ROOT + '/data/raw_data/',
    },
    'baike_qa': {
        'note': 'ç™¾åº¦ç™¾ç§‘é—®ç­”æ•°æ®é›†éœ€è¦ä»HuggingFaceä¸‹è½½ï¼Œä½¿ç”¨datasetsåº“',
        'hf_dataset': 'silver/baike_qa2019',
        'save_dir': PROJECT_ROOT + '/data/raw_data/',
    },
    'belle': {
        'note': 'BELLEæ•°æ®é›†éœ€è¦ä»HuggingFaceä¸‹è½½ï¼Œä½¿ç”¨datasetsåº“',
        'hf_datasets': [
            'BelleGroup/train_1M_CN',
            'BelleGroup/train_2M_CN',
            'BelleGroup/train_3.5M_CN',
        ],
        'save_dir': PROJECT_ROOT + '/data/raw_data/belle/',
    },
}
```

### 3. ä¸‹è½½å‡½æ•°ä¿®æ”¹

#### webtext2019zh

**ä¿®æ”¹å‰**ï¼š
```python
def download_webtext2019zh() -> bool:
    """ä¸‹è½½webtext2019zhæ•°æ®é›†"""
    config = DATASETS_CONFIG['webtext2019zh']
    ensure_dir(config['save_dir'])
    
    success = True
    for name, url in config['urls'].items():
        save_path = os.path.join(config['save_dir'], f'web_text_zh_{name}.json')
        if not download_file(url, save_path):
            success = False
    
    return success
```

**ä¿®æ”¹å**ï¼š
```python
def download_webtext2019zh() -> bool:
    """ä¸‹è½½webtext2019zhæ•°æ®é›†ï¼ˆä½¿ç”¨HuggingFace datasetsåº“ï¼‰"""
    try:
        from datasets import load_dataset
        
        config = DATASETS_CONFIG['webtext2019zh']
        ensure_dir(config['save_dir'])
        
        log.info(f"ä»HuggingFaceä¸‹è½½: {config['hf_dataset']}", save_to_file=True)
        
        # ä¸‹è½½æ•°æ®é›†ï¼ˆåŒ…å«train, valid, teståˆ†å‰²ï¼‰
        dataset = load_dataset(config['hf_dataset'])
        
        # åˆ†åˆ«ä¿å­˜å„ä¸ªåˆ†å‰²
        for split_name in dataset.keys():
            save_path = os.path.join(config['save_dir'], f'web_text_zh_{split_name}.parquet')
            dataset[split_name].to_parquet(save_path)
            log.info(f"{split_name} æ•°æ®é›†å·²ä¿å­˜åˆ°: {save_path}", save_to_file=True)
            log.info(f"{split_name} æ•°æ®é›†å¤§å°: {len(dataset[split_name])} è¡Œ", save_to_file=True)
        
        return True
        
    except ImportError:
        log.error("éœ€è¦å®‰è£… datasets åº“: pip install datasets", save_to_file=True)
        return False
    except Exception as e:
        log.error(f"ä¸‹è½½å¤±è´¥: {str(e)}", save_to_file=True)
        return False
```

#### baike_qa

**ä¿®æ”¹å‰**ï¼š
```python
def download_baike_qa() -> bool:
    """ä¸‹è½½ç™¾åº¦ç™¾ç§‘é—®ç­”æ•°æ®é›†"""
    config = DATASETS_CONFIG['baike_qa']
    ensure_dir(config['save_dir'])
    
    success = True
    for name, url in config['urls'].items():
        save_path = os.path.join(config['save_dir'], f'baike_qa_{name}.json')
        if not download_file(url, save_path):
            success = False
    
    return success
```

**ä¿®æ”¹å**ï¼š
```python
def download_baike_qa() -> bool:
    """ä¸‹è½½ç™¾åº¦ç™¾ç§‘é—®ç­”æ•°æ®é›†ï¼ˆä½¿ç”¨HuggingFace datasetsåº“ï¼‰"""
    try:
        from datasets import load_dataset
        
        config = DATASETS_CONFIG['baike_qa']
        ensure_dir(config['save_dir'])
        
        log.info(f"ä»HuggingFaceä¸‹è½½: {config['hf_dataset']}", save_to_file=True)
        
        # ä¸‹è½½æ•°æ®é›†ï¼ˆåŒ…å«train, validåˆ†å‰²ï¼‰
        dataset = load_dataset(config['hf_dataset'])
        
        # åˆ†åˆ«ä¿å­˜å„ä¸ªåˆ†å‰²
        for split_name in dataset.keys():
            save_path = os.path.join(config['save_dir'], f'baike_qa_{split_name}.parquet')
            dataset[split_name].to_parquet(save_path)
            log.info(f"{split_name} æ•°æ®é›†å·²ä¿å­˜åˆ°: {save_path}", save_to_file=True)
            log.info(f"{split_name} æ•°æ®é›†å¤§å°: {len(dataset[split_name])} è¡Œ", save_to_file=True)
        
        return True
        
    except ImportError:
        log.error("éœ€è¦å®‰è£… datasets åº“: pip install datasets", save_to_file=True)
        return False
    except Exception as e:
        log.error(f"ä¸‹è½½å¤±è´¥: {str(e)}", save_to_file=True)
        return False
```

#### belle

**ä¿®æ”¹å‰**ï¼š
```python
def download_belle_datasets() -> bool:
    """ä¸‹è½½BELLEå¼€æºæ•°æ®é›†"""
    config = DATASETS_CONFIG['belle']
    ensure_dir(config['save_dir'])
    
    success = True
    for name, url in config['urls'].items():
        filename = url.split('/')[-1]
        save_path = os.path.join(config['save_dir'], filename)
        if not download_file(url, save_path):
            success = False
    
    return success
```

**ä¿®æ”¹å**ï¼š
```python
def download_belle_datasets() -> bool:
    """ä¸‹è½½BELLEå¼€æºæ•°æ®é›†ï¼ˆä½¿ç”¨HuggingFace datasetsåº“ï¼‰"""
    try:
        from datasets import load_dataset
        
        config = DATASETS_CONFIG['belle']
        ensure_dir(config['save_dir'])
        
        success = True
        for dataset_name in config['hf_datasets']:
            try:
                log.info(f"ä»HuggingFaceä¸‹è½½: {dataset_name}", save_to_file=True)
                
                # ä¸‹è½½æ•°æ®é›†
                dataset = load_dataset(dataset_name, split='train')
                
                # æå–æ•°æ®é›†åç§°ä½œä¸ºæ–‡ä»¶å
                # ä¾‹å¦‚: BelleGroup/train_1M_CN -> train_1M_CN
                file_name = dataset_name.split('/')[-1]
                save_path = os.path.join(config['save_dir'], f'{file_name}.parquet')
                
                # ä¿å­˜ä¸ºparquetæ ¼å¼
                dataset.to_parquet(save_path)
                
                log.info(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {save_path}", save_to_file=True)
                log.info(f"æ•°æ®é›†å¤§å°: {len(dataset)} è¡Œ", save_to_file=True)
                
            except Exception as e:
                log.error(f"ä¸‹è½½ {dataset_name} å¤±è´¥: {str(e)}", save_to_file=True)
                success = False
        
        return success
        
    except ImportError:
        log.error("éœ€è¦å®‰è£… datasets åº“: pip install datasets", save_to_file=True)
        return False
    except Exception as e:
        log.error(f"ä¸‹è½½å¤±è´¥: {str(e)}", save_to_file=True)
        return False
```

### 4. å¯ç”¨æ‰€æœ‰æ•°æ®é›†ä¸‹è½½

åœ¨ `download_all_datasets()` å‡½æ•°ä¸­ï¼Œå–æ¶ˆäº† webtext2019zh å’Œ baike_qa çš„æ³¨é‡Šï¼š

```python
def download_all_datasets() -> dict:
    results = {}
    
    log.info("å¼€å§‹ä¸‹è½½æ‰€æœ‰æ•°æ®é›†...", save_to_file=True)
    
    # 1. webtext2019zh
    results['webtext2019zh'] = download_webtext2019zh()
    
    # 2. baike_qa
    results['baike_qa'] = download_baike_qa()
    
    # 3. chinese_medical
    results['chinese_medical'] = download_chinese_medical()
    
    # 4. belle
    results['belle'] = download_belle_datasets()
    
    # 5. zhihu_kol
    results['zhihu_kol'] = download_zhihu_kol()
    
    # 6. wiki - ä¸éœ€è¦ä¸‹è½½ï¼Œç›´æ¥ä½¿ç”¨data/wiki.simple.txt
    log.info("æ³¨æ„: Wikiæ•°æ®ä½¿ç”¨é¡¹ç›®ä¸­å·²æœ‰çš„ data/wiki.simple.txt æ–‡ä»¶", save_to_file=True)
    results['wiki'] = check_wiki_simple_file()
    
    return results
```

### 5. æ›´æ–°æ–‡æ¡£

æ›´æ–°äº† [README_DOWNLOAD.md](README_DOWNLOAD.md)ï¼Œå¼ºè°ƒäº† `datasets` åº“çš„é‡è¦æ€§ã€‚

## âœ¨ ä¼˜åŠ¿

ç›¸æ¯”ä¹‹å‰ç›´æ¥ä¸‹è½½ URL æ–‡ä»¶çš„æ–¹å¼ï¼Œä½¿ç”¨ `datasets` åº“æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

### 1. **è‡ªåŠ¨ç¼“å­˜ç®¡ç†**
- HuggingFace datasets ä¼šè‡ªåŠ¨ç®¡ç†ä¸‹è½½çš„ç¼“å­˜
- é¿å…é‡å¤ä¸‹è½½ç›¸åŒçš„æ•°æ®é›†
- æ”¯æŒæ–­ç‚¹ç»­ä¼ 

### 2. **ç»Ÿä¸€çš„æ•°æ®æ ¼å¼**
- æ‰€æœ‰æ•°æ®é›†éƒ½ä¿å­˜ä¸º parquet æ ¼å¼
- ä¾¿äºåç»­å¤„ç†å’Œè¯»å–
- æ›´é«˜æ•ˆçš„å­˜å‚¨å’ŒåŠ è½½

### 3. **æ›´å¥½çš„é”™è¯¯å¤„ç†**
- è‡ªåŠ¨å¤„ç†ç½‘ç»œé”™è¯¯å’Œé‡è¯•
- æä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
- æ”¯æŒéªŒè¯æ•°æ®å®Œæ•´æ€§

### 4. **ç‰ˆæœ¬æ§åˆ¶**
- å¯ä»¥æŒ‡å®šæ•°æ®é›†çš„ç‰¹å®šç‰ˆæœ¬
- ç¡®ä¿å®éªŒçš„å¯é‡å¤æ€§

### 5. **æ›´ç®€æ´çš„ä»£ç **
- ä¸éœ€è¦æ‰‹åŠ¨å¤„ç† HTTP è¯·æ±‚
- ä¸éœ€è¦æ‰‹åŠ¨è§£æ JSON æ–‡ä»¶
- ä»£ç æ›´ç®€æ´æ˜“ç»´æŠ¤

### 6. **ä¸ HuggingFace ç”Ÿæ€é›†æˆ**
- å¯ä»¥ç›´æ¥ä½¿ç”¨ HuggingFace Hub çš„æ‰€æœ‰åŠŸèƒ½
- æ”¯æŒç§æœ‰æ•°æ®é›†
- æ”¯æŒæ•°æ®é›†çš„æµå¼åŠ è½½ï¼ˆå¯¹äºå¤§æ•°æ®é›†ï¼‰

## ğŸ“‚ æ–‡ä»¶æ ¼å¼å˜åŒ–

### ä¿®æ”¹å‰
```
data/raw_data/
â”œâ”€â”€ web_text_zh_train.json
â”œâ”€â”€ web_text_zh_valid.json
â”œâ”€â”€ web_text_zh_test.json
â”œâ”€â”€ baike_qa_train.json
â”œâ”€â”€ baike_qa_valid.json
â””â”€â”€ bell_open_source/
    â”œâ”€â”€ Belle_open_source_1M.json
    â”œâ”€â”€ train_2M_CN.json
    â””â”€â”€ train_3.5M_CN.json
```

### ä¿®æ”¹å
```
data/raw_data/
â”œâ”€â”€ web_text_zh_train.parquet
â”œâ”€â”€ web_text_zh_valid.parquet
â”œâ”€â”€ web_text_zh_test.parquet
â”œâ”€â”€ baike_qa_train.parquet
â”œâ”€â”€ baike_qa_valid.parquet
â””â”€â”€ belle/
    â”œâ”€â”€ train_1M_CN.parquet
    â”œâ”€â”€ train_2M_CN.parquet
    â””â”€â”€ train_3.5M_CN.parquet
```

**å˜åŒ–**ï¼š
- âœ… æ–‡ä»¶æ ¼å¼ä» `.json` æ”¹ä¸º `.parquet`
- âœ… belle æ•°æ®é›†ç›®å½•ä» `bell_open_source/` æ”¹ä¸º `belle/`
- âœ… æ–‡ä»¶åæ›´è§„èŒƒï¼ˆä¾‹å¦‚ï¼š`train_1M_CN.parquet` è€Œä¸æ˜¯ `Belle_open_source_1M.json`ï¼‰

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### å®‰è£…ä¾èµ–

**é‡è¦**ï¼šå¿…é¡»å®‰è£… `datasets` åº“

```bash
pip install datasets requests tqdm ujson pandas pyarrow
```

### ä¸‹è½½æ‰€æœ‰æ•°æ®é›†

```bash
cd pretrain
python download_and_process_datasets.py --download-all
```

### ä¸‹è½½ç‰¹å®šæ•°æ®é›†

```bash
python download_and_process_datasets.py --download webtext2019zh baike_qa belle
```

### ä¸‹è½½å¹¶å¤„ç†

```bash
python download_and_process_datasets.py --download-all --process
```

## ğŸ” æ•°æ®å¤„ç†å…¼å®¹æ€§

**é‡è¦æç¤º**ï¼šè™½ç„¶ä¸‹è½½çš„æ–‡ä»¶æ ¼å¼ä» JSON æ”¹ä¸º parquetï¼Œä½†åŸæœ‰çš„æ•°æ®å¤„ç†å‡½æ•°ï¼ˆåœ¨ `raw_data_process.py` ä¸­ï¼‰ä»ç„¶éœ€è¦æ›´æ–°ä»¥æ”¯æŒ parquet æ ¼å¼ã€‚

### éœ€è¦æ›´æ–°çš„å¤„ç†å‡½æ•°

ä»¥ä¸‹å‡½æ•°å¯èƒ½éœ€è¦æ›´æ–°ä»¥æ”¯æŒ parquet æ ¼å¼ï¼š

1. `process_web_text()` - å¤„ç† webtext2019zh
2. `process_bake_qa()` - å¤„ç† baike_qa
3. `process_belle_knowledge_enhanced_dataset()` - å¤„ç† belle

### æ›´æ–°å»ºè®®

å¯ä»¥åœ¨å¤„ç†å‡½æ•°ä¸­æ·»åŠ å¯¹ parquet æ ¼å¼çš„æ”¯æŒï¼š

```python
def process_web_text():
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ parquet æ–‡ä»¶
    parquet_file = PROJECT_ROOT + '/data/raw_data/web_text_zh_train.parquet'
    json_file = PROJECT_ROOT + '/data/raw_data/web_text_zh_train.json'
    
    if os.path.exists(parquet_file):
        # è¯»å– parquet æ–‡ä»¶
        df = pd.read_parquet(parquet_file)
        # å¤„ç†æ•°æ®...
    elif os.path.exists(json_file):
        # è¯»å– JSON æ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
        with open(json_file, 'r') as f:
            data = json.load(f)
        # å¤„ç†æ•°æ®...
    else:
        log.error("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
```

## ğŸ“ ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨

1. âœ… [pretrain/download_and_process_datasets.py](download_and_process_datasets.py) - ä¸»è¦ä¿®æ”¹
   - æ›´æ–° `DATASETS_CONFIG` é…ç½®
   - é‡å†™ `download_webtext2019zh()` å‡½æ•°
   - é‡å†™ `download_baike_qa()` å‡½æ•°
   - é‡å†™ `download_belle_datasets()` å‡½æ•°
   - å¯ç”¨æ‰€æœ‰æ•°æ®é›†ä¸‹è½½

2. âœ… [pretrain/README_DOWNLOAD.md](README_DOWNLOAD.md) - æ›´æ–°æ–‡æ¡£
   - å¼ºè°ƒ `datasets` åº“çš„é‡è¦æ€§
   - æ›´æ–°å®‰è£…è¯´æ˜

3. âœ… [pretrain/HUGGINGFACE_DATASETS_UPDATE.md](HUGGINGFACE_DATASETS_UPDATE.md) - æ–°å¢è¯´æ˜æ–‡æ¡£
   - è¯¦ç»†è¯´æ˜æ‰€æœ‰ä¿®æ”¹å†…å®¹
   - æä¾›ä½¿ç”¨æ–¹æ³•å’ŒæŠ€æœ¯ç»†èŠ‚

## ğŸ¯ å‘åå…¼å®¹æ€§

### ä¿ç•™çš„åŠŸèƒ½

- âœ… ä¿ç•™äº†åŸæœ‰çš„ `download_file()` å‡½æ•°ï¼ˆç”¨äº chinese_medicalï¼‰
- âœ… ä¿ç•™äº†åŸæœ‰çš„å‘½ä»¤è¡Œå‚æ•°
- âœ… ä¿ç•™äº†åŸæœ‰çš„ç›®å½•ç»“æ„

### å¯èƒ½çš„å…¼å®¹æ€§é—®é¢˜

1. **æ–‡ä»¶æ ¼å¼å˜åŒ–**ï¼šä» JSON æ”¹ä¸º parquet
   - è§£å†³æ–¹æ¡ˆï¼šæ›´æ–°æ•°æ®å¤„ç†å‡½æ•°ä»¥æ”¯æŒ parquet æ ¼å¼

2. **æ–‡ä»¶è·¯å¾„å˜åŒ–**ï¼šbelle ç›®å½•ä» `bell_open_source/` æ”¹ä¸º `belle/`
   - è§£å†³æ–¹æ¡ˆï¼šæ›´æ–°å¤„ç†å‡½æ•°ä¸­çš„è·¯å¾„å¼•ç”¨

3. **æ–‡ä»¶åå˜åŒ–**ï¼šbelle æ–‡ä»¶åæ›´è§„èŒƒ
   - è§£å†³æ–¹æ¡ˆï¼šæ›´æ–°å¤„ç†å‡½æ•°ä¸­çš„æ–‡ä»¶ååŒ¹é…é€»è¾‘

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ 1ï¼šImportError: No module named 'datasets'

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
pip install datasets
```

### é—®é¢˜ 2ï¼šä¸‹è½½é€Ÿåº¦æ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨ HuggingFace é•œåƒç«™ç‚¹
- è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
  ```bash
  export HF_ENDPOINT=https://hf-mirror.com
  ```

### é—®é¢˜ 3ï¼šç£ç›˜ç©ºé—´ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼š
- datasets åº“ä¼šç¼“å­˜ä¸‹è½½çš„æ•°æ®ï¼Œé»˜è®¤åœ¨ `~/.cache/huggingface/datasets/`
- å¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡æ›´æ”¹ç¼“å­˜ä½ç½®ï¼š
  ```bash
  export HF_DATASETS_CACHE="/path/to/cache"
  ```

### é—®é¢˜ 4ï¼šç½‘ç»œè¿æ¥é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
- datasets åº“æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°è¿è¡Œè„šæœ¬å³å¯
- æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | åŸæ–¹å¼ï¼ˆURLä¸‹è½½ï¼‰ | æ–°æ–¹å¼ï¼ˆdatasetsåº“ï¼‰ |
|------|------------------|---------------------|
| ä¸‹è½½é€Ÿåº¦ | å–å†³äºç½‘ç»œ | å–å†³äºç½‘ç»œ + è‡ªåŠ¨é‡è¯• |
| æ–­ç‚¹ç»­ä¼  | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ |
| ç¼“å­˜ç®¡ç† | âŒ æ‰‹åŠ¨ç®¡ç† | âœ… è‡ªåŠ¨ç®¡ç† |
| æ•°æ®éªŒè¯ | âŒ æ—  | âœ… è‡ªåŠ¨éªŒè¯ |
| ä»£ç å¤æ‚åº¦ | è¾ƒé«˜ | è¾ƒä½ |
| é”™è¯¯å¤„ç† | éœ€è¦æ‰‹åŠ¨å®ç° | è‡ªåŠ¨å¤„ç† |
| æ–‡ä»¶æ ¼å¼ | JSON | Parquetï¼ˆæ›´é«˜æ•ˆï¼‰ |

## ğŸ‰ æ€»ç»“

âœ… **å·²å®Œæˆçš„ä¿®æ”¹**ï¼š
1. å°† webtext2019zhã€baike_qaã€belle æ•°æ®é›†çš„ä¸‹è½½æ–¹å¼æ”¹ä¸ºä½¿ç”¨ `datasets` åº“
2. æ›´æ–°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨ HuggingFace æ•°æ®é›†è·¯å¾„è€Œä¸æ˜¯ URL
3. é‡å†™ä¸‹è½½å‡½æ•°ï¼Œä½¿ç”¨ `load_dataset()` API
4. å¯ç”¨æ‰€æœ‰æ•°æ®é›†çš„ä¸‹è½½
5. æ›´æ–°æ–‡æ¡£è¯´æ˜

âœ… **ä¼˜åŠ¿**ï¼š
- æ›´ç®€æ´çš„ä»£ç 
- æ›´å¥½çš„é”™è¯¯å¤„ç†
- è‡ªåŠ¨ç¼“å­˜ç®¡ç†
- æ”¯æŒæ–­ç‚¹ç»­ä¼ 
- ç»Ÿä¸€çš„æ•°æ®æ ¼å¼ï¼ˆparquetï¼‰

âœ… **æ³¨æ„äº‹é¡¹**ï¼š
- å¿…é¡»å®‰è£… `datasets` åº“
- æ–‡ä»¶æ ¼å¼ä» JSON æ”¹ä¸º parquet
- å¯èƒ½éœ€è¦æ›´æ–°æ•°æ®å¤„ç†å‡½æ•°ä»¥æ”¯æŒæ–°æ ¼å¼

ç°åœ¨å¯ä»¥ä½¿ç”¨æ›´ç°ä»£ã€æ›´å¯é çš„æ–¹å¼ä¸‹è½½ HuggingFace æ•°æ®é›†äº†ï¼ğŸ‰
