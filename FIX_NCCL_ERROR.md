# NCCL å…±äº«å†…å­˜é”™è¯¯ä¿®å¤æŒ‡å—

## ğŸ”´ é”™è¯¯ä¿¡æ¯

```
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:3690
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
Error while attaching to shared memory segment /dev/shm/nccl-ĞŒ (size 0), error: No such file or directory (2)
```

## ğŸ¯ é—®é¢˜åŸå› 

è¿™ä¸ªé”™è¯¯æ˜¯ç”±äº `/dev/shm` (å…±äº«å†…å­˜) ç©ºé—´ä¸è¶³æˆ–å­˜åœ¨æ®‹ç•™çš„NCCLå…±äº«å†…å­˜æ–‡ä»¶å¯¼è‡´çš„ã€‚

NCCLåœ¨å¤šGPUé€šä¿¡æ—¶éœ€è¦ä½¿ç”¨å…±äº«å†…å­˜ï¼Œå¦‚æœï¼š
1. `/dev/shm` ç©ºé—´ä¸è¶³
2. å­˜åœ¨æ—§çš„NCCLè¿›ç¨‹æ®‹ç•™æ–‡ä»¶
3. æƒé™é—®é¢˜

å°±ä¼šå¯¼è‡´è¿™ä¸ªé”™è¯¯ã€‚

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šæ¸…ç†å…±äº«å†…å­˜ï¼ˆæœ€ç®€å•ï¼Œæ¨èï¼‰

åœ¨**æœåŠ¡å™¨ä¸Š**æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
# 1. æ£€æŸ¥ /dev/shm ä½¿ç”¨æƒ…å†µ
df -h /dev/shm

# 2. æ¸…ç†æ—§çš„NCCLå…±äº«å†…å­˜æ–‡ä»¶
sudo rm -f /dev/shm/nccl-*

# 3. æ¸…ç†å…¶ä»–ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
sudo rm -f /dev/shm/sem.*

# 4. å†æ¬¡æ£€æŸ¥ç©ºé—´
df -h /dev/shm
```

**ç„¶åé‡æ–°è¿è¡Œè®­ç»ƒå‘½ä»¤**ï¼š

```bash
accelerate launch --multi_gpu --num_processes 2 \
    ./train_low_mem.py train \
    --is_finetune=True \
    --use_small_config=True
```

---

### æ–¹æ¡ˆ2ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœæ–¹æ¡ˆ1æ— æ•ˆï¼‰

åœ¨è®­ç»ƒå‘½ä»¤å‰æ·»åŠ ç¯å¢ƒå˜é‡ï¼š

```bash
# ç¦ç”¨NCCLä½¿ç”¨å…±äº«å†…å­˜ï¼Œæ”¹ç”¨socketé€šä¿¡
export NCCL_SHM_DISABLE=1

# æˆ–è€…æŒ‡å®šNCCLä½¿ç”¨å…¶ä»–ä¸´æ—¶ç›®å½•
export NCCL_SHM_DIR=/tmp

# ç„¶åè¿è¡Œè®­ç»ƒ
accelerate launch --multi_gpu --num_processes 2 \
    ./train_low_mem.py train \
    --is_finetune=True \
    --use_small_config=True
```

---

### æ–¹æ¡ˆ3ï¼šå¢åŠ  /dev/shm å¤§å°ï¼ˆå¦‚æœç©ºé—´ç¡®å®ä¸è¶³ï¼‰

å¦‚æœ `/dev/shm` ç©ºé—´å¤ªå°ï¼ˆé€šå¸¸é»˜è®¤æ˜¯å†…å­˜çš„50%ï¼‰ï¼Œå¯ä»¥ä¸´æ—¶å¢åŠ ï¼š

```bash
# æŸ¥çœ‹å½“å‰å¤§å°
df -h /dev/shm

# ä¸´æ—¶å¢åŠ åˆ°8GBï¼ˆéœ€è¦rootæƒé™ï¼‰
sudo mount -o remount,size=8G /dev/shm

# éªŒè¯
df -h /dev/shm
```

**æ°¸ä¹…ä¿®æ”¹**ï¼ˆéœ€è¦rootæƒé™ï¼‰ï¼š

ç¼–è¾‘ `/etc/fstab`ï¼Œæ·»åŠ æˆ–ä¿®æ”¹ï¼š

```
tmpfs /dev/shm tmpfs defaults,size=8G 0 0
```

ç„¶åé‡å¯ç³»ç»Ÿã€‚

---

### æ–¹æ¡ˆ4ï¼šä½¿ç”¨å•GPUè®­ç»ƒï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰

å¦‚æœä¸Šè¿°æ–¹æ¡ˆéƒ½ä¸è¡Œï¼Œå¯ä»¥å…ˆç”¨å•GPUè®­ç»ƒï¼š

```bash
# å•GPUä¸éœ€è¦NCCLé€šä¿¡ï¼Œä¸ä¼šæœ‰è¿™ä¸ªé—®é¢˜
python train_low_mem.py train \
    --is_finetune=True \
    --use_small_config=True
```

**æ³¨æ„**ï¼šå•GPUè®­ç»ƒé€Ÿåº¦ä¼šæ…¢ä¸€äº›ï¼Œä½†å¯ä»¥æ­£å¸¸å·¥ä½œã€‚

---

## ğŸ”§ å¿«é€Ÿä¿®å¤è„šæœ¬

åˆ›å»ºä¸€ä¸ªä¿®å¤è„šæœ¬ `fix_nccl.sh`ï¼š

```bash
#!/bin/bash
# åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œæ­¤è„šæœ¬

echo "=========================================="
echo "ä¿®å¤NCCLå…±äº«å†…å­˜é—®é¢˜"
echo "=========================================="

# 1. æ£€æŸ¥ç©ºé—´
echo ""
echo "1. å½“å‰ /dev/shm ä½¿ç”¨æƒ…å†µ:"
df -h /dev/shm

# 2. æ¸…ç†NCCLæ–‡ä»¶
echo ""
echo "2. æ¸…ç†æ—§çš„NCCLæ–‡ä»¶..."
sudo rm -f /dev/shm/nccl-* 2>/dev/null
sudo rm -f /dev/shm/sem.* 2>/dev/null
echo "æ¸…ç†å®Œæˆ"

# 3. æ£€æŸ¥æ¸…ç†åçš„ç©ºé—´
echo ""
echo "3. æ¸…ç†åçš„ /dev/shm ä½¿ç”¨æƒ…å†µ:"
df -h /dev/shm

# 4. æ€æ­»å¯èƒ½æ®‹ç•™çš„è®­ç»ƒè¿›ç¨‹
echo ""
echo "4. æ£€æŸ¥å¹¶æ¸…ç†æ®‹ç•™çš„è®­ç»ƒè¿›ç¨‹..."
pkill -f "train_low_mem.py" 2>/dev/null
pkill -f "accelerate" 2>/dev/null
echo "è¿›ç¨‹æ¸…ç†å®Œæˆ"

echo ""
echo "=========================================="
echo "ä¿®å¤å®Œæˆï¼ç°åœ¨å¯ä»¥é‡æ–°è¿è¡Œè®­ç»ƒå‘½ä»¤ï¼š"
echo "accelerate launch --multi_gpu --num_processes 2 \\"
echo "    ./train_low_mem.py train \\"
echo "    --is_finetune=True \\"
echo "    --use_small_config=True"
echo "=========================================="
```

**ä½¿ç”¨æ–¹æ³•**ï¼š

```bash
# åœ¨æœåŠ¡å™¨ä¸Š
chmod +x fix_nccl.sh
./fix_nccl.sh
```

---

## ğŸ“‹ æ¨èçš„å®Œæ•´ä¿®å¤æµç¨‹

### æ­¥éª¤1ï¼šåœ¨æœåŠ¡å™¨ä¸Šæ¸…ç†ç¯å¢ƒ

```bash
# SSHåˆ°æœåŠ¡å™¨
ssh rongtw@rongtw

# è¿›å…¥é¡¹ç›®ç›®å½•
cd /data3/ChatLM-mini-Chinese

# æ¸…ç†å…±äº«å†…å­˜
sudo rm -f /dev/shm/nccl-*

# æ¸…ç†æ®‹ç•™è¿›ç¨‹
pkill -f "train_low_mem.py"
pkill -f "accelerate"

# æ£€æŸ¥ç©ºé—´
df -h /dev/shm
```

### æ­¥éª¤2ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰

```bash
# æ·»åŠ åˆ° ~/.bashrc æˆ–ä¸´æ—¶è®¾ç½®
export NCCL_SHM_DISABLE=0  # 0=å¯ç”¨, 1=ç¦ç”¨
export NCCL_DEBUG=INFO     # å¯ç”¨è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
```

### æ­¥éª¤3ï¼šé‡æ–°è¿è¡Œè®­ç»ƒ

```bash
accelerate launch --multi_gpu --num_processes 2 \
    ./train_low_mem.py train \
    --is_finetune=True \
    --use_small_config=True
```

---

## ğŸ” è¯Šæ–­å‘½ä»¤

å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¿è¡Œä»¥ä¸‹è¯Šæ–­å‘½ä»¤ï¼š

```bash
# 1. æ£€æŸ¥ /dev/shm ç©ºé—´
df -h /dev/shm

# 2. æŸ¥çœ‹ /dev/shm ä¸­çš„æ–‡ä»¶
ls -lh /dev/shm/

# 3. æ£€æŸ¥NCCLç‰ˆæœ¬
python -c "import torch; print(torch.cuda.nccl.version())"

# 4. æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# 5. æ£€æŸ¥æ˜¯å¦æœ‰æ®‹ç•™è¿›ç¨‹
ps aux | grep train_low_mem
ps aux | grep accelerate

# 6. æµ‹è¯•NCCLé€šä¿¡
python -c "import torch; import torch.distributed as dist; print('NCCL available:', torch.cuda.nccl.is_available(['cuda:0', 'cuda:1']))"
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ä¸ªé”™è¯¯ï¼Ÿ

**A**: é€šå¸¸æ˜¯å› ä¸ºï¼š
1. ä¹‹å‰çš„è®­ç»ƒè¿›ç¨‹å¼‚å¸¸é€€å‡ºï¼Œç•™ä¸‹äº†æ®‹ç•™çš„å…±äº«å†…å­˜æ–‡ä»¶
2. `/dev/shm` ç©ºé—´ä¸è¶³ï¼ˆé»˜è®¤æ˜¯å†…å­˜çš„50%ï¼‰
3. å¤šä¸ªè®­ç»ƒä»»åŠ¡åŒæ—¶è¿è¡Œï¼Œå ç”¨äº†å…±äº«å†…å­˜

### Q2: æ¸…ç† /dev/shm ä¼šå½±å“å…¶ä»–ç¨‹åºå—ï¼Ÿ

**A**: æ¸…ç† `nccl-*` æ–‡ä»¶æ˜¯å®‰å…¨çš„ï¼Œè¿™äº›æ˜¯NCCLçš„ä¸´æ—¶æ–‡ä»¶ã€‚ä½†ä¸è¦åˆ é™¤å…¶ä»–ç¨‹åºçš„å…±äº«å†…å­˜æ–‡ä»¶ã€‚

### Q3: å¦‚æœæ²¡æœ‰sudoæƒé™æ€ä¹ˆåŠï¼Ÿ

**A**: ä½¿ç”¨æ–¹æ¡ˆ2ï¼Œè®¾ç½®ç¯å¢ƒå˜é‡ï¼š
```bash
export NCCL_SHM_DISABLE=1
```
æˆ–è€…ä½¿ç”¨å•GPUè®­ç»ƒï¼ˆæ–¹æ¡ˆ4ï¼‰ã€‚

### Q4: ä¸ºä»€ä¹ˆå•GPUè®­ç»ƒä¸ä¼šæœ‰è¿™ä¸ªé—®é¢˜ï¼Ÿ

**A**: å•GPUè®­ç»ƒä¸éœ€è¦NCCLè¿›è¡ŒGPUé—´é€šä¿¡ï¼Œæ‰€ä»¥ä¸ä¼šä½¿ç”¨å…±äº«å†…å­˜ã€‚

---

## ğŸ¯ æœ€å¿«çš„è§£å†³æ–¹æ³•ï¼ˆ3æ­¥ï¼‰

```bash
# 1. æ¸…ç†ï¼ˆåœ¨æœåŠ¡å™¨ä¸Šï¼‰
sudo rm -f /dev/shm/nccl-*

# 2. æ€æ­»æ®‹ç•™è¿›ç¨‹
pkill -f train_low_mem

# 3. é‡æ–°è¿è¡Œ
accelerate launch --multi_gpu --num_processes 2 \
    ./train_low_mem.py train \
    --is_finetune=True \
    --use_small_config=True
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [NCCL Documentation](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html)
- [PyTorch Distributed Troubleshooting](https://pytorch.org/docs/stable/distributed.html#troubleshooting)
- [Accelerate Multi-GPU Training](https://huggingface.co/docs/accelerate/usage_guides/distributed)

---

## âœ… éªŒè¯ä¿®å¤æˆåŠŸ

å¦‚æœçœ‹åˆ°ä»¥ä¸‹è¾“å‡ºï¼Œè¯´æ˜ä¿®å¤æˆåŠŸï¼š

```
================================================================================
ä½¿ç”¨ TrainConfigSFTSmall é…ç½®ï¼ˆå°æ•°æ®é›† - é€‚åˆ16Gå†…å­˜ï¼‰
================================================================================
[2026-02-09 11:25:21.049] [INFO]: ä½å†…å­˜æ¨¡å¼è®­ç»ƒ - é’ˆå¯¹16Gå†…å­˜ä¼˜åŒ–
[2026-02-09 11:25:21.049] [INFO]: cpu memory available: 13.15 GB, disk space available: 44.79 GB
[2026-02-09 11:25:21.049] [INFO]: ä½¿ç”¨LowMemDataset: æ”¯æŒå¤šGPU + ä½å†…å­˜æ¨¡å¼ï¼ŒæŒ‰éœ€ä»ç£ç›˜è¯»å–æ•°æ®
...
[2026-02-09 11:25:26.228] [INFO]: train dataset size: 5000, steps per epoch:2500
```

ç„¶åè®­ç»ƒä¼šæ­£å¸¸å¼€å§‹ï¼Œä¸ä¼šå†æŠ¥NCCLé”™è¯¯ã€‚

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€**
