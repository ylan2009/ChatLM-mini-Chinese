# è®­ç»ƒé—®é¢˜æ·±åº¦åˆ†æä¸è§£å†³æ–¹æ¡ˆ

## ğŸ“Š é—®é¢˜ç°çŠ¶

### å½“å‰è®­ç»ƒè¡¨ç°ï¼ˆEpoch 9ï¼‰
```
Loss: 5.5-6.0 (éå¸¸é«˜ï¼)
BLEU4: 0.004 (å‡ ä¹ä¸º0ï¼)
åˆå§‹Loss: 7.3-7.5
```

è¿™ä¸ªç»“æœè¡¨æ˜**æ¨¡å‹å‡ ä¹æ²¡æœ‰å­¦åˆ°ä»»ä½•ä¸œè¥¿**ã€‚

---

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### 1. é¢„è®­ç»ƒä¸¥é‡ä¸è¶³ ğŸš¨ **æ ¸å¿ƒé—®é¢˜**

#### é¢„è®­ç»ƒé…ç½®ï¼ˆTrainConfigï¼‰
```python
train_file: str = PROJECT_ROOT + '/data/my_train_dataset_3k.parquet'  # åªæœ‰3kæ¡æ•°æ®ï¼
epochs: int = 8
learn_rate: float = 0.0001
batch_size_per_gpu: int = 16
gradient_accumulation_steps: int = 8
```

#### é—®é¢˜åˆ†æ

**æ•°æ®é‡ä¸¥é‡ä¸è¶³**ï¼š
- ä½ çš„é¢„è®­ç»ƒï¼š**3,000æ¡å¯¹è¯** â‰ˆ çº¦50-100ä¸‡tokens
- æ­£å¸¸é¢„è®­ç»ƒéœ€è¦ï¼š**å‡ ç™¾ä¸‡åˆ°å‡ åäº¿tokens**
- å·®è·ï¼š**1000-10000å€ï¼**

**å¯¹æ¯”ä¸šç•Œæ ‡å‡†**ï¼š
| æ¨¡å‹ | é¢„è®­ç»ƒæ•°æ®é‡ | ä½ çš„æ¨¡å‹ |
|------|-------------|---------|
| GPT-2 Small | 40GBæ–‡æœ¬ (çº¦80äº¿tokens) | 3kæ¡ (çº¦100ä¸‡tokens) |
| T5-Small | C4æ•°æ®é›† (çº¦750GB) | 3kæ¡ |
| BERT-Base | 16GBæ–‡æœ¬ | 3kæ¡ |
| **ä½ çš„æ¨¡å‹** | **3kæ¡å¯¹è¯** | âŒ ä¸¥é‡ä¸è¶³ |

**ç»“è®º**ï¼šä½ çš„"é¢„è®­ç»ƒæ¨¡å‹"å®é™…ä¸Š**å‡ ä¹ç­‰äºéšæœºåˆå§‹åŒ–**ï¼Œæ²¡æœ‰å­¦åˆ°è¶³å¤Ÿçš„è¯­è¨€çŸ¥è¯†ï¼

---

### 2. SFTé…ç½®ä¸åŒ¹é…é¢„è®­ç»ƒè´¨é‡ âš ï¸

#### åŸSFTé…ç½®é—®é¢˜

```python
learn_rate: float = 2e-5      # å¤ªä½ï¼é€‚åˆé«˜è´¨é‡é¢„è®­ç»ƒæ¨¡å‹
div_factor: int = 50          # åˆå§‹å­¦ä¹ ç‡æ›´ä½
warmup_steps: int = 500       # å¤ªé•¿
```

è¿™ä¸ªé…ç½®é€‚åˆ**é«˜è´¨é‡é¢„è®­ç»ƒæ¨¡å‹**ï¼ˆå¦‚GPT-2ã€T5ï¼‰ï¼Œä½†ä½ çš„é¢„è®­ç»ƒæ¨¡å‹è´¨é‡å¾ˆå·®ï¼Œéœ€è¦ï¼š
- âœ… **æ›´é«˜çš„å­¦ä¹ ç‡**ï¼šè®©æ¨¡å‹å¿«é€Ÿå­¦ä¹ 
- âœ… **æ›´å¤šçš„è®­ç»ƒè½®æ•°**ï¼šå¼¥è¡¥é¢„è®­ç»ƒä¸è¶³
- âœ… **æ›´å¤§çš„æœ‰æ•ˆbatch size**ï¼šæé«˜è®­ç»ƒç¨³å®šæ€§

---

### 3. æ•°æ®é‡å¯¹æ¯”

| é˜¶æ®µ | æ•°æ®é‡ | æ˜¯å¦è¶³å¤Ÿ |
|------|--------|---------|
| é¢„è®­ç»ƒ | 3kæ¡ | âŒ ä¸¥é‡ä¸è¶³ï¼ˆéœ€è¦100ä¸‡+ï¼‰ |
| SFT | 8kæ¡ | âš ï¸ åå°‘ï¼ˆå»ºè®®5ä¸‡+ï¼‰ |

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šä¼˜åŒ–SFTé…ç½®ï¼ˆå·²å®æ–½ï¼‰âœ…

æ—¢ç„¶é¢„è®­ç»ƒä¸å……åˆ†ï¼Œæˆ‘ä»¬è®©SFTæ‰¿æ‹…æ›´å¤š"å­¦ä¹ "ä»»åŠ¡ï¼š

#### æ–°é…ç½®å¯¹æ¯”

| é…ç½®é¡¹ | æ—§å€¼ | æ–°å€¼ | åŸå›  |
|--------|------|------|------|
| **å­¦ä¹ ç‡** | 2e-5 | **5e-5** | æé«˜2.5å€ï¼ŒåŠ å¿«å­¦ä¹  |
| **div_factor** | 50 | **25** | åˆå§‹å­¦ä¹ ç‡æ›´é«˜ |
| **Epochs** | 10 | **20** | å¢åŠ è®­ç»ƒæ—¶é—´ |
| **æ¢¯åº¦ç´¯ç§¯** | 4 | **6** | æœ‰æ•ˆbatch=240ï¼Œæ›´ç¨³å®š |
| **Warmup** | 500 | **200** | å¿«é€Ÿè¿›å…¥æ­£å¸¸å­¦ä¹ ç‡ |
| **logging_steps** | 25 | **20** | æ›´é¢‘ç¹ç›‘æ§ |
| **save_steps** | 125 | **83** | æ¯epochä¿å­˜ |

#### é¢„æœŸæ•ˆæœ

ä½¿ç”¨æ–°é…ç½®åï¼Œè®­ç»ƒåº”è¯¥çœ‹åˆ°ï¼š

**Epoch 0-2**ï¼ˆå¿«é€Ÿä¸‹é™æœŸï¼‰ï¼š
```
Epoch 0: loss 7.0 â†’ 5.5 â†’ 4.5 â†’ 3.8, BLEU4: 0.01-0.03
Epoch 1: loss 3.5 â†’ 3.0 â†’ 2.7, BLEU4: 0.05-0.08
Epoch 2: loss 2.5 â†’ 2.2 â†’ 2.0, BLEU4: 0.10-0.15
```

**Epoch 3-10**ï¼ˆç¨³å®šä¸‹é™æœŸï¼‰ï¼š
```
Epoch 5: avg_loss 1.5-1.8, BLEU4: 0.20-0.25
Epoch 10: avg_loss 1.0-1.3, BLEU4: 0.30-0.35
```

**Epoch 11-20**ï¼ˆæ”¶æ•›æœŸï¼‰ï¼š
```
Epoch 15: avg_loss 0.7-1.0, BLEU4: 0.35-0.40
Epoch 20: avg_loss 0.5-0.8, BLEU4: 0.40-0.45
```

---

### æ–¹æ¡ˆ2ï¼šå¢åŠ SFTæ•°æ®é‡ï¼ˆæ¨èï¼‰ğŸŒŸ

#### å½“å‰é—®é¢˜
- 8kæ¡SFTæ•°æ®å¯¹äºä¸€ä¸ª"å‡ ä¹éšæœºåˆå§‹åŒ–"çš„æ¨¡å‹æ¥è¯´å¤ªå°‘äº†

#### å»ºè®®
1. **æ”¶é›†æ›´å¤šSFTæ•°æ®**ï¼š
   - ç›®æ ‡ï¼š**5ä¸‡-10ä¸‡æ¡**é«˜è´¨é‡å¯¹è¯æ•°æ®
   - æ¥æºï¼šå¼€æºæ•°æ®é›†ï¼ˆå¦‚BELLEã€Alpacaä¸­æ–‡ç‰ˆã€ShareGPTç­‰ï¼‰

2. **æ•°æ®å¢å¼º**ï¼š
   - å¯¹ç°æœ‰8kæ•°æ®è¿›è¡Œæ”¹å†™ã€æ‰©å±•
   - ä½¿ç”¨GPT-4ç­‰ç”Ÿæˆæ›´å¤šè®­ç»ƒæ ·æœ¬

3. **å¤šè½®å¯¹è¯æ•°æ®**ï¼š
   - å¢åŠ å¤šè½®å¯¹è¯æ ·æœ¬
   - æé«˜æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›

#### æ¨èæ•°æ®é›†
```bash
# BELLEä¸­æ–‡æ•°æ®é›†ï¼ˆæ¨èï¼‰
https://huggingface.co/datasets/BelleGroup/train_2M_CN

# Alpacaä¸­æ–‡ç‰ˆ
https://github.com/ymcui/Chinese-LLaMA-Alpaca

# ShareGPTä¸­æ–‡
https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k
```

---

### æ–¹æ¡ˆ3ï¼šé‡æ–°é¢„è®­ç»ƒï¼ˆæœ€ä½³ä½†è€—æ—¶ï¼‰â­â­â­

#### ä¸ºä»€ä¹ˆéœ€è¦é‡æ–°é¢„è®­ç»ƒï¼Ÿ

å½“å‰é¢„è®­ç»ƒæ¨¡å‹è´¨é‡å¤ªå·®ï¼Œç›¸å½“äº"åœ°åŸºä¸ç‰¢"ï¼Œå†æ€ä¹ˆSFTä¹Ÿå¾ˆéš¾è¾¾åˆ°å¥½æ•ˆæœã€‚

#### é¢„è®­ç»ƒæ•°æ®å»ºè®®

**æœ€ä½è¦æ±‚**ï¼š
- æ•°æ®é‡ï¼š**50ä¸‡-100ä¸‡æ¡**å¯¹è¯æˆ–æ–‡æœ¬
- Tokensï¼š**1-2äº¿tokens**
- è®­ç»ƒæ—¶é—´ï¼š2-3å¤©ï¼ˆåŒå¡3080ï¼‰

**æ¨èé…ç½®**ï¼š
- æ•°æ®é‡ï¼š**500ä¸‡-1000ä¸‡æ¡**
- Tokensï¼š**10-20äº¿tokens**
- è®­ç»ƒæ—¶é—´ï¼š1-2å‘¨

#### é¢„è®­ç»ƒæ•°æ®æ¥æº

```python
# 1. ç»´åŸºç™¾ç§‘ä¸­æ–‡ï¼ˆé«˜è´¨é‡ï¼‰
https://dumps.wikimedia.org/zhwiki/

# 2. ç™¾åº¦ç™¾ç§‘çˆ¬è™«æ•°æ®
https://github.com/brightmart/nlp_chinese_corpus

# 3. æ–°é—»è¯­æ–™
https://github.com/brightmart/nlp_chinese_corpus

# 4. é€šç”¨å¯¹è¯æ•°æ®
https://github.com/codemayq/chinese_chatbot_corpus

# 5. æ··åˆæ•°æ®é›†ï¼ˆæ¨èï¼‰
- ç»´åŸºç™¾ç§‘ï¼š30%
- æ–°é—»ï¼š20%
- ä¹¦ç±ï¼š20%
- å¯¹è¯ï¼š30%
```

#### ä¿®æ”¹é¢„è®­ç»ƒé…ç½®

```python
# config.py - TrainConfig
class TrainConfig:
    epochs: int = 10                           # å¢åŠ åˆ°10ä¸ªepoch
    batch_size_per_gpu: int = 32               # å¢åŠ batch size
    
    learn_rate: float = 0.0001                 # ä¿æŒ1e-4
    div_factor: int = 50
    
    gradient_accumulation_steps: int = 8
    warmup_steps: int = 2000                   # å¢åŠ warmup
    
    # ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†
    train_file: str = PROJECT_ROOT + '/data/pretrain_dataset_500k.parquet'  # 50ä¸‡æ¡
    validation_file: str = PROJECT_ROOT + '/data/pretrain_valid_10k.parquet'
    
    max_seq_len: int = 256
    save_steps: int = 5000                     # æ¯5000æ­¥ä¿å­˜
```

---

## ğŸ¯ æ¨èè¡ŒåŠ¨æ–¹æ¡ˆ

### çŸ­æœŸæ–¹æ¡ˆï¼ˆ1-2å¤©ï¼‰ï¼šä¼˜åŒ–SFTé…ç½® âœ…

**å·²å®Œæˆ**ï¼šé…ç½®å·²æ›´æ–°

**ç«‹å³æ‰§è¡Œ**ï¼š
```bash
cd /Users/twrong/git/code/ChatLM-mini-Chinese

# 1. æ¸…ç†æ—§çŠ¶æ€
rm -rf model_save/sft/train_latest_state_sft
rm -f model_save/sft/chat_small_t5.*.bin

# 2. å¯åŠ¨è®­ç»ƒ
accelerate launch --multi_gpu --num_processes 2 ./train.py train --is_finetune=True

# 3. ç›‘æ§ï¼ˆå¦ä¸€ä¸ªç»ˆç«¯ï¼‰
tail -f logs/chat_trainer-*.log | grep "epoch log"
```

**é¢„æœŸç»“æœ**ï¼š
- Lossèƒ½é™åˆ°1.0ä»¥ä¸‹
- BLEU4èƒ½è¾¾åˆ°0.35-0.45
- ä½†æ•ˆæœä»ç„¶æœ‰é™ï¼ˆå› ä¸ºé¢„è®­ç»ƒä¸è¶³ï¼‰

---

### ä¸­æœŸæ–¹æ¡ˆï¼ˆ3-7å¤©ï¼‰ï¼šå¢åŠ SFTæ•°æ® ğŸŒŸ

1. **ä¸‹è½½å¼€æºæ•°æ®é›†**ï¼š
```bash
# ä¸‹è½½BELLEæ•°æ®é›†ï¼ˆæ¨èï¼‰
git clone https://huggingface.co/datasets/BelleGroup/train_2M_CN

# æˆ–ä½¿ç”¨ShareGPT
git clone https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k
```

2. **æ•°æ®é¢„å¤„ç†**ï¼š
```python
# åˆ›å»º prepare_large_sft_data.py
import pandas as pd

# è¯»å–å¼€æºæ•°æ®
belle_data = pd.read_json('train_2M_CN.json')

# è½¬æ¢æ ¼å¼
sft_data = pd.DataFrame({
    'prompt': belle_data['instruction'],
    'response': belle_data['output']
})

# é‡‡æ ·5ä¸‡æ¡
sft_data_50k = sft_data.sample(n=50000, random_state=42)

# ä¿å­˜
sft_data_50k.to_parquet('data/sft_train_dataset_50k.parquet')
```

3. **æ›´æ–°é…ç½®**ï¼š
```python
# config.py
train_file: str = PROJECT_ROOT + '/data/sft_train_dataset_50k.parquet'
epochs: int = 15  # æ•°æ®å¤šäº†å¯ä»¥å‡å°‘epoch
```

**é¢„æœŸç»“æœ**ï¼š
- Lossèƒ½é™åˆ°0.5-0.8
- BLEU4èƒ½è¾¾åˆ°0.45-0.55
- æ¨¡å‹è´¨é‡æ˜æ˜¾æå‡

---

### é•¿æœŸæ–¹æ¡ˆï¼ˆ1-2å‘¨ï¼‰ï¼šé‡æ–°é¢„è®­ç»ƒ â­â­â­

1. **å‡†å¤‡é¢„è®­ç»ƒæ•°æ®**ï¼ˆ500ä¸‡æ¡ï¼‰
2. **ä¿®æ”¹é¢„è®­ç»ƒé…ç½®**ï¼ˆè§ä¸Šæ–‡ï¼‰
3. **é¢„è®­ç»ƒ10ä¸ªepoch**ï¼ˆçº¦1-2å‘¨ï¼‰
4. **ä½¿ç”¨æ–°é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒSFT**

**é¢„æœŸç»“æœ**ï¼š
- é¢„è®­ç»ƒååˆå§‹lossï¼š2-3ï¼ˆæ­£å¸¸æ°´å¹³ï¼‰
- SFTålossï¼š0.3-0.6
- BLEU4ï¼š0.55-0.70
- æ¨¡å‹è´¨é‡æ¥è¿‘å°å‹å•†ç”¨æ¨¡å‹

---

## ğŸ“Š ä¸‰ç§æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | æ—¶é—´æˆæœ¬ | æ•ˆæœ | æ¨èåº¦ | é€‚ç”¨åœºæ™¯ |
|------|---------|------|--------|---------|
| **ä¼˜åŒ–SFTé…ç½®** | 1-2å¤© | â­â­ | âœ… | å¿«é€ŸéªŒè¯ |
| **å¢åŠ SFTæ•°æ®** | 3-7å¤© | â­â­â­â­ | â­â­â­ | ä¸­æœŸæ”¹è¿› |
| **é‡æ–°é¢„è®­ç»ƒ** | 1-2å‘¨ | â­â­â­â­â­ | â­â­â­â­â­ | é•¿æœŸæœ€ä½³ |

---

## ğŸ” è®­ç»ƒç›‘æ§å…³é”®æŒ‡æ ‡

### æ­£å¸¸è®­ç»ƒçš„æ ‡å¿—

#### ä½¿ç”¨æ–°é…ç½®ï¼ˆæ–¹æ¡ˆ1ï¼‰
```
Epoch 0: loss 7.0 â†’ 4.0, BLEU4: 0.02
Epoch 2: loss 3.0 â†’ 2.0, BLEU4: 0.12
Epoch 5: loss 1.5 â†’ 1.2, BLEU4: 0.25
Epoch 10: loss 1.0 â†’ 0.8, BLEU4: 0.35
Epoch 20: loss 0.6 â†’ 0.5, BLEU4: 0.42
```

#### ä½¿ç”¨æ›´å¤šæ•°æ®ï¼ˆæ–¹æ¡ˆ2ï¼‰
```
Epoch 0: loss 6.5 â†’ 3.5, BLEU4: 0.03
Epoch 2: loss 2.5 â†’ 1.8, BLEU4: 0.15
Epoch 5: loss 1.2 â†’ 0.9, BLEU4: 0.30
Epoch 10: loss 0.7 â†’ 0.5, BLEU4: 0.48
Epoch 15: loss 0.4 â†’ 0.3, BLEU4: 0.55
```

#### é‡æ–°é¢„è®­ç»ƒåï¼ˆæ–¹æ¡ˆ3ï¼‰
```
SFT Epoch 0: loss 2.5 â†’ 2.0, BLEU4: 0.12  # åˆå§‹lossæ­£å¸¸ï¼
SFT Epoch 2: loss 1.5 â†’ 1.2, BLEU4: 0.28
SFT Epoch 5: loss 0.8 â†’ 0.6, BLEU4: 0.45
SFT Epoch 10: loss 0.4 â†’ 0.3, BLEU4: 0.65
```

---

## âš ï¸ é‡è¦æé†’

### 1. æ£€æŸ¥åˆå§‹Loss

è®­ç»ƒå¼€å§‹åï¼Œ**ç«‹å³æ£€æŸ¥ç¬¬ä¸€æ­¥çš„loss**ï¼š

- âœ… **5-7**ï¼šæ–°é…ç½®ç”Ÿæ•ˆï¼Œç»§ç»­è®­ç»ƒ
- âš ï¸ **7-9**ï¼šé…ç½®å¯èƒ½æ²¡ç”Ÿæ•ˆï¼Œæ£€æŸ¥é…ç½®æ–‡ä»¶
- âŒ **>9**ï¼šä¸¥é‡é—®é¢˜ï¼Œç«‹å³åœæ­¢

### 2. è§‚å¯ŸLossä¸‹é™è¶‹åŠ¿

**æ­£å¸¸**ï¼š
```
Step 0: 7.0
Step 20: 6.2
Step 40: 5.5
Step 60: 4.8
Step 80: 4.2
```

**å¼‚å¸¸**ï¼ˆåœæ­¢è®­ç»ƒï¼‰ï¼š
```
Step 0: 7.0
Step 20: 7.5  # ä¸Šå‡äº†ï¼
Step 40: 8.2  # ç»§ç»­ä¸Šå‡ï¼
```

### 3. ä½•æ—¶åœæ­¢è®­ç»ƒ

**æå‰åœæ­¢æ¡ä»¶**ï¼š
- Lossè¿ç»­3ä¸ªepochä¸ä¸‹é™
- BLEU4è¿ç»­5ä¸ªepochä¸æå‡
- Losså¼€å§‹ä¸Šå‡

---

## ğŸ’¡ é¢å¤–å»ºè®®

### 1. ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨

å½“å‰ä½¿ç”¨OneCycleLRï¼Œå¯ä»¥è€ƒè™‘ï¼š
```python
# ä½¿ç”¨ä½™å¼¦é€€ç«
scheduler = 'cosine'  # æ›´å¹³æ»‘çš„å­¦ä¹ ç‡å˜åŒ–
```

### 2. æ•°æ®è´¨é‡æ£€æŸ¥

```python
# æ£€æŸ¥æ•°æ®è´¨é‡è„šæœ¬
import pandas as pd

df = pd.read_parquet('data/sft_train_dataset.parquet')

# æ£€æŸ¥ç©ºå€¼
print(df.isnull().sum())

# æ£€æŸ¥é•¿åº¦åˆ†å¸ƒ
print(df['prompt'].str.len().describe())
print(df['response'].str.len().describe())

# æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
print(f"é‡å¤æ ·æœ¬: {df.duplicated().sum()}")

# æ˜¾ç¤ºæ ·æœ¬
for i in range(5):
    print(f"\næ ·æœ¬ {i+1}:")
    print(f"Q: {df.iloc[i]['prompt']}")
    print(f"A: {df.iloc[i]['response']}")
```

### 3. æ¨¡å‹è¯„ä¼°

è®­ç»ƒå®Œæˆåï¼Œæ‰‹åŠ¨æµ‹è¯•ï¼š
```python
from model.chat_model import TextToTextModel
from transformers import PreTrainedTokenizerFast

# åŠ è½½æ¨¡å‹
model = TextToTextModel.from_pretrained('model_save/sft/')
tokenizer = PreTrainedTokenizerFast.from_pretrained('model_save/my_tokenizer_wiki/')

# æµ‹è¯•
test_prompts = [
    "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
]

for prompt in test_prompts:
    inputs = tokenizer(prompt, return_tensors='pt')
    outputs = model.generate(**inputs, max_length=100)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"Q: {prompt}")
    print(f"A: {response}\n")
```

---

## ğŸ“š æ€»ç»“

### æ ¸å¿ƒé—®é¢˜
1. âŒ **é¢„è®­ç»ƒä¸¥é‡ä¸è¶³**ï¼ˆ3kæ¡ vs éœ€è¦100ä¸‡+æ¡ï¼‰
2. âš ï¸ **SFTæ•°æ®åå°‘**ï¼ˆ8kæ¡ï¼Œå»ºè®®5ä¸‡+ï¼‰
3. âš ï¸ **SFTé…ç½®ä¸åŒ¹é…**ï¼ˆå­¦ä¹ ç‡å¤ªä½ï¼‰

### å·²é‡‡å–æªæ–½
âœ… ä¼˜åŒ–SFTé…ç½®ï¼ˆå­¦ä¹ ç‡5e-5ï¼Œ20 epochsï¼Œæ›´å¤§batch sizeï¼‰

### ä¸‹ä¸€æ­¥å»ºè®®
1. **ç«‹å³**ï¼šä½¿ç”¨æ–°é…ç½®è®­ç»ƒï¼Œè§‚å¯Ÿæ•ˆæœ
2. **3å¤©å†…**ï¼šæ”¶é›†æ›´å¤šSFTæ•°æ®ï¼ˆç›®æ ‡5ä¸‡æ¡ï¼‰
3. **1å‘¨å†…**ï¼šå‡†å¤‡é¢„è®­ç»ƒæ•°æ®ï¼Œé‡æ–°é¢„è®­ç»ƒ

### é¢„æœŸæœ€ç»ˆæ•ˆæœ
- çŸ­æœŸï¼ˆæ–°é…ç½®ï¼‰ï¼šBLEU4 0.35-0.45
- ä¸­æœŸï¼ˆæ›´å¤šæ•°æ®ï¼‰ï¼šBLEU4 0.45-0.55
- é•¿æœŸï¼ˆé‡æ–°é¢„è®­ç»ƒï¼‰ï¼šBLEU4 0.55-0.70

---

**ç°åœ¨ç«‹å³å¼€å§‹è®­ç»ƒï¼Œè§‚å¯Ÿæ–°é…ç½®çš„æ•ˆæœï¼** ğŸš€
