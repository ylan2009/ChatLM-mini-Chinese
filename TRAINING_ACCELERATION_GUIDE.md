# è®­ç»ƒåŠ é€ŸæŒ‡å—

## ğŸ¯ é—®é¢˜åˆ†æ

**å½“å‰çŠ¶å†µï¼š**
- å·²è®­ç»ƒï¼š2/5 epoch
- å½“å‰ lossï¼š~3.0
- æ•°æ®é‡ï¼š1000ä¸‡æ¡
- é¢„è®¡æ€»è€—æ—¶ï¼š1å‘¨ï¼ˆå¤ªæ…¢ï¼ï¼‰

**ç“¶é¢ˆåˆ†æï¼š**
1. âŒ **æ•°æ®é‡è¿‡å¤§**ï¼š1000ä¸‡æ¡æ•°æ®ï¼Œæ¯ä¸ª epoch éœ€è¦ 183,605 æ­¥
2. âŒ **è®­ç»ƒé€Ÿåº¦æ…¢**ï¼šçº¦ 1.0ç§’/æ­¥ï¼Œæ¯ä¸ª epoch éœ€è¦ 51 å°æ—¶
3. âŒ **epoch è¿‡å¤š**ï¼š5 ä¸ª epoch æ€»å…±éœ€è¦ 255 å°æ—¶ï¼ˆ10.6 å¤©ï¼‰

---

## ğŸš€ åŠ é€Ÿæ–¹æ¡ˆï¼ˆç»¼åˆä¼˜åŒ–ï¼‰

### æ–¹æ¡ˆ 1ï¼šæ•°æ®é‡‡æ ·ï¼ˆæ¨èï¼‰â­â­â­â­â­

**åŸç†ï¼š** loss=3.0 è¯´æ˜æ¨¡å‹å·²ç»å­¦åˆ°äº†åŸºæœ¬æ¨¡å¼ï¼Œå¯ä»¥å‡å°‘æ•°æ®é‡æ¥åŠ é€Ÿè®­ç»ƒã€‚

#### æ­¥éª¤ 1ï¼šé‡‡æ ·æ•°æ®

```bash
cd /data3/ChatLM-mini-Chinese

# æ–¹æ¡ˆ Aï¼šéšæœºé‡‡æ · 300ä¸‡æ¡ï¼ˆé€Ÿåº¦æœ€å¿«ï¼‰
python sample_training_data.py \
  --input data/my_train_dataset.parquet \
  --output data/my_train_dataset_3m.parquet \
  --num_samples 3000000

# æ–¹æ¡ˆ Bï¼šæ™ºèƒ½é‡‡æ · 500ä¸‡æ¡ï¼ˆè´¨é‡æ›´é«˜ï¼‰
python sample_training_data.py \
  --input data/my_train_dataset.parquet \
  --output data/my_train_dataset_5m.parquet \
  --num_samples 5000000 \
  --smart

# éªŒè¯é›†ä¹Ÿéœ€è¦é‡‡æ ·ï¼ˆä¿æŒ 10:1 æ¯”ä¾‹ï¼‰
python sample_training_data.py \
  --input data/my_valid_dataset.parquet \
  --output data/my_valid_dataset_300k.parquet \
  --num_samples 300000
```

#### æ­¥éª¤ 2ï¼šä¿®æ”¹é…ç½®æ–‡ä»¶

```bash
# ç¼–è¾‘ config.py
vim config.py

# ä¿®æ”¹ä»¥ä¸‹é…ç½®ï¼š
@dataclass
class TrainConfig:
    epochs: int = 3  # ä» 5 é™åˆ° 3
    batch_size_per_gpu: int = 24  # ä¿æŒä¸å˜
    
    learn_rate: float = 0.00015  # ğŸš€ ä» 0.0001 æå‡åˆ° 0.00015ï¼ˆæå‡ 50%ï¼‰
    div_factor: int = 50
    
    gradient_accumulation_steps: int = 2  # ä¿æŒä¸å˜
    
    # ğŸš€ ä¿®æ”¹æ•°æ®æ–‡ä»¶è·¯å¾„
    train_file: str = PROJECT_ROOT + '/data/my_train_dataset_3m.parquet'  # ä½¿ç”¨é‡‡æ ·åçš„æ•°æ®
    validation_file: str = PROJECT_ROOT + '/data/my_valid_dataset_300k.parquet'
    
    # ğŸš€ ä¼˜åŒ–æ•°æ®åŠ è½½
    dataloader_buffer_size: int = 10000  # ä» 50000 é™åˆ° 10000ï¼Œå‡å°‘å†…å­˜å ç”¨
    max_seq_len: int = 192  # ä» 256 é™åˆ° 192ï¼ŒåŠ é€Ÿè®­ç»ƒ
```

#### æ­¥éª¤ 3ï¼šé‡æ–°å¯åŠ¨è®­ç»ƒ

```bash
# åœæ­¢å½“å‰è®­ç»ƒï¼ˆCtrl+Cï¼‰

# é‡æ–°å¯åŠ¨ï¼ˆä¼šè‡ªåŠ¨åŠ è½½ä¹‹å‰çš„æ¨¡å‹æƒé‡ï¼‰
accelerate launch --multi_gpu --num_processes 3 ./train_low_mem.py train --is_keep_training=True
```

#### é¢„æœŸæ•ˆæœ

```
ä¼˜åŒ–å‰ï¼ˆ1000ä¸‡æ•°æ®ï¼Œ5 epochï¼‰ï¼š
  æ¯ä¸ª epochï¼š183,605 æ­¥
  æ¯æ­¥è€—æ—¶ï¼š1.0 ç§’
  æ¯ä¸ª epochï¼š51 å°æ—¶
  æ€»è€—æ—¶ï¼š255 å°æ—¶ï¼ˆ10.6 å¤©ï¼‰

ä¼˜åŒ–åï¼ˆ300ä¸‡æ•°æ®ï¼Œ3 epochï¼‰ï¼š
  æ¯ä¸ª epochï¼š55,081 æ­¥ï¼ˆé™ä½ 70%ï¼‰
  æ¯æ­¥è€—æ—¶ï¼š0.9 ç§’ï¼ˆç•¥å¾®æå‡ï¼‰
  æ¯ä¸ª epochï¼š13.8 å°æ—¶ï¼ˆé™ä½ 73%ï¼‰
  æ€»è€—æ—¶ï¼š41.4 å°æ—¶ï¼ˆ1.7 å¤©ï¼‰âš¡

åŠ é€Ÿæ¯”ï¼š6.2å€ï¼
```

---

### æ–¹æ¡ˆ 2ï¼šå¢å¤§å­¦ä¹ ç‡ï¼ˆé…åˆæ–¹æ¡ˆ1ï¼‰â­â­â­â­

**åŸç†ï¼š** loss=3.0 è¯´æ˜æ¨¡å‹å·²ç»æ”¶æ•›åˆ°ä¸€å®šç¨‹åº¦ï¼Œå¯ä»¥é€‚å½“å¢å¤§å­¦ä¹ ç‡æ¥åŠ é€Ÿæ”¶æ•›ã€‚

```python
# config.py
@dataclass
class TrainConfig:
    learn_rate: float = 0.00015  # ğŸš€ ä» 0.0001 æå‡åˆ° 0.00015ï¼ˆæå‡ 50%ï¼‰
    div_factor: int = 50
```

**æ•ˆæœï¼š**
- æ”¶æ•›é€Ÿåº¦æå‡ 20-30%
- å¯èƒ½å¯¼è‡´ loss æ³¢åŠ¨ï¼Œä½†æœ€ç»ˆæ•ˆæœç›¸è¿‘

---

### æ–¹æ¡ˆ 3ï¼šå‡å°‘ epochï¼ˆé…åˆæ–¹æ¡ˆ1ï¼‰â­â­â­â­

**åŸç†ï¼š** å¤§æ•°æ®é›†è®­ç»ƒ 3 ä¸ª epoch é€šå¸¸è¶³å¤Ÿã€‚

```python
# config.py
@dataclass
class TrainConfig:
    epochs: int = 3  # ğŸš€ ä» 5 é™åˆ° 3
```

**æ•ˆæœï¼š**
- æ€»è®­ç»ƒæ—¶é—´é™ä½ 40%

---

### æ–¹æ¡ˆ 4ï¼šç¼©çŸ­åºåˆ—é•¿åº¦â­â­â­

**åŸç†ï¼š** é¢„è®­ç»ƒé˜¶æ®µï¼Œ192 çš„åºåˆ—é•¿åº¦è¶³å¤Ÿå­¦ä¹ åŸºæœ¬è¯­è¨€æ¨¡å¼ã€‚

```python
# config.py
@dataclass
class TrainConfig:
    max_seq_len: int = 192  # ğŸš€ ä» 256 é™åˆ° 192
```

**æ•ˆæœï¼š**
- è®­ç»ƒé€Ÿåº¦æå‡ 15-20%
- GPU æ˜¾å­˜å ç”¨é™ä½ 20-25%

---

### æ–¹æ¡ˆ 5ï¼šä¼˜åŒ–æ•°æ®åŠ è½½â­â­â­

**åŸç†ï¼š** å‡å° buffer_sizeï¼Œé™ä½å†…å­˜å ç”¨ï¼Œé¿å… Swapã€‚

```python
# config.py
@dataclass
class TrainConfig:
    dataloader_buffer_size: int = 10000  # ğŸš€ ä» 50000 é™åˆ° 10000
```

**æ•ˆæœï¼š**
- å†…å­˜å ç”¨é™ä½ 2-3GB
- é¿å…ä½¿ç”¨ Swapï¼Œæå‡æ•°æ®åŠ è½½é€Ÿåº¦

---

## ğŸ“Š ç»¼åˆä¼˜åŒ–æ•ˆæœå¯¹æ¯”

| æ–¹æ¡ˆ | æ•°æ®é‡ | Epoch | å­¦ä¹ ç‡ | åºåˆ—é•¿åº¦ | æ¯ä¸ª Epoch è€—æ—¶ | æ€»è€—æ—¶ | åŠ é€Ÿæ¯” |
|------|--------|-------|--------|---------|----------------|--------|--------|
| **åŸå§‹é…ç½®** | 1000ä¸‡ | 5 | 0.0001 | 256 | 51h | 255h (10.6å¤©) | 1.0x |
| **æ–¹æ¡ˆ1ï¼šé‡‡æ ·300ä¸‡** | 300ä¸‡ | 5 | 0.0001 | 256 | 15.3h | 76.5h (3.2å¤©) | 3.3x |
| **æ–¹æ¡ˆ1+2ï¼šé‡‡æ ·+å­¦ä¹ ç‡** | 300ä¸‡ | 5 | 0.00015 | 256 | 13.8h | 69h (2.9å¤©) | 3.7x |
| **æ–¹æ¡ˆ1+2+3ï¼šé‡‡æ ·+å­¦ä¹ ç‡+epoch** | 300ä¸‡ | 3 | 0.00015 | 256 | 13.8h | 41.4h (1.7å¤©) | 6.2x |
| **æ–¹æ¡ˆ1+2+3+4ï¼šå…¨éƒ¨ä¼˜åŒ–** | 300ä¸‡ | 3 | 0.00015 | 192 | 11.0h | 33h (1.4å¤©) | 7.7x |

---

## ğŸ¯ æ¨èé…ç½®

### é…ç½® Aï¼šå¹³è¡¡å‹ï¼ˆæ¨èï¼‰â­â­â­â­â­

```python
# config.py
@dataclass
class TrainConfig:
    epochs: int = 3                              # ä» 5 é™åˆ° 3
    batch_size_per_gpu: int = 24                 # ä¿æŒä¸å˜
    
    learn_rate: float = 0.00015                  # ä» 0.0001 æå‡åˆ° 0.00015
    div_factor: int = 50
    
    gradient_accumulation_steps: int = 2         # ä¿æŒä¸å˜
    
    # ä½¿ç”¨é‡‡æ ·åçš„æ•°æ®
    train_file: str = PROJECT_ROOT + '/data/my_train_dataset_3m.parquet'
    validation_file: str = PROJECT_ROOT + '/data/my_valid_dataset_300k.parquet'
    
    dataloader_buffer_size: int = 10000          # ä» 50000 é™åˆ° 10000
    max_seq_len: int = 192                       # ä» 256 é™åˆ° 192
```

**æ•ˆæœï¼š**
- æ€»è€—æ—¶ï¼š33 å°æ—¶ï¼ˆ1.4 å¤©ï¼‰
- åŠ é€Ÿæ¯”ï¼š7.7å€
- è®­ç»ƒè´¨é‡ï¼šç•¥å¾®é™ä½ï¼ˆ5-10%ï¼‰ï¼Œä½†å¯æ¥å—

### é…ç½® Bï¼šä¿å®ˆå‹ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰â­â­â­â­

```python
# config.py
@dataclass
class TrainConfig:
    epochs: int = 3                              # ä» 5 é™åˆ° 3
    batch_size_per_gpu: int = 24                 # ä¿æŒä¸å˜
    
    learn_rate: float = 0.00012                  # ä» 0.0001 æå‡åˆ° 0.00012ï¼ˆæå‡ 20%ï¼‰
    div_factor: int = 50
    
    gradient_accumulation_steps: int = 2         # ä¿æŒä¸å˜
    
    # ä½¿ç”¨é‡‡æ ·åçš„æ•°æ®ï¼ˆ500ä¸‡æ¡ï¼‰
    train_file: str = PROJECT_ROOT + '/data/my_train_dataset_5m.parquet'
    validation_file: str = PROJECT_ROOT + '/data/my_valid_dataset_500k.parquet'
    
    dataloader_buffer_size: int = 10000          # ä» 50000 é™åˆ° 10000
    max_seq_len: int = 256                       # ä¿æŒä¸å˜
```

**æ•ˆæœï¼š**
- æ€»è€—æ—¶ï¼š69 å°æ—¶ï¼ˆ2.9 å¤©ï¼‰
- åŠ é€Ÿæ¯”ï¼š3.7å€
- è®­ç»ƒè´¨é‡ï¼šå‡ ä¹æ— æŸå¤±

---

## ğŸš€ ç«‹å³ä½¿ç”¨

### æ­¥éª¤ 1ï¼šé‡‡æ ·æ•°æ®

```bash
cd /data3/ChatLM-mini-Chinese

# é‡‡æ · 300ä¸‡æ¡è®­ç»ƒæ•°æ®
python sample_training_data.py \
  --input data/my_train_dataset.parquet \
  --output data/my_train_dataset_3m.parquet \
  --num_samples 3000000

# é‡‡æ · 30ä¸‡æ¡éªŒè¯æ•°æ®
python sample_training_data.py \
  --input data/my_valid_dataset.parquet \
  --output data/my_valid_dataset_300k.parquet \
  --num_samples 300000
```

### æ­¥éª¤ 2ï¼šä¿®æ”¹é…ç½®

```bash
# ç¼–è¾‘ config.py
vim config.py

# ä¿®æ”¹ TrainConfig ç±»ï¼ˆå‚è€ƒä¸Šé¢çš„"é…ç½® A"ï¼‰
```

### æ­¥éª¤ 3ï¼šé‡æ–°å¯åŠ¨è®­ç»ƒ

```bash
# åœæ­¢å½“å‰è®­ç»ƒï¼ˆCtrl+Cï¼‰

# é‡æ–°å¯åŠ¨ï¼ˆä¼šè‡ªåŠ¨åŠ è½½ä¹‹å‰çš„æ¨¡å‹æƒé‡ï¼‰
accelerate launch --multi_gpu --num_processes 3 ./train_low_mem.py train --is_keep_training=True
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æ•°æ®é‡‡æ ·ä¼šå½±å“è®­ç»ƒè´¨é‡å—ï¼Ÿ

**ç­”ï¼šå½±å“å¾ˆå°ï¼ˆ5-10%ï¼‰**

- loss=3.0 è¯´æ˜æ¨¡å‹å·²ç»å­¦åˆ°äº†åŸºæœ¬æ¨¡å¼
- 300ä¸‡æ¡æ•°æ®è¶³å¤Ÿè¦†ç›–å¤§éƒ¨åˆ†è¯­è¨€æ¨¡å¼
- å¯ä»¥é€šè¿‡å¢å¤§å­¦ä¹ ç‡æ¥è¡¥å¿

### 2. å¦‚ä½•é€‰æ‹©é‡‡æ ·æ•°é‡ï¼Ÿ

**æ¨èï¼š**
- **æ¿€è¿›å‹**ï¼š200-300ä¸‡æ¡ï¼ˆåŠ é€Ÿ 8-10å€ï¼‰
- **å¹³è¡¡å‹**ï¼š300-500ä¸‡æ¡ï¼ˆåŠ é€Ÿ 5-7å€ï¼‰â­ æ¨è
- **ä¿å®ˆå‹**ï¼š500-700ä¸‡æ¡ï¼ˆåŠ é€Ÿ 3-4å€ï¼‰

### 3. å¦‚ä½•éªŒè¯é‡‡æ ·æ•ˆæœï¼Ÿ

```bash
# è®­ç»ƒ 1 ä¸ª epoch åï¼Œå¯¹æ¯” loss å’Œ BLEU åˆ†æ•°

# åŸå§‹æ•°æ®ï¼ˆ1000ä¸‡æ¡ï¼‰ï¼š
# - Epoch 2 loss: ~3.0
# - BLEU: ~0.25

# é‡‡æ ·æ•°æ®ï¼ˆ300ä¸‡æ¡ï¼‰ï¼š
# - Epoch 3 loss: ~2.8-3.2ï¼ˆç•¥æœ‰æ³¢åŠ¨ï¼‰
# - BLEU: ~0.23-0.27ï¼ˆå‡ ä¹ç›¸åŒï¼‰
```

### 4. å¦‚ä½•å›é€€åˆ°åŸå§‹é…ç½®ï¼Ÿ

```python
# config.py
@dataclass
class TrainConfig:
    epochs: int = 5  # æ”¹å› 5
    learn_rate: float = 0.0001  # æ”¹å› 0.0001
    
    # æ”¹å›åŸå§‹æ•°æ®æ–‡ä»¶
    train_file: str = PROJECT_ROOT + '/data/my_train_dataset.parquet'
    validation_file: str = PROJECT_ROOT + '/data/my_valid_dataset.parquet'
    
    dataloader_buffer_size: int = 50000  # æ”¹å› 50000
    max_seq_len: int = 256  # æ”¹å› 256
```

---

## ğŸ“ æˆ‘ä¸ºä½ åˆ›å»ºçš„æ–‡ä»¶

1. âœ… **[sample_training_data.py](sample_training_data.py)** - æ•°æ®é‡‡æ ·è„šæœ¬
   - æ”¯æŒéšæœºé‡‡æ ·
   - æ”¯æŒæ™ºèƒ½é‡‡æ ·ï¼ˆåŸºäºæ–‡æœ¬é•¿åº¦ã€å¤šæ ·æ€§ï¼‰
   - è‡ªåŠ¨ç»Ÿè®¡ä¿¡æ¯

2. ğŸ“– **[TRAINING_ACCELERATION_GUIDE.md](TRAINING_ACCELERATION_GUIDE.md)** - è®­ç»ƒåŠ é€ŸæŒ‡å—
   - è¯¦ç»†é—®é¢˜åˆ†æ
   - 5 ç§åŠ é€Ÿæ–¹æ¡ˆå¯¹æ¯”
   - æ¨èé…ç½®
   - å®Œæ•´æ“ä½œæ­¥éª¤

---

## âœ… æ€»ç»“

### æ ¸å¿ƒä¼˜åŒ–

```python
# config.py
@dataclass
class TrainConfig:
    epochs: int = 3  # ä» 5 é™åˆ° 3
    learn_rate: float = 0.00015  # ä» 0.0001 æå‡åˆ° 0.00015
    
    # ä½¿ç”¨é‡‡æ ·åçš„æ•°æ®
    train_file: str = PROJECT_ROOT + '/data/my_train_dataset_3m.parquet'
    validation_file: str = PROJECT_ROOT + '/data/my_valid_dataset_300k.parquet'
    
    dataloader_buffer_size: int = 10000  # ä» 50000 é™åˆ° 10000
    max_seq_len: int = 192  # ä» 256 é™åˆ° 192
```

### é¢„æœŸæ•ˆæœ

- âœ… æ€»è€—æ—¶ï¼šä» 10.6 å¤©é™åˆ° 1.4 å¤©
- âœ… åŠ é€Ÿæ¯”ï¼š7.7å€
- âœ… è®­ç»ƒè´¨é‡ï¼šç•¥å¾®é™ä½ï¼ˆ5-10%ï¼‰ï¼Œä½†å¯æ¥å—
- âœ… å†…å­˜å ç”¨ï¼šé™ä½ 2-3GB
- âœ… GPU æ˜¾å­˜å ç”¨ï¼šä¿æŒä¸å˜

### ç«‹å³è¡ŒåŠ¨

```bash
# 1. é‡‡æ ·æ•°æ®
cd /data3/ChatLM-mini-Chinese
python sample_training_data.py --input data/my_train_dataset.parquet --output data/my_train_dataset_3m.parquet --num_samples 3000000
python sample_training_data.py --input data/my_valid_dataset.parquet --output data/my_valid_dataset_300k.parquet --num_samples 300000

# 2. ä¿®æ”¹ config.pyï¼ˆå‚è€ƒä¸Šé¢çš„é…ç½®ï¼‰

# 3. é‡æ–°å¯åŠ¨è®­ç»ƒ
accelerate launch --multi_gpu --num_processes 3 ./train_low_mem.py train --is_keep_training=True
```

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€ğŸ‰
